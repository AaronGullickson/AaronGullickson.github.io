[
["index.html", "Statistical Analysis in Sociology Preface", " Statistical Analysis in Sociology Aaron Gullickson 2020-01-16 Preface This online textbook combines all of the information taught in my undergraduate statistics course as well as both terms of my introductory graduate statistics course. The undergraduate course consists of the first five chapters/modules up to and including Building Models. I have put a lot of work into this book, but it is still very much a work in progress, so you may notice typos and other errors on occasion. This textbook is designed to be used with the R statistical software program. If you are taking this course from me, then you will be running R through the RStudio Cloud. Various snippets of R code are interspersed throughout the book in order to show you how to do things. You will also find useful information in the appendices of this book. © Aaron Gullickson, 2017 This material is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License. Users are free to remix tweak, and build upon this work for non-commercial purposes, although new work must acknowledge the original author and use the same license. Full license is available here. "],
["understanding-data.html", "Understanding Data", " Understanding Data In this first module, we will cover what it actually means to have “data” and give a broad overview of what kinds of things we can do with data. Data are the foundation of any statistical analysis and most data that we use in the social sciences consist of variables measured on some observations. In the next two sections, we will learn more about these concepts. Slides for this module can be found here. "],
["what-does-data-look-like.html", "What Does Data Look Like?", " What Does Data Look Like? The data that we look at typically take the format of a “spreadsheet” with rows and columns. The table below shows some characteristics of four randomly drawn passengers from the Titanic, in this type of spreadsheet format. Table 1: Data on four passengers from the Titanic survival sex age agegroup pclass fare family Survived Female 24.0000 Adult First 69.3000 0 Died Male 24.0000 Adult Third 7.7958 0 Survived Male 0.9167 Child First 151.5500 3 Died Male 60.0000 Adult First 26.5500 0 Clearly, we can see variation in who survived and died, the passenger classes they were housed in, gender, and age. We also have a measure of the fare they paid for the trip (in English pounds) and the number of family members traveling with them. To understand how to think about data, we need to understand the concepts of an observation and a variable and the distinction between them. The observations The observations are what you have on the rows of your dataset. In the Titanic example, the observations are individual passengers on board the Titanic, but observations can take many different forms. We use the term unit of analysis to distinguish what kind of observation you have in your dataset. If you are interviewing individual people and recording their responses, then the unit of analysis is individual people. If you are collecting cross-national data by country, then the unit of analysis would be a country. If you are analyzing data on the “best colleges in the US” then the unit of analysis is a university/college. The most common unit of analysis that we will see in this course is an individual person, but several of our datasets involve other units of analysis and it is important to keep in mind that an observation can be many different kinds of things. The variables The variables are what you have on the columns of your dataset. Variables measure specific attributes of your observations. If you conduct a survey of individual people and ask them for their age, gender, and education, then these three attributes would be recorded as variables in your dataset. We refer to them as “variables” because they can take different values across the observations. If you were to conduct a survey of individual people and ask your respondents if they are human, then you probably wouldn’t have a proper variable because everyone would likely respond “yes” and there would be no variation (although we can’t necessarily rule out jedis. There are two major types of variables. Some variables measure quantities of something and thus can be represented by a number. We refer to these as quantitative variables. Other variables indicate a category to which the observation belongs. We refer to these as categorical variables. Quantitative variables Quantitative variables measure quantities of something. A person’s height, a worker’s hourly wage, the number of children that a woman has given birth to, a country’s gross domestic product, a US State’s poverty rate, and the percent of a university’s student body that are women are all examples of quantitative variables. They can all be represented by a number which indicates how much of the thing the observation has. There are two important sub-types of quantitative variables. Discrete variables can logically only take certain values within a range, while continuous variables can logically take any value within a range. The most common example of a discrete variable is a count variable. The number of children that a woman has given birth to is an example of a count variable. This number can only take the value of whole numbers (integers) such as 0, 1, 2, 3, and so on. It makes no sense if a respondent says they have given birth to 2.5 children. Count variables are discrete variables because only whole numbers are logical responses. A person’s height is an example of a continuous variable. It is true that we typically measure height only down to a certain level of precision, typically inches in the United States. We might think that if we were to measure a person’s height in inches, it would only take whole number values and therefore it is discrete. But limitations in measurement don’t define whether a variable is continuous or discrete. Rather the distinction is whether the value could be logically measured to any degree of accuracy. We often measure height out to half inches and we could imagine that if we have a precise enough measurement instrument, we could measure a person’s height out to any decimal level that we desired. So, it is perfectly sensible for someone to say they were 69.825467 inches tall, even though we might think they are being a bit tedious. Note that in both the height and the number of children examples, there are logical limits to the values. You can’t have negative children or height. There are no exact upper limits to the values that either can take, but we would likely think we have a data coding error if we saw a report of a 20 foot person or a woman who gave birth to 50 children. This is what I mean by the statement “within a range” above. Both discrete and continuous variables can be limited in the range of values that they can take. What distinguishes them from each other is what values they can logically take within that limited range. Categorical variables Categorical variables are not represented by numerical quantities but rather by a set of mutually exclusive categories to which observations can belong. The gender, race, political party affiliation, and highest educational degree of a person, the public/private status of a university, and the passenger class of a passenger on the Titanic are all examples of categorical variables. There are also two sub-types of categorical variables. Ordinal variables are categorical variables where the categories have an explicit ordered structure, while nominal variables are categorical variables where the categories are unordered. Highest educational degree is an example of an ordinal variables because it is ordered such that Graduate Degree &gt; BA Degree &gt; AA Degree &gt; High School Diploma &gt; No degree. Passenger class is also an ordinal variables that starts in Third class (or steerage - Think Leonardo DiCaprio) and ends in First class (think Kate Winslet), with a Second class in between. Race, gender, and political party affiliation are all examples of nominal variables. The categories here have no ordering to them. While some people might have their own political party preferences, these sort of normative evaluations of categories are irrelevant. For the same reason, even the variable of survival on the Titanic is a nominal variable. We don’t judge the value of life and death. "],
["what-can-we-do-with-data.html", "What Can We Do With Data?", " What Can We Do With Data? We now know what data looks like, but what do social scientists do with data? in the first part of this course, we will learn three fundamental data analysis tasks: analysis of the distribution of a single variable, measuring association, and statistical inference. In the final part of the course, we will build on these fundamentals to learn how to build more complex statistical models. How is a variable distributed? Sometimes, we just want to understand what a single variable “looks like.” We may simply be interested in its “average” or we may want to know something else, like how spread out the values of the variable are. In these cases, we calculate univariate (\"one variable) statistics on the distribution of a variable. Typically, univariate statistics aren’t as interesting to social scientists as the measures of association discussed below, but even then its often a good idea to look at univariate statistics to understand all of the variables in your research. In some cases, the calculation of a univariate statistics is the important question at hand. For example, when poll researchers try to figure out who is going to win an election, they are very much interested in the univariate distribution of support for each candidate, which gives the proportions of likely voters who intend to vote for each candidate. Here are some other questions we could ask in our data: How much variability is there in the amount of money that movies make? What percent of passengers survived the Titanic disaster? What is the average age of voters in the United States? Measuring association Social scientists are often most interested in the relationships, or association, between two or more variables. These associations allow us to test hypotheses about causal relationships between underlying social constructs. For example, we might be interested in whether divorce affected children’s well-being. In this case, we would want to look at the relationship between the categorical variable indicating whether a child’s parents were divorced and some measure of their well-being, such as feelings of stress, academic performance, etc. There are different ways of measuring association depending on the types of variables involved.. Here are some questions about association we could ask in our data: Did the probability of surviving the Titanic depend on passenger class? (categorical and categorical) Do the earnings of movies vary by genre? (quantitative and categorical) Is income inequality in a state related to its crime rate? (quantitative and quantitative) In the first part of the course, we will learn how basic measures of association between two variables, depending on what kind of variables we are using. In the final part of the course, we will return to this topic when we learn how to build more complex statistical models discussed below. Making statistical inferences If I told you that in my sample of twenty people, brown-eyed individuals make $5 more than all other eye colors combined, would you believe me? You probably shouldn’t, because in a sample of twenty people, even when drawn without systematic bias, weird results like this are not unlikely just by random chance. If I told you I observed this phenomenon on a well-drawn sample of 20,000 individuals, you would probably be more likely to believe me. The underlying concept here is called statistical inference. We often want to draw conclusions about the larger populations from which our samples are drawn. Statistical inference is the technique of quantifying how uncertain we are about whether our data are similar to the population or not. When you hear press reports on political polls with the term “margin of error,” they are referring to statistical inference. Many introductory statistics course focus most of their attention on statistical inference, partly because it is more abstract and complex. However, statistical inference is always secondary to the basic descriptive measures of univariate, bivariate, and multivariate statistics. Therefore, I spend considerably less time on this topic than in most statistics courses, so that we can focus on the more important stuff. Building Models Although our basic measures of association are useful, the most common tool in social science analysis is a statistical model in which the user can specify the relationships between variables by some kind of mathematical functions. In the final module of the course, we will learn how to build basic versions of these models that allow us to look at the relationships between multiple quantitative and categorical variables. This section will build on our prior work in all of the previous modules. We will specifically focus on two uses of statistical models. First, statistical models will allow us to “control” for other variables when we look at the association between any two variables. Controlling for other variables is important because they may be confounded with the relationship we want to measure. For example, we may be interested in the relationship between marital status (e.g. never married, married, widowed, divorced) and sexual frequency in our GSS data. However, these different groups vary significantly in their age. Never married individuals are much younger than all of the other groups and widowed individuals much older. Given the fact that sexual frequency tends to decline with age (something we will show later in this term), it seems problematic to just compare the average sexual frequency across these groups. This is because age confounds the relationship between marital status and sexual frequency. Statistical models will gives us tools to account for this problem and to get a better estimate of the relationship between marital status and sexual frequency, net of this confounder. Second, statistical models will allow us to account for how the relationship between two variables might differ depending on the context of a third variable. This is what we call an interaction. For example, lets say we were interested in the relationship between the number of sports played and a student’s popularity (measured by friend nominations) in our Add Health data. Because of gender norms, we might expect that this relationship is different for boys and girls. We can use statistical models to empirically examine whether this is true. This kind of contextualization is an important component of sociological practice. Observational Data, Experimental Thinking Much of the data that we use in sociology is observational rather than experimental. In an experimental design, the researcher randomly selects subjects to receive some sort of treatment and then observes how this treatment affects some outcome. Thus, the research engages in systematic manipulation to observe a response. In observational data, the researcher does not directly manipulate the environment but rather just observes and records the social setting as it is. Experimental data can be more powerful than observational data because the random assignment of a treatment through researcher manipulation strengthens claims of causation. If there is a relationship between treatment and response it can only come through a causal relationship or random chance. In observational data, the relationship between any two variables can be a result of a causal relationship, random chance,spuriousness. Spuriousness occurs when other variables produce the relationship between two other variables rather than them directly causing each other. The example above about marital status and sexual frequency is a simple example. If we note that widows have less sex than other people, we may be tempted to think that something about being widowed reduces someone’s sexual drive or their interactions with others. However, the more obvious explanation is that widows tend to be quite a bit older than other marital status groups and older people have less sex. Age is generating a spurious relationship between widowhood and sexual frequency. This kind of spuriousness is the reason for the frequent claim that “correlation is not causation.” There are two different philosophical approaches to the statistical analysis of observational data where spuriousness can be a problem. The first approach approaches it as pseudo-experimental data. The goal of this approach is to try to find ways to mimic the experimental design approach with observational data. At a basic level this can include “controlling” for other variables (which we will learn) and can extend to a variety of techniques of causal modeling that are intending to use some feature of the data to recover causation (which we will not learn). The second approach treats statistical analysis as a way to describe observed data in a formal, systematic, and replicable way. The goal is to establish to what extent the data are consistent with competing theories that seek to understand the outcome in question, rather than to mimic the experimental approach. Although quantitative and qualitative approaches are often seen as philosophically different approaches, this approach to observational data shares many features with more purely qualitative approaches to data analysis. This is the approach that I take in this course. "],
["the-distribution-of-a-variable.html", "The Distribution of a Variable", " The Distribution of a Variable The distribution of a variable refers to how the different values of that variable are spread out across observations. This distribution gives us a sense of what the most common values are for a given variable and how much these values vary. It can also alert us to unusual patterns in the data. Intuitively, we think about distributions in our personal life any time we think about how we “measure” up relative to everyone else on something (income, number of Facebook friends, GRE scores, etc.). We want to know where we “fall” in the distribution of one of these variables. In this chapter we will first learn graphical techniques that allow us to visualize what the distribution of a variable looks like. For quantitative variables we will then move on to calculate important summary statistics that measure the center and spread of a distribution. Slides for this module can be found here. "],
["looking-at-distributions.html", "Looking at Distributions", " Looking at Distributions One of the best ways to understand the distribution of a variable is to visualize that distribution with a graph. However, the technique we use to graph the distribution will depend on whether we have a categorical or a quantitative variable. For categorical variables, we will use a barplot, while for quantitative variables, we will use a histogram. Looking at the distribution of a categorical variable Calculating the frequency In order to graph the distribution of a categorical variable, we first need to calculate its frequency. The frequency is just the number of observations that fall into a given category of a categorical variable. We could, for example, count up the number of passengers who were in the various passenger classes on the Titanic. Doing so would give us the following: Table 2: Passengers on the Titanic by passenger class Passenger class Frequency First 323 Second 277 Third 709 Total 1309 There were 323 first class passengers, 277 second class passengers, and 709 third class passengers. Adding those numbers up gives us 1,309 total passengers. R will calculate these numbers for us easily using the table command: table(titanic$pclass) ## ## First Second Third ## 323 277 709 Frequency, proportion, and percent The frequency we just calculated is sometimes called the absolute frequency because it just counts the raw number of observations. However such raw numbers are not usually very helpful because they will vary by the overall number of observations. Instead, we typically want to calculate the proportion of observations that fall within each category. These proportions are sometimes also called the relative frequency. We can calculate the proportion by simply dividing our absolute frequency by the total number of observations: Table 3: Passengers on the Titanic by passenger class Passenger class Frequency Proportion First 323 323/1309=0.247 Second 277 277/1309=0.212 Third 709 709/1309=0.542 Total 1309 1.0 R provides a nice shorthand function titled prop.table to conduct this operation. The prop.table command should be run on the output from a table command. prop.table(table(titanic$pclass)) ## ## First Second Third ## 0.2467532 0.2116119 0.5416348 Note that I have “wrapped” the prop.table command around the table command here to do this calculation in a single line. We often convert proportions to percents which are more familiar to most people. To convert a proportion to a percent, just multiply by 100: Table 4: Passengers on the Titanic by passenger class Passenger class Frequency Proportion Percent First 323 323/1309=0.247 0.247*100=24.7% Second 277 277/1309=0.212 0.212*100=21.2% Third 709 709/1309=0.542 0.542*100=54.2% Total 1309 1.0 100% 24.7% of passengers were first class, 21.2% of passengers were second class, and 54.2% of passengers were third class. Just over half of passengers were third class and the remaining passengers were fairly evenly split between first and second class. Don’t make a piechart Now that we have proportions/percents, we can use these values to construct a graphical display of the distribution. One of the most common techniques for doing this is a piechart. Figure 1 shows a piechart of the distribution of passengers on the Titanic. Figure 1: Piechart of passenger class distribution on Titanic You will notice that I do not show you the code for constructing this piechart. I hid this code because I don’t ever want you to construct a piechart. Despite their popularity, piecharts are a poor tool for visualizing distributions. In order to judge the relative size of the slices on a piechart, your eye has to make judgments in two dimensions and with an unusual pie-shaped slice (\\(\\pi*r^2\\) anyone?). As a result, it can often be difficult to decide which slice is bigger and to properly evaluate the relative sizes of each of the slices. In this case, for example, the relative size of first and second class are quite close and it is not immediately obvious which category is larger. Make a barplot A better way to display the distribution is by using a barplot in which vertical or horizontal bars give the proportion or percent. Figure 2 shows a barplot for the distribution of passengers on the Titanic. ggplot(titanic, aes(x=pclass, y=..prop.., group=1))+ geom_bar()+ labs(x=&quot;passenger class&quot;, y=&quot;percent&quot;)+ scale_y_continuous(labels=scales::percent)+ coord_flip()+ theme_bw() Figure 2: Barplot of passenger class distribution on the Titanic Figure 2 is our first code example of the ggplot library that we will use to make figures in R. I will discuss how this code works below, but first I want to focus on the figure itself. Unlike the piechart, our eye only has to work in one dimension (in this case, horizontal). We can clearly see that third class is the largest passenger class with slightly more than 50% of all passengers. We also can see easily determine visually that third class passengers are at more than twice as common as either of the other two categories, simply by comparing the height of the bars. We can also see that slightly more passengers were in first class than second class simply by comparing the heights of those two bars. So how did I create this graph? The ggplot syntax is a little different than most of the other R syntax we will look at this term. With ggplot, we add several commands together with the + sign to create our overall plot. Each command adds a “layer” to our plot. These layers are: ggplot(titanic, aes(x=pclass, y=..prop.., group=1)): The first command is always the ggplot command itself which defines the dataset we will use (in this case, titanic) and the “aesthetics” that are listed in the aes argument. In this case, we defined an x (horizontal) variable as the pclass variable itself and the y (vertical) variable as a proportion (which in ggplot-speak is ..prop..). The group=1 argument is a trick for barplots that makes sure our proportions add up to 1 across all categories. geom_bar(): This command defines what geometric shape (in this case a bar) to actually plot. These two layers would be enough for a figure, but I also add four more layers that make for a nicer looking figure. You can try the command above with only the first two elements to see how it is different. labs(x=\"passenger class\", y=\"percent\"): The labs command allows me to add a variety of nice labels to my graph. In this case I labeled my x and y axes. scale_y_continuous(labels=scales::percent): This is not a necessary command but it is useful to get better labels on my y-axis tickmarks. Without this command, the tickmarks would just show proportions (e.g. 0.2, 0.4). This command allows me define custom labels for those tickmarks that show them as percents rather than proportions. coord_flip: This does exactly what it sounds like. It flips the x and y axes of the graph. This causes the bars to display horizontally rather than vertically. This is not necessary, but is often a useful feature to avoid problems with long category names overlapping on the x-axis. Note that even though passenger class looks like it is on the y-axis, ggplot still treats it as the “x” variable for things like defining aesthetics and labeling with the labs layer. theme_bw(): This last command is for the “theme” layer that just defines a variety of characteristics for the overall look of the figure. I like the theme_bw (bw for black and white) theme over the default theme that comes with ggplot. Ggplot is a very flexible system that repeats these same basic layer elements in all the graphs it creates. In an appendix to this book, you can see cookbook examples of all the ways we will we use it to create figures throughout the term. Looking at the distribution of a quantitative variable Barplots won’t work for quantitative variables because quantitative variables don’t have categories. However, we can do something quite similar with the histogram. One way to think about the histogram is that we are imposing a set of categories on a quantitative variable by breaking our quantitative variable into a set of equally wide intervals that we call bins. The most important decision with a quantitative variable is how wide to make these bins. As an example, lets take the age variable from our politics dataset. I could break this variable into five-year intervals that go from 0-5, 5-10, 10-15, 15-20, and so on. Alternatively, I could use 10-year intervals from 0-9, 10-19, 20-29, and so on. I could also use any other interval I like such as 1-year, 3-year, and so on. Lets use 10-year intervals for this example. I then just have to count up the number of observations that fall into each 10 year interval. Table 5: Age distribution in politics datatset in 10 year groups Age Group Frequency 0-9 0 10-19 127 20-29 616 30-39 756 40-49 650 50-59 822 60-69 745 70-79 369 80-89 153 Because the survey was only administered to adults, I have zero individuals from 0-9 and only a few in the 10-19 age range. Now that we have the frequencies for each bin, we can plot the histogram. The histogram looks much like a barplot except for two important differences. First, on the x-axis we have a numeric scale for the quantitative variable rather than categories. Second, we don’t put any space between our bars. Below I show some R code for producing the histogram shown in Figure 3 using ggplot. This code is simpler than the barplot code. We only need to define an x aesthetic in our ggplot command. Instead of a geom_bar we use a geom_histogram. I can specify different bin widths in this command with the binwidth argument. Here I have specified a binwidth of ten years. I have also specified two different colors. The col argument is the color of the border for each bar and the fill argument is the color of the bars themselves. ggplot(politics, aes(x=age))+ geom_histogram(binwidth=10, col=&quot;black&quot;, fill=&quot;grey&quot;)+ labs(x=&quot;age&quot;)+ theme_bw() Figure 3: Histogram of age in the politics data Looking at Figure 3, I can see the peak in the distribution in the 50’s (the baby boomers) with a long fat tail to the right and a steeper drop off at older ages, due to smaller cohorts and old age mortality.I can also sort of see a smaller peak in the 25-34 range which is largely the children of the baby boomers. What would this histogram look like if I had used 5-year bin widths instead of 10-year bins? Lets try it: ggplot(politics, aes(x=age))+ geom_histogram(binwidth=5, col=&quot;black&quot;, fill=&quot;grey&quot;)+ labs(x=&quot;age&quot;)+ theme_bw() Figure 4: Histogram of age in the politics data with five year bins As Figure 4 shows, I get more or less the same overall impression but a more fine-grained view. I can more easily pick out the two distinct “humps” in the distribution that correspond to the baby boomers and their children. Sometimes adjusting bin width can reveal or hide important trends and sometimes it can just make it more difficult to visualize the distribution. As an exercise, you can play around with the interactive example below. What kinds of things should you be looking for in your histogram? There are four general things to be on the lookout for when you examine a histogram. Center Where is the center of the distribution? Loosely we can think of the center as the peak in the data, although we will develop some more technical terms for center in the next section. Some distributions might have more than one distinct peak. When a distribution has one peak, we call it a unimodal distribution. Figure 5 shows a clear unimodal distribution for the runtime of movies. We can clearly see that the peak is somewhere between 90 and 100 minutes. When a distribution has two distinct peaks, we call it a bimodal distribution. Figure 6 shows that the distribution of violent crime rates across states is bimodal. The first peak is around 2200-2700 crimes per 100,000 and the second is around 3500 crimes per 100,000. This suggests that there are two distinctive clusters of states: in the first cluster are states with a moderate crime rate and in the second cluster, states with high crime rate states. You can probably guess what we call a distribution with three peaks (and so on), but its fairly rare to see more than two distinct peaks in a distribution. Figure 5: The distribution of movie runtimes is unimodal with one clear peak around 90-100 minutes Figure 6: The distribution of property crimes by states is bimodal with a two separate peaks. Shape Is the shape symmetric or is one of the tails longer than the other? When the long tail is on the right, we refer to this distribution as right skewed. When the long tail is on the left, we refer to the distribution as left skewed. Figures 7 and 8 show examples of roughly symmetric and heavily right-skewed distributions, respectively, from the movies dataset. The Tomato Rating that movies receive (a score from 1-10) is roughly symmetric with about equal numbers of movies above and below the peak. Box office returns on the other hand are heavily right-skewed. Most movies make less than $100 million at the box office but there are few “blockbusters” that rake in far more. Right-skewness to some degree or another is common in social science data, partially because many variables can’t logically have values below zero and thus the left tail of the distribution is truncated. Left-skewed distributions are rare. In fact, they are so rare that I don’t really have a very good example to show you from our datasets. Figure 7: The distribution of movie tomato ratings is roughly symmetric Figure 8: The distribution of movie box office returns is heavily right skewed Spread How spread out are the values around the center? Do they cluster tightly around the center or are they spread out more widely? Typically this is a question that can only be asked in relative terms. We can only say that the spread of a distribution is larger or smaller than some comparable distribution. We might be interested for example in the spread of the income distribution in the US compared to Sweden, because this spread is one measure of income inequality. Figure 9 compares the distribution of movie runtime for comedy movies and sci-fi/fantasy movies. The figure clearly shows that the spread of movie runtime is much greater for sci-fi/fantasy movies. Comedy movies are all tightly clustered between 90 and 120 minutes while longer movies are more common for sci-fi/fantasy movies leading to a longer right tail and a correspondingly higher spread. Figure 9: The distribution of movie runtime is much more spread out for sci-fi/fantasy films than it is for comedies. Outliers Are there extreme values which fall outside the range of the rest of the data? We want to pay attention to these values because they may have a strong influence on the statistics that we will learn to calculate to summarize a distribution. They might also influence some of the measures of association that we will learn later. Finally, extreme outliers may help identify data coding errors. In Figure 6 above, we can see a clear outlier in the violent crime rate for Washington DC. Washington DC’s crime rate is such an outlier relative to the other states that we will pay attention to it throughout this term as we conduct our analysis. Figure 9 above shows that Peter Jackson’s Return of the King was an outlier for the runtime of sci-fi/fantasy movies, although it doesn’t seem quite as extreme as for the case of Washington DC. In large datasets, it can sometimes be very difficult to detect a single outlier on a histogram because the height of its bar will be so small. Later in this chapter, we will learn another graphical technique called the boxplot that can be more useful for detecting outliers in a quantitative variable. "],
["measuring-the-center-of-a-distribution.html", "Measuring the Center of a Distribution", " Measuring the Center of a Distribution When we look at a distribution, we often can get an intuitive sense of where its “center” is. But what do we really mean by the term “center?” The notion of center often allows us to think about the value we expect a typical or “average” observation to have, but there are multiple ways of defining this center. In statistics, three different measures of center are used: the mean, median, and mode. The mean The mean is the measure most frequently referred to as the “average” although that term could apply to the median and mode as well. The mean is the balancing point of a distribution. Imagine trying to balance a distribution on your finger like a basketball. Where along the distribution would you place your finger to achieve this balance? This point is the mean. It is equivalent to the concept of “center of mass” from physics. The calculation of the mean is straightforward: Sum up all the values of your variable across all observations. Divide this sum by the number of observations. As an example, lets take a sub-sample of our movie data. I am going to select all the romances (not including rom-coms which are coded as comedies) produced in 2013. There were nine “pure” romances in 2013. I want to know their mean Tomato Meter rating. Table 6: Tomato meter of pure romance movies released in 2013 Title Tomato Meter Kill Your Darlings 77 I’m in Love with a Church Girl 6 Safe Haven 12 The Face of Love 43 Love and Honor 13 Before Midnight 98 Drinking Buddies 83 Ain’t Them Bodies Saints 79 The Great Gatsby 49 Sum 460 In the very last row, I show the sum of the Tomato Meter rating which simply sums up the Tomato Meter rating of each movie. To calculate the mean, I simply divide this sum by the number of movies which is 9. \\[\\bar{x}=\\frac{460}{9}=51.11\\] The mean Tomato Meter rating for romantic movies in 2013 was 51.11. Notice the funny \\(x\\) with a bar over it (“x bar”). Mathematically, we often represent variables with lower-case roman letters like \\(x\\) or \\(y\\). Putting the bar above the letter is the way to mathematically signify the mean of that variable. Since we are discussing math symbols, lets talk about creating a mathematical formula for what we just did. In order to do that, I need to introduce a variety of mathematical symbols, but don’t get frightened. We are just formalizing the method of calculating the mean that I just demonstrated above. First, as I said we represent a given variable by a lower-case roman letter, such as \\(x\\). If we want to specify a particular observation of \\(x\\), we use a subscript number. So \\(x_1\\) is the value of the first observation of \\(x\\) in our data and \\(x_{25}\\) is the value of the 25th observation of \\(x\\) in our data. We use the letter n to signify the number of observations so \\(x_n\\) is always the last observation of \\(x\\) in our data. Now we need some way to indicate “sum up all the values of \\(x\\).” This is given by the summation sign which looks as follows: \\[\\sum_{i=1}^n x_i\\] In English, this just means “sum up all the values of \\(x\\), starting at \\(x_1\\) and ending at \\(x_n\\).” That gives us our sum, which we just need to divide by the number of observations, \\(n\\). \\[\\bar{x}=\\frac{\\sum_{i=1}^n x_i}{n}\\] In R, the mean is very straightforward to calculate. Lets calculate the mean of the Tomato Meter rating for all movies in our dataset: sum(movies$TomatoMeter)/nrow(movies) ## [1] 47.77595 The sum command calculates the sum and the nrow command calculates the number of rows of our dataset, which is equivalent to the number of observations. Or we could go even simpler and just use the mean command: mean(movies$TomatoMeter) ## [1] 47.77595 We can see that the mean Tomato Meter of romantic movies in 2013 (51.1) were slightly above the mean for all the movies in our dataset (47.8), but not by much. One last thing to note is that it only makes sense to calculate the mean of quantitative variables. You cannot add up the values for categorical variables because categorical values don’t have numeric values. Rather, they have categories. Notice that you will get an “NA” value and a warning if you try to do this using the mean command: mean(movies$Rating) ## Warning in mean.default(movies$Rating): argument is not numeric or logical: ## returning NA ## [1] NA The median The median is almost as widely used as the mean as a measure of center, for reasons I will discuss below. The median is the midpoint of the distribution. It is the point at which 50% of observations have lower values and 50% of the observations have higher values. In order to calculate the median, we need to first order our observations from lowest to highest. Lets do that with the romantic movie data above. Table 7: Tomato meter of pure romance movies released in 2013, sorted from worst to best Title Tomato Meter Order I’m in Love with a Church Girl 6 1 Safe Haven 12 2 Love and Honor 13 3 The Face of Love 43 4 The Great Gatsby 49 5 Kill Your Darlings 77 6 Ain’t Them Bodies Saints 79 7 Drinking Buddies 83 8 Before Midnight 98 9 To find the median, we have to find the observation right in the middle. When we have an odd number of observations, finding the exact middle is easy. In this case, it is given by the 5th observation because it has four observations lower than it and four observations higher than it. So the median Tomato Meter rating for romantic movies in 2013 was 49. When you have an even number of observations, then finding the median is slightly more tricky because you have no single observation that is exactly in the middle. In this case, you find the two observations that are closest to the middle and calculate their mean (sum them up and divide by two). For example, if we had ten observations, we would take the mean of the 5th and 6th observations to calculate the median. The median command in R will calculate the median for you. median(movies$TomatoMeter) ## [1] 47 In this particular case, the mean (47.8) and median (47) of the Tomato Meter produced very similar measures of center, but this isn’t always the case as I will demonstrate below. As for the mean, it makes no sense to calculate the median of a categorical variable. The mode The mode is the least used of the three measures of center. In fact, it is so infrequently used that R does not even have a built-in function to calculate it. The mode is the high point or peak of the distribution. When you look at a distribution graphically, the mode is what your eye is drawn to as a measure of center. Calculating the mode however is much trickier. Simply speaking, the mode is the most common value in the data. However, when data are recorded down to very precise decimal levels, this can be a misleading number. Furthermore, there may be multiple “peaks” in the data and so speaking of a single mode can be misleading. In the sample of 2013 romance movies, no value was repeated and so there is no legitimate value for the mode. In the case of the tomato meter distribution for the movies dataset, we have a tie for the most common value. 41 movies had Tomato Meter ratings of 29, 33, and 51, respectively. Figure 10 shows that the distribution of tomato meter ratings lacks any real peak, with a relatively even distribution across most values. ggplot(movies, aes(x=TomatoMeter))+ geom_histogram(binwidth=5, col=&quot;black&quot;, fill=&quot;grey&quot;)+ labs(x=&quot;age&quot;)+ geom_vline(xintercept = c(29,33,51), size=2, color=&quot;red&quot;)+ theme_bw() Figure 10: Histogram of Tomato Meter ratings from politics dataset with three candidates for the mode drawn in red Interestingly, while it does not make sense to think about a mean or a median for quantitative variables, we can use the idea of the mode for categorical variables. The most common category is often referred to as the modal category. If you want to quickly identify the modal category for a categorical variable, you can wrap a table command inside a sort command with the decreasing=TRUE argument to see the top category at the right. sort(table(movies$Rating), decreasing = TRUE) ## ## R PG-13 PG G ## 1143 991 363 56 R-Rated movies are the modal category for maturity rating in our movies dataset. Comparing the mean and median How can the mean and median give different results? Remember that the mean defines the balancing point and the median defines the midpoint. If you have a perfectly symmetric distribution, then these two points are the same because you would balance the distribution at the midpoint. However, when the distribution is skewed in one direction or another, the mean and the median will be different. In order to maintain balance, the mean will be pulled in the direction of the skew. When you have heavily skewed distributions, this can lead to dramatically different values for the mean and median. Lets look at this phenomenon for a couple of variables in the movie dataset. As the histogram above showed, the Tomato Meter variable is fairly symmetric and as a result we end up with a mean (47.8) and median (47) that are pretty close. Figure 11 shows the distribution of movie runtime which is somewhat more right-skewed. Figure 11: Distribution of movie runtime with mean (105.2) and median (102) shown as vertical lines The skew here is not too dramatic, but it pulls the mean about 3 minutes higher than the median. Figure 12 shows the distribution of box office returns to movies which is heavily right skewed. Most movies make moderate amounts of money, and then there are a few star performers that make bucket loads of cash. Figure 12: Distribution of movie box office returns with mean (45.2) and median (21.6) shown as vertical lines As a result of this skew, the mean box office returns are about $45.2 million, while the median box office returns are about $21.6 million. The mean here is more than double the median! Note that neither estimate is in some fundamental way incorrect. They are both correctly estimating what they were intended to estimate. It is up to us to understand and interpret these numbers correctly and to understand their limitations. In many cases, we are actually more interested in the median as a measure of “average” experience than the mean, even though we think of the mean as the “average.” This is, for example, why you see home prices in an area always reported in terms of medians rather than means. Mean home prices tend to be much higher than median home prices because of the relatively few very expensive homes in a given area. "],
["percentiles-and-the-five-number-summary.html", "Percentiles and the Five Number Summary", " Percentiles and the Five Number Summary In this section, we will learn about the concept of percentiles. Percentiles will allow us to calculate a five number summary of a distribution and introduce a new kind of graph for describing a distribution called the boxplot. Percentiles We have already seen one example of a percentile. The median is the 50th percentile of the distribution. It is the point at which 50% of the observations are lower and 50% are higher. We can actually use this same logic to calculate other percentiles. We could calculate the 25th percentile of the distribution by finding the point where 25% of the observations are below and 75% are above. We could even calculate something like the 43rd percentile if we were so inclined. We calculate percentiles in a fashion similar to the median. First, sort the data from lowest to highest. Then, find the exact observation where X% of the observations fall below to find the Xth percentile. In some cases, there might not be an exact observation that fits this description and so you may have to take the mean across the two closest numbers. The quantile command in R will calculate percentiles for us in this fashion (quantile is a synonym for percentile). In addition to telling the quantile command which variable we want the percentiles of, we need to tell it which percentiles we want. In the command below, I ask for the 27th and 57th percentile of age in our sexual frequency data. quantile(sex$age, p=c(.27,.57)) ## 27% 57% ## 32 46 27% of the sample were younger than 32 years of age and 57% of the sample were younger than 46 years of age. The five number summary We can split our distribution into quarters by calculating the minimum(0th percentile), the 25th percentile, the 50th percentile (the median), the 75th percentile, and the maximum (100th percentile). Collectively, these percentiles are known at the quartiles of the distribution (not to be confused with quantile) and are also described as the five number summary of the distribution. We can calculate these quartiles with the quantile command. If I don’t enter in specific percentiles, the quantile command will give me the quartiles by default: quantile(sex$age) ## 0% 25% 50% 75% 100% ## 18 31 43 56 89 The bottom 25% of respondents are between the ages of 18-31. The next 25% are between the ages of 31-43. The next 25% are between the ages of 43-56. The top 25% are between the ages of 56-89. We can also use this five number summary to calculate the interquartile range (IQR) which is just the difference between the 25th and 75th percentile. This gives us a sense of how spread out observations are. In this data: \\[IQR=56-31=25\\] So, the 25th and 75th percentile of age are separated by 25 years. Boxplots We can also use this five number summary to create another graphical representation of the distribution called the boxplot. Figure 13 below shows a boxplot for the age variable from the sexual frequency data. Figure 13: Boxplot of respondent’s age in sexual frequency data The “box” in the boxplot is drawn from the 25th to the 75th percentile. The height of this box is equal to the interquartile range. The median is drawn as a thick bar within the box. Finally, “whiskers” are then drawn to the minimum and maximum of the data. Sometimes, the whiskers are drawn to less than the minimum and maximum if these values are very extreme and instead the whiskers are drawn out to 1.5xIQR in length and then individual points are plotted. In this case there were no extreme values, so the whiskers were drawn all the way out to the actual maximum and minimum. The boxplot provides many pieces of information. It shows the center of the distribution as measured by the median. It also gives a sense of the spread of the distribution and extreme values by the height of the box and whiskers. It can also show skewness in the distribution depending on where the median is drawn within the box and the size of the whiskers. If the median is in the center of the box, then that indicates a symmetric distribution. If the median is towards the bottom of the box, then the distribution is right-skewed. If the median is towards the top of the box, then the distribution is left-skewed. I have not yet shown you how to make a boxplot using ggplot. The code for Figure 13 is shown below. ggplot(sex, aes(x=&quot;&quot;, y=age))+ geom_boxplot(fill=&quot;skyblue&quot;)+ labs(x=NULL)+ theme_bw() Most of this code is straightforward. We use the y aesthetic to indicate the variable we want the boxplot for and we use the geom_boxplot command to graph the boxplot (in this case the fill argument can be used to specify a color choice for the box of the boxplot). The only unusual thing here is the use of x=\"\" in the top-level aesthetics and the use of x=NULL in the labs command. These additions are not strictly necessary but they do cause the horizontal x-scale on the graph to be suppressed. Otherwise we would see some non-intuitive numbers here. The exercise below allows you to adjust a slider to see different percentiles on both a histogram and a boxplot. In general, boxplots for a single variable do not contain as much information as a histogram and so are generally inferior for understanding the full shape of the distribution. The real advantage of boxplots will come in the next module when we learn to use comparative boxplots to make comparisons of the distribution of a quantitative variable across different categories of a categorical variable. "],
["measuring-the-spread-of-a-distribution.html", "Measuring the Spread of a Distribution", " Measuring the Spread of a Distribution The second most important measure of a distribution is its spread. Spread indicates how far individual values tend to fall from the center of the distribution. As Figure 14 below shows, two distributions can have the same center and general shape (in this case, a bell curve) but have very different spreads. Figure 14: Two different distributions with the same mean but very different spreads, based on simulated data. Range and interquartile range One of the simplest measures of spread is to calculate the range. The range is the distance between the highest and lowest value. Lets take a look at the range in the fare paid (in British pounds) for tickets on the Titanic. The summary command, will give us the information we need: summary(titanic$fare) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 7.896 14.454 33.276 31.275 512.329 Note that at least one person made it on the Titanic for free. The highest fare paid was 512.3 pounds. So the range is easy to calculate 512.3 - 0 = 512.3. The difference between the highest and lowest paying passenger was about 512 pounds. This example also reveals the shortcoming of the range as a measure of spread. If there are any outliers in the data, they are going to show up in the range and so the range may give you a misleading idea of how spread out the values are. Note that the 75th percentile here is only 31.28 pounds, which would suggest that the 512.3 maximum is a pretty high outlier. We can check this by graphing a boxplot, as I have done in Figure 15. ggplot(titanic, aes(x=&quot;&quot;, y=fare))+ geom_boxplot(fill=&quot;seagreen&quot;, outlier.color = &quot;red&quot;)+ labs(x=NULL, y=&quot;fare paid in British pounds&quot;)+ theme_bw() Figure 15: Boxplot of fare paid on the Titanic The maximum value is such an outlier that the rest of the boxplot has to be “scrunched” in order to fit it all into the graph. Clearly this is not a good indicator of spread. However, we have already seen a better measure of spread using a similar idea: the interquartile range or IQR. The IQR is just the range between the 25th and 75th percentile. We already have these numbers from the output above, so the IQR = 31.28-7.90=23.38. So, the difference in fare between the 25th and 75th percentile (the middle 50% of the data) was 23.4 pounds. That result suggests a much smaller spread of fares. You can also use the IQR command in R to directly calculate the IQR. IQR(titanic$fare) ## [1] 23.3792 Variance and standard deviation The most common measure of spread is the variance and its derivative measurement, the standard deviation. It is so common in fact, that most people simply refer to the concept of “spread” as “variance.” The variance can be defined as the “average squared distance to the mean.” Of course, “squared distance” is a bit hard to think about, so we more commonly take the square root of the variance to get the standard deviation which gives us the “average distance to the mean.” Imagine if you were to randomly pick one observation from your dataset and guess how far it would be from the mean. Your best guess would be the standard deviation. The calculation for the variance and standard deviation is a bit intimidating but we will break it down into steps to show it is not that hard. At the same time, I will show you to calculate the parts in R using the fare variable from the Titanic data. The overall formula for the variance (which is represented as \\(s^2\\)) is: \\[s^2=\\frac{\\sum_{i=1}^n (x_i-\\bar{x})^2}{n-1}\\] That looks tough, but lets break it down. The first step is this: \\[(x_i-\\bar{x})\\] You take each value of your variable \\(x\\) and subtract the mean from it. This can be done in R easily: diffx &lt;- titanic$fare-mean(titanic$fare) This measure gives us a description of how far each observation is from the mean which is already kind of a measure of spread, but we can’t do much with it yet because some differences are positive (higher than the mean) and some are negative (lower than the mean). In fact, if we take the mean of these differences, it will be zero by definition because this is what it means for the mean to be the balancing point of the distribution. round(mean(diffx),5) ## [1] 0 The next step is: \\[(x_i-\\bar{x})^2\\] We just need to square the differences. This will get rid of our negative/positive problem, because the squared values will all be positive. diffx.squared &lt;- diffx^2 The next step is: \\[\\sum_{i=1}^n (x_i-\\bar{x})^2\\] We need to sum up all of our values. This value is sometimes called the sum of squared X or SSX for short. It is already pretty close to a measure of variance already. ssx &lt;- sum(diffx.squared) The more distance there is from the mean on average, the larger this value will be. However, it also gets larger when we have more values because we are just taking a sum. To get a number that is comparable across different number of observations, we need to do the final step: \\[s^2=\\frac{\\sum_{i=1}^n (x_i-\\bar{x})^2}{n-1}\\] We are going to divide our SSX value by the number of observations minus one. The “minus one” thing is a bit tricky and I don’t want to get into the details of why we do it here. When n is large, this will have little effect and basically you are taking an average of the squared distance from the mean. variance &lt;- ssx/(length(diffx)-1) variance ## [1] 2677.398 So the average squared distance from the mean fare is 2677.39 pounds squared. Of course, this isn’t a very interpretable number, so its probably better to square root it and get the standard deviation: sqrt(variance) ## [1] 51.74358 So, the average distance from the mean fare is 51.74 pounds. Note that I could have used the power of R to do this entire calculation in one line: sqrt(sum((titanic$fare-mean(titanic$fare))^2)/(length(titanic$fare)-1)) ## [1] 51.74358 Alternatively, I could have just used the sd command to have R do all the heavy lifting: sd(titanic$fare) ## [1] 51.74358 "],
["measuring-association-1.html", "Measuring Association", " Measuring Association Measuring association between variables, in some form or another, is really what social scientists use statistics for the most. Establishing whether two variables are related to one another can help to affirm or cast doubt on theories about the social world. Does substance use improve an adolescent’s popularity in school? Does increasing wealth in a country lead to more or less environmental degradation? Does income inequality inhibit voter participation? These are just a few of the questions one could ask that require a measurement of the association between two or more variables. In this module, we will learn about how to visualize and measure association between two variables. How we do this depends on the kinds of variables that we have. There are three possibilities: two categorical variables one categorical and one quantitative variable two quantitative variables Each of these cases requires that we learn and master different techniques. In the three sections that follow, we will learn how to measure association for all three cases. Slides for this module can be found here. "],
["the-two-way-table.html", "The Two-Way Table", " The Two-Way Table The two-way table (also known as a cross-tabulation or crosstab) gives the joint distribution of two categorical variables. Lets use our politics dataset to construct a two-way table of belief in anthropogenic climate change by political party: tab &lt;- table(politics$party, politics$globalwarm) tab ## ## No Yes ## Democrat 230 1235 ## Republican 577 664 ## Independent 326 1055 ## Other 47 104 The two-way table gives us the joint distribution of the two variables, which is the number of respondents who fell into both categories. For example, we can see that 234 democrats did not believe in anthropogenic climate change while 1230 did. From this table, we can also calculate the marginal distribution of each of the variables, which are just the distributions of each of the variables separately. We can do that by adding up across the rows and down the columns: Table 8: Two-way table of party affiliation by climate change belief, ANES data 2016 Deniers Believers Total Democrat 230 1235 230+1235=1465 Republican 577 664 577+664=1241 Independent 326 1055 326+1055=1381 Other 47 104 47+104=151 Total 230+577+326+47=1180 1235+664+1055+104=3058 4238 The marginal distribution of party affiliation is given by the Total column on the right and the marginal distribution of climate change belief is given by the Total row at the bottom. Looking at the column marginal, we can see that there were a total of 1465 Democrats, 1241 Republicans, and so on. Looking at the row marginal, we can see that there were 1180 anthropogenic climate change deniers and 3058 anthropogenic climate change believers. The final number (4238) in the far right corner is the total number of respondents altogether. You can get this number by summing up the column marginals (1180+3058) or row marginals (1465+1241+1381+151). The margin.table command in R will also calculate marginals for us. I can use the margin.table command on the table I created and saved above as tab to calculate the same marginals as above. Note that you need to indicate which marginal you want by a number, where 1=row and 2=column, as the second option to margin.table: margin.table(tab,1) ## ## Democrat Republican Independent Other ## 1465 1241 1381 151 margin.table(tab,2) ## ## No Yes ## 1180 3058 The two-way table provides us with evidence about the association between two categorical variables. To understand what the association looks like, we will learn how to calculate conditional distributions. Conditional distributions To this point, we have learned about the joint and marginal distributions in a two-way table. In order to look at the relationship between two categorical variables, we need to understand a third kind of distribution: the conditional distribution. The conditional distribution is the distribution of one variable conditional on being in a certain category of the other variable. In a two-way table, there are always two ways to calculate a conditional distribution. In our case, we could look at the distribution of climate change belief conditional on party affiliation, or we could look at the distribution of party affiliation conditional on climate change belief. Both of these distributions really give us the same information about the association, but sometimes one way is more intuitive to understand. In this case, I am going to start with the former case and calculate the distribution of climate change belief conditional on party affiliation. This conditional distribution is basically given by the rows of our two-way table, which give the number of individuals of a given party who fall into each belief category. For example, the distribution of denial/belief among Democrats is 429 and 1932, while among Republicans, this distribution is 708 and 681. However, these two rows are not directly comparable as they are because Republicans are a much smaller group than Democrats. Thus, even if the shares were very different between the two groups, the absolute numbers for Republicans would probably be smaller for both categories. In order to make these rows comparable, we need the proportion of each party that falls into each belief category. In order to do that, we need to divide our rows through by the marginal distribution of party affiliation, like so: Table 9: Calculation of distribution of belief conditional on party affiliation Deniers Believers Total Democrat 230/1465 1235/1465 1465 Republican 577/1241 664/1241 1241 Independent 326/1381 1055/1381 1381 Other 47/151 104/151 151 Note that each row gets divided by its row marginal. If we do the math here, we will come out with the following proportions: ## Warning in base::cbind(...): number of rows of result is not a multiple of ## vector length (arg 2) Table 10: Distribution of belief conditional on party affiliation Deniers Believers Total Democrat 0.157 0.843 1 Republican 0.465 0.535 1 Independent 0.236 0.764 1 Other 0.311 0.689 1 Note that the proportions should add up to 1 within each row because we are basically calculating the share of each row that belongs to each column category. To understand these conditional distributions, you need to look at the numbers within each row. For example, the first row tells us that 15.7% of Democrats are deniers and 84.3% of Democrats are believers. The second row tells us that 46.5% of Republicans are deniers and 53.5% of Republicans are believers. We can tell if there is an association between the row and column variable if these conditional distributions are different across rows. In this case, they are clearly very different. About 84.3% of Democrats are believers while only about half (53.5%) of Republicans are believers. About 76.4% of Independents are believers, while about 68.9% of members of other parties are believers. We can use the prop.table command we saw in the last module to estimate these conditional distributions. In order to do that we feed in the crosstab we calculated with tab and one additional argument that indicates which dimension (row or column ) we want to condition across. Like margin.table a value of 1 will condition on rows (rows sum to 1) and a value of 2 will condition on columns (columns sum to 1). If we condition on rows here, we will get the same table as above. prop.table(tab,1) ## ## No Yes ## Democrat 0.1569966 0.8430034 ## Republican 0.4649476 0.5350524 ## Independent 0.2360608 0.7639392 ## Other 0.3112583 0.6887417 Its important to remember which way you did the conditional distribution and get the interpretation correct. If you are not sure, just note which way the proportions add up to one - this is the direction you should be looking (i.e. within row or column). In this case, I am looking at the distribution of variables within rows, so the proportions refer to the proportion of respondents from a given political party who hold a given belief. But, I could have done my conditional distribution the other way: prop.table(tab,2) ## ## No Yes ## Democrat 0.19491525 0.40385873 ## Republican 0.48898305 0.21713538 ## Independent 0.27627119 0.34499673 ## Other 0.03983051 0.03400916 Note that this table looks deceptively similar to the table above. But look again. The numbers now don’t add up to one within each row. They do however add up to one within each column. In order to read this table properly, we have to understand that it is giving us the distribution within each column: the proportion of respondents who have a given belief who belong to a given political party. So we can see in the first number that 19.5% of deniers are Democrats, 48.2% are Republicans, 27.9% are Independents, and 4.1% belong to other parties. This distribution is very different from the party affiliation distribution of believers in the second column which tells us that there is an association. However, the large party cleavages on the issue are not as immediately obvious here as they were with the previous conditional distribution. Always think carefully about which conditional distribution is more sensible to interpret and always make sure that you are interpreting them in the correct way. It is also possible to graph the conditional distribution as a set of barplots. To do that, we will learn a new feature of ggplot called faceting. Faceting allows us to make the same plot based on subsets of the data in a series of panels. In this case, we can use the code for a univariate barplot but faceted by First, lets save the output of our prop.table into a new object. ggplot(politics, aes(x=globalwarm, y=..prop.., group=1))+ geom_bar()+ facet_wrap(~party)+ scale_y_continuous(labels = scales::percent)+ labs(x=&quot;belief in anthropogenic climate change&quot;, y=NULL)+ theme_bw() Figure 16: Distribution of belief in anthropogenic climate change by party affiliation, ANES 2016 Figure 16 is a comparative barplot. This is our first example of a graph that looks at a relationship. Each panel shows the distribution of climate change beliefs for respondents with that particular party affiliation. What we are interested in is whether the distribution looks different across each panel. In this case, because there were only two categories of the response variable, we only really need to look at the heights of the bar for one category to see the variation across party affiliation, which is substantial. Lets try an example with more than two categories. For this example, I want to know whether there was a difference in passenger class by gender on the Titanic. I start with a comparative barplot: ggplot(titanic, aes(x=pclass, y=..prop.., group=1))+ geom_bar()+ facet_wrap(~sex)+ scale_y_continuous(labels = scales::percent)+ labs(x=&quot;passenger class&quot;, y=NULL)+ theme_bw() Figure 17: Distribution of passenger class by gender I can also calculate the conditional distributions by hand using prop.table: round(prop.table(table(titanic$sex, titanic$pclass),1),3)*100 ## ## First Second Third ## Female 30.9 22.7 46.4 ## Male 21.2 20.3 58.5 As these numbers and Figure 17 both show, the distribution of men and women by passenger class is very different. Women were less likely to be in third class and more likely to be in first class than men, while about the same percent of men and women were in second class. Odds ratio (advanced) We can also use the odds ratio to measure the association between two categorical variables. The odds ratio is not a term that is common in everyday speech but it is a critical concept in all kinds of scientific research. Lets take the different distributions of climate change belief for Democrats and Republicans. About 84% of Democrats were believers, but only 54% of Republicans were believers. How can we talk about how different these two numbers are from one another? We could subtract one from the other or we could take the ratio by dividing one by the other. However, both of these approaches have a major problem. Because the percents (and proportions) have minimum and maximum values of 0 and 100, as you approach those boundaries the differences necessarily have to shrink because one group is hitting its upper or lower limit. This makes it difficult to compare percentage or proportional differences across different groups because the overall average proportion across groups will affect the differences in proportion. Odds ratios are a way to get around this problem. To understand odds ratios, you first have to understand what odds are. All probabilities (or proportions) can be converted to a corresponding odds. If you have a \\(p\\) probability of success on a task, then your odds \\(O\\) of success are given by: \\[O=\\frac{p}{1-p}\\] The odds are basically the ratio of the probability of success to the probability of failure. This tells you how many successes you expect to get for every one failure. Lets say a baseball player gets a hit 25% of the time that the player comes up to bat (an average of 250). The odds are then given by: \\[O=\\frac{0.25}{1-0.25}=\\frac{0.25}{0.75}=0.33333\\] The hitter will get on average 0.33 hits for every one out. Alternatively, you could say that the hitter will get one hit for every three outs. Re-calculating probabilities in this way is useful because unlike the probability, the odds has no upper limit. As the probability of success approaches one, the odds will just get larger and larger. We can use this same logic to construct the odds that a Democratic and Republican respondent, respectively, will be climate change believers. For the Democrat, the probability is 0.843, so the odds are: \\[O=\\frac{0.843}{1-0.843}=\\frac{0.843}{0.157}=5.369\\] Among Democrats, there are 5.369 believers for every one denier. Among Republicans, the probability is 0.541, so the odds are: \\[O=\\frac{0.535}{1-0.535}=\\frac{0.535}{0.465}=1.151\\] Among Republicans, there are 1.151 believers for every one denier. This number is close to “even” odds of 1, which happen when the probability is 50%. The final step here is to compare those two odds. We do this by taking their ratio, which means we divide one number by the other: \\[\\frac{5.369}{1.151}=4.67\\] This is our odds ratio. How do we interpret it? This odds ratio tells us how much more or less likely climate change belief is among Democrats relative to Republicans. In this case, I would say that “the odds of belief in anthropogenic climate change are 4.665 times higher among Democrats than Republicans.” Note the “times” here. This 4.665 is a multiplicative factor because we are taking a ratio of the two numbers. You can calculate odds ratios from conditional distributions just as I have done above, but there is also a short cut technique called the cross-product. Lets look at the two-way table of party affiliation but this time just for Democrats and Republicans. For reasons I will explain below, I am going to reverse the ordering of the columns so that believers come first. Believer Denier Democrat 1235 230 Republican 664 577 The two numbers in blue are called the diagonal and the two numbers in red are the reverse diagonal. The cross-product is calculated by multiplying the two numbers in the diagonal by each other and multiplying the two numbers in the reverse diagonal together and then dividing the former product by the latter: \\[\\frac{1235*577}{664*230}=4.67\\] I get the exact same odds ratio as above without having to calculate the proportions and odds themselves. This is a useful shortcut for calculating odds ratios. The odds ratio that you calculate is always the odds of the first row being in the first column relative to those odds for the second row. Its easy to show how this would be different if I had kept the original ordering of believers and deniers: Denier Believer Democrat 230 1235 Republican 577 664 \\[\\frac{230*664}{577*1235}=0.21\\] I get a very different odds ratio, but that is because I am calculating something different. I am now calculating the odds ratio of being a denier rather than a believer. So I would say that the “the odds of denial of anthropogenic climate change among Democrats are only 22% of the odds for Republicans.” In other words, the odds of being a denier are much lower among Democrats. However, the information here is the same because the 0.21 here is exactly equal to 1/4.67. In other words, the odds ratio of denial is just the inverted mirror image of the odds ratio of belief. Its just important that you remember that when you calculate the cross-product, you are always calculating the odds ratio of being in the category of the first column, whatever category that may be. "],
["mean-differences.html", "Mean Differences", " Mean Differences Measuring association between a quantitative and categorical variable is fairly straightforward. We want to look for differences in the distribution of the quantitative variable at different categories of the categorical variables. For example, if we were interested in the gender wage gap, we would want to compare the distribution of wages for women to the distribution of wages for men. There are two ways we can do this. First, we can graphically examine the distributions using the techniques we have already developed, particularly the boxplot. Second, we can compare summary measures like the mean across categories. Graphically examining differences in distributions We could compare entire histograms of the quantitative variable across different categories of the categorical variable, but this is often too much information. A cleaner method is to use comparative boxplots. Comparative boxplots construct boxplots of the quantitative variable across all categories of the categorical variable and plot them next to each other for easier comparison. We can easily construct a comparative boxplot in ggplot by adding an x aesthetic to our existing boxplot code. Lets try an example looking at differences in the distribution of movie runtime across different movie genres. ggplot(movies, aes(x=Genre, y=Runtime))+ geom_boxplot(fill=&quot;seagreen&quot;, outlier.color = &quot;red&quot;)+ labs(x=NULL, y=&quot;movie runtime in minutes&quot;)+ theme_bw() Figure 18: Boxplots of movie runtime by genre This plot is a good start, but I am running into a problem where the genre labels are running into each other on the x-axis because they are too long. We can solve this problem very easily by using the coord_flip command to flip the axis: ggplot(movies, aes(x=Genre, y=Runtime))+ geom_boxplot(fill=&quot;seagreen&quot;, outlier.color = &quot;red&quot;)+ labs(x=NULL, y=&quot;movie runtime in minutes&quot;)+ coord_flip()+ theme_bw() Figure 19: Boxplots of movie runtime by genre, with coordinates flipped Now my genre labels are much easier to read. However, there is one more addition I can make to this graph in order to improve its readability. I want to order my genre categories so that they are ordered from largest to smallest median runtime on the graph. I can do this by applying the reorder command directly within ggplot. The reorder command takes three arguments. The first argument is the categorical variable to be reordered (genre in my case). The second variable is the variable that reordering should be based upon (runtime in my case). The third argument is the mathematical function that will be used for sorting (median in my case). The full command looks like: ggplot(movies, aes(x=reorder(Genre, Runtime, median), y=Runtime))+ geom_boxplot(fill=&quot;seagreen&quot;, outlier.color = &quot;red&quot;)+ labs(x=NULL, y=&quot;movie runtime in minutes&quot;)+ coord_flip()+ theme_bw() Figure 20: Boxplots of movie runtime by genre, with coordinates flipped Figure 20 now contains a lot of information. At a glance, I can see which genres had the longest median runtime and which had the shortest. But I can also see how much variation in runtime there is within movies by comparing the width of the boxes. For example, I can see that there is relatively little variation in runtime for animation movies, and the most variation in runtime among sci-fi/fantasy and action movies. I can also see some of the extreme outliers by genre. Most of these outliers are for extremely long movies, relative to the genre norm, but in a couple of cases, I can see movies that were remarkably short for their genre. Comparing differences in the mean We can also establish a relationship by looking at differences in summary measures. Implicitly, we are already doing this in the comparative boxplot when we look at the median bars across categories. However, in practice it is more common to compare the mean of the quantitative variable across different categories of the categorical variable. In R, you can get the mean of one variable at different levels of a categorical variable using the tapply command like so: tapply(movies$Runtime, movies$Genre, mean) ## Action Animation Comedy Drama Family ## 111.91304 90.06475 100.45711 112.65060 102.60000 ## Horror Musical/Music Mystery Romance SciFi/Fantasy ## 97.39545 108.46602 114.25641 109.89855 112.56202 ## Thriller ## 110.13260 The tapply command takes three arguments. The first argument is the quantitative variable for which we want means. The second argument is the categorical variable. The third argument is the method we want to run on the quantitative variable, in this case the mean. The output is the mean movie runtime by genre. If we want a quantitative measure of how genre and runtime are related we can calculate a mean difference. This number is simply the difference in means between any two categories. For example, action movies are 111.9 minutes long, on average, while horror movies are 97.4 minutes long, on average. The difference is \\(111.9-97.4=14.5\\). Thus, we can say that, on average, action movies are 14.5 minutes longer than horror movies. We could have reversed that subtraction to get \\(97.4-111.9=-14.5\\). In this case, we would say that horror movies are 14.5 minutes shorter than action movies. Either way, we get the same information. However, its important to keep track of which number applies to which category when you take the difference, so that you get the interpretation correct. We can also display these results graphically using a barplot, although this will take a little more processing for ggplot because ggplot expects data to be in a “data.frame” object and the output of tapply is a single vector of numbers. To make this work, we have to use the as.data.frame.table command to convert our object and then rename the variables. Also, to make this prettier, I am first going to sort the output from largest to smallest mean runtime: mruntime &lt;- tapply(movies$Runtime, movies$Genre, mean) #sort highest to lowest mruntime &lt;- sort(mruntime, decreasing=FALSE) #convert to data.frame mruntime &lt;- as.data.frame.table(mruntime) #rename variables colnames(mruntime) &lt;- c(&quot;genre&quot;,&quot;runtime&quot;) ggplot(mruntime, aes(x=genre, y=runtime))+ geom_col()+ coord_flip()+ labs(x=NULL, y=&quot;runtime in minutes&quot;)+ theme_bw() Figure 21: Mean runtime by movie genre This is is one of the few examples this term where we have to process a dataset into something else prior to feeding it into ggplot. The result is shown in Figure 21. While this information is useful, it doesn’t really tell us anything that we haven’t already seen in the comparative boxplot from Figure 20. The only real difference is that the boxplots showed us medians while this figure shows us means. However, the comparative boxplots also showed us additional information about spread and outliers and so are generally preferable. Figure 21 also breaks a common stylistic rule in data visualization. The big thick bars take up a lot of ink but carry relatively little information. This is called the “ink to information ratio” made famous by Edward Tufte. An alternative way to display this information would be to use “lollipops” rather than bars: ggplot(mruntime, aes(x=genre, y=runtime))+ geom_lollipop()+ coord_flip()+ labs(x=NULL, y=&quot;runtime in minutes&quot;)+ theme_bw() Figure 22: Using a lollipop graph to display ean runtime by movie genre is a lot easier on the eye and the ink cartridge "],
["scatterplot-and-correlation-coefficient.html", "Scatterplot and Correlation Coefficient", " Scatterplot and Correlation Coefficient The techniques for looking at the association between two quantitative variables are more developed than the other two cases, so we will spend more time on this topic. Additionally, the major approach here of ordinary least squares regression turns out to be a very flexible, extendable method that we will build on later in the term. When examining the association between two quantitative variables, we usually distinguish the two variables by referring to one variable as the dependent variable and the other variable as the independent variable. The dependent variable is the variable whose outcome we are interested in predicting. The independent variable is the variable that we treat as the predictor of the dependent variable. For example, lets say we were interested in the relationship between income inequality and life expectancy. We are interested in predicting life expectancy by income inequality, so the dependent variable is life expectancy and the independent variable is income inequality. The language of dependent vs. independent variable is causal, but its important to remember that we are only measuring the association. That association is the same regardless of which variable we set as the dependent and which we set as the independent. Thus, the selection of the dependent and independent variable is more about which way it more intuitively makes sense to interpret our results. The scatterplot We can examine the relationship between two quantitative variables by constructing a scatterplot. A scatterplot is a two-dimensional graph. We put the independent variable on the x-axis and the dependent variable on the y-axis. For this reason, we often refer generically to the independent variable as x and the dependent variable generically as y. To construct the scatterplot, we plot each observation as a point, based on the value of its independent and dependent variable. For example, lets say we are interested in the relationship between the median age of the state population and violent crime in our crime data. Our first observation, Alabama, has a median age of 37.8 and a violent crime rate of 378 crimes per 100,000. this is plotted in 23 below. Figure 23: Starting a scatterplot by plotting the firest observation If I repeat that process for all of my observations, I will get a scatterplot that looks like: Figure 24: Scatterplot of median age by the violent crime rate for all US states What are we looking for when we look at a scatterplot? There are four important questions we can ask of the scatterplot. First, what is the direction of the relationship. We refer to a relationship as positive if both variables move in the same direction. if y tends to be higher when x is higher and y tends to be lower when x is lower, then we have a positive relationship. On the other hand, if the variables move in opposite directions, then we have a negative relationship. If y tends to be lower when x is higher and y tends to be higher when x is lower, then we have a negative relationship. In the case above, it seems like we have a generally negative relationship. States with higher median age tend to have lower violent crime rates. Second, is the relationship linear? I don’t mean here that the points fall exactly on a straight line (which is part of the next question) but rather does the general shape of the points appear to have any “curve” to it. If it has a curve to it, then the relationship would be non-linear. This issue will become important later, because our two primary measures of association are based on the assumption of a linear relationship. In this case, there is no evidence that the relationship is non-linear. Third, what is the strength of the relationship. If all the points fall exactly on a straight line, then we have a very strong relationship. On the other hand, if the points form a broad elliptical cloud, then we have a weak relationship. In practice, in the social sciences, we never expect our data to conform very closely to a straight line. Judging the strength of a relationship often takes practice. I would say the relationship above is of moderate strength. Fourth, are there outliers? We are particularly concerned about outliers that go against the general trend of the data, because these may exert a strong influence on our later measurements of association. In this case, there are two clear outliers, Washington DC and Utah. Washington DC is an outlier because it has an extremely high level of violent crime relative to the rest of the data. Its median age tends to be on the younger side, so its placement is not inconsistent with the general trend. Utah is an outlier that goes directly against the general trend because it has one of the lowest violent crime rates and the youngest populations. This is, of course, driven by Utah’s heavily Mormon population, who both have high rates of fertility (leading to a young population) and whose church communities are able to exert a remarkable degree of social control over these young populations. Constructing scatterplots in R You can construct scatterplots in ggplot by using the geom_point geometry. You just need to define the aesthetics for x (on the x-axis) and y (on the y-axis). ggplot(crimes, aes(x=Poverty, y=Property))+ geom_point()+ labs(x=&quot;poverty rate&quot;, y=&quot;property crimes per 100,000 population&quot;)+ theme_bw() Figure 25: Scatterplot of a state’s poverty rate by property crime rate, for all US states Sometimes with large datasets, scatterplots can be difficult to read because of the problem of overplotting. This happens when many data points overlap, so that its difficult to see how many points are showing. For example: ggplot(movies, aes(x=Runtime, y=TomatoMeter))+ geom_point()+ labs(x=&quot;movie runtime in minutes&quot;, y=&quot;Percent of reviews that are positive&quot;)+ theme_bw() Figure 26: An example of the problem of overplotting where points are being plotted on top of each other Because so many movies are in the 90-120 minute range, it is difficult to see the density of points in this range and thus difficult to understand the relationship. One way to address this is to allow the points to be semi-transparent, so that as more and more points are plotted in the same place, the shading will become darker. We can do this in geom_point by setting the alpha argument to something less than one but greater than zero: ggplot(movies, aes(x=Runtime, y=TomatoMeter))+ geom_point(alpha=0.3)+ labs(x=&quot;movie runtime in minutes&quot;, y=&quot;Percent of reviews that are positive&quot;)+ theme_bw() Figure 27: Using a semi-transparent point will help us see areas where there is a dense tangle of points Overplotting can also be a problem with discrete variables because these variables can only take on certain values which will then exactly overlap with one another. This can be seen in Figure 28 which shows the relationship between age and wages in the earnings data. ggplot(earnings, aes(x=age, y=wages))+ geom_point()+ labs(x=&quot;age&quot;, y=&quot;hourly wage&quot;)+ theme_bw() Figure 28: Age is discrete so the scatterplot looks like a bunch of vertical lines of dots and is very hard to understand Because age is discrete all points are stacked up in vertical bars making it difficult to understand what is going on. One solution for this problem is to “jitter” each point a little bit by adding a small amount of randomness to the x and y values. The randomness added won’t affect our sense of the relationship but will reduce the issue of overplotting. We can do this simply in ggplot by replacing the geom_point command with geom_jitter. ggplot(earnings, aes(x=age, y=wages))+ geom_jitter()+ labs(x=&quot;age&quot;, y=&quot;hourly wage&quot;)+ theme_bw() Figure 29: Jittering helps with overplotting but its still difficult to see how dense points are for most of the plot Jittering helped get rid of those vertical lines, but there are so many observations that we still have problems of understanding the density of points for most of the plot. If we add an alpha parameter to the geom_jitter command, we should be better able to understand the scatterplot. ggplot(earnings, aes(x=age, y=wages))+ geom_jitter(alpha=0.01)+ labs(x=&quot;age&quot;, y=&quot;hourly wage&quot;)+ theme_bw() Figure 30: Jittering and transparency help us to make sense of the relationship between age and wages The correlation coefficient We can measure the association between two quantitative variables with the correlation coefficient, r. The formula for the correlation coefficient is: \\[r=\\frac{1}{n-1}\\sum_{i=1}^n (\\frac{x_i-\\bar{x}}{s_x}*\\frac{y_i-\\bar{y}}{s_y})\\] That looks complicated, but lets break it down step by step. We will use the association between median age and violent crimes as our example. The first step is to subtract the means from each of our x and y variables. This will give us the distance above or below the mean for each variable. diffx &lt;- crimes$MedianAge-mean(crimes$MedianAge) diffy &lt;- crimes$Violent-mean(crimes$Violent) The second step is to divide these differences from the mean of x and y by the standard deviation of x and y, respectively. diffx.sd &lt;- diffx/sd(crimes$MedianAge) diffy.sd &lt;- diffy/sd(crimes$Violent) Now each of your x and y values have been converted from their original form into the number of standard deviations above or below the mean. This is often called standarization. By doing this, we have put both variables on the same scale and have removed whatever original units they were measured in (in our case, years of age and crimes per 100,000). The third step is to to multiply each converted value of x by each converted value of y. product &lt;- diffx.sd*diffy.sd Why do we do this? First consider this scatterplot of our standardized x and y: Figure 31: Where a point falls in the four quadrants of this scatterplot indicate whether it provides evidence for a positive or negative relationship Points shown in blue have either both positive or both negative x and y values. When you take the product of these two numbers, you will get a positive product. This is evidence of a positive relationship. Points shown in red have one positive and one negative x and y value. When you take the product of these two numbers, you will get a negative product. This is evidence of a negative relationship. The final step is to add up all this evidence of a positive and negative relationship and divide by the number of observations (minus one). sum(product)/(length(product)-1) ## [1] -0.3015232 This final value is our correlation coefficient. We could have also calculated it by using the cor command: cor(crimes$MedianAge, crimes$Violent) ## [1] -0.3015232 How do we interpret this correlation coefficient? It turns out the correlation coefficient r has some really nice properties. First, the sign of r indicates the direction of the relationship. If r is positive, the association is positive. If r is negative, the association is negative. if r is zero, there is no association. Second, r has a maximum value of 1 and a minimum value of -1. These cases will only happen if the points line up exactly on a straight line, which never happens with social science data. However, it gives us some benchmark to measure the strength of our relationship. Figure 32 shows simulated scatterplots with different r in order to help you get a sense of the strength of association for different values of r. Figure 32: Strength of association for various values of the correlation coefficient, based on simulated data Third, r is a unitless measure of association. It can be compared across different variables and different datasets in order to make a comparison of the strength of association. For example, the correlation coefficient between unemployment and violent crimes is 0.45. Thus, violent crimes are more strongly correlated with unemployment than with median age (0.44&gt;0.30). The association between median age and property crimes is -0.36, so median age is more strongly related to property crimes than violent crimes (0.36&gt;0.30). There are some important cautions when using the correlation coefficient. First, the correlation coefficient will only give a proper measure of association when the underlying relationship is linear. if there is a non-linear (curved) relationship, then r will not correctly estimate the association. Second, the correlation coefficient can be affected by outliers. We will explore this issue of outliers and influential points more in later sections. "],
["statistical-inference.html", "Statistical Inference", " Statistical Inference Now that we have the basics of examining data down, we turn to another issue that we can address with statistical analysis. Howe confident are we that the the results from the data represent the larger population from which the data are drawn? This issue only applies to cases where the data we use constitutes a sample from a larger population. However, many of the datasets that we work with in the social sciences are of this type, so this is typically an important issue. We don’t want to to reach an incorrect conclusion that \\(x\\) is associated with \\(y\\) in cases when that association in our sample is basically a result of random chance. In many introductory statistics courses, statistical inference would take up the majority of the course and you would learn a variety of cookbook formulas for conducting “tests.” We won’t do much of that here. Instead I will focus on the logic of the two most common procedures in statistical inference: the confidence interval and the hypothesis test. Once you understand the logic behind these procedures, it turns out that all of the various “tests” are just iterations on the same basic theme. Nonetheless, we will have to use some formulas in this module with associated number crunching. This is the most math heavy module of the course, so be prepared. Slides for this module can be found here. "],
["the-problem-of-statistical-inference.html", "The Problem of Statistical Inference", " The Problem of Statistical Inference So far, we have only been looking at measurements from our actual datasets. We examined both univariate statistics like the mean, median, and standard deviation, as well as measures of association like the mean difference, correlation coefficient and OLS regression line slope. We can use this measures to draw conclusions about our data. In many cases, the dataset that we are working is only a sample from some larger population. Importantly, we don’t just want to know something about the sample, but rather we want to know something about the population from which the sample was drawn. For example, when polling organizations like Gallup conduct political polls of 500 people, they are not drawing conclusions about just those 500 people, but rather about the whole population from which those 500 people were sampled. To take another example from our General Social Survey (GSS) data on sexual frequency. We can calculate the mean sexual frequency by marital status: tapply(sex$sexf, sex$marital, mean) ## Married Widowed Divorced Separated Never married ## 56.094106 9.222628 41.388720 55.652778 53.617704 Married respondents had sex 2.5 (56.1-53.6) times more per year than never married individuals. We don’t want to draw this conclusion just for our sample. Rather, we want to know what the relationship is between marital status and sexual frequency in the US population as a whole. In other words, we want to infer from our sample to the population. Figure 33 shows this process graphically. Figure 33: The process of making statistical inferences The large blue rectangle is the population that we want to know about. Within this population, there is some value that we want to know. In this case, that value is the mean difference in sexual frequency between married and never married individuals. We refer to this unknown value in the population as a parameter. You will also notice that there are some funny-looking Greek letters in that box. We always use Greek symbols to represent values in the population. In this case, \\(\\mu_1\\) is the population mean of sexual frequency for married individuals and \\(\\mu_2\\) is the population mean of sexual frequency for never married individuals. Thus, \\(\\mu_1-\\mu_2\\) is the population mean difference in sexual frequency between married and never married individuals. We typically don’t have data on the entire population, which is why we need to draw a sample in the first place. Therefore, these population parameters are unknown. To estimate what they are, we draw a sample as shown by the smaller yellow square. In this sample, we can calculate the sample mean difference in sexual frequency between married and never married individuals, \\(\\bar{x}_1-\\bar{x}_2\\). We refer to a measurement in the sample as a statistic. We represent these statistics with roman letters to distinguish them from the corresponding value in the population. The statistic is always an estimate of the parameter. In this case, \\(\\bar{x}_1-\\bar{x}_2\\) is an estimate of \\(\\mu_1-\\mu_2\\). We can infer from the sample to the population and conclude that our best guess as to the true mean difference in the population is the value we got in the sample. The sample mean difference may be our best guess as to the true value in the population, but how confident are we in that guess? Intuitively, if I only had a sample of 10 people I would be much less confident than if I had a sample of 10,000 people. Statistical inference is the technique of quantifying our uncertainty in the estimate. If you have ever read the results of a political poll, you will be familiar with the term “margin of error.” This is a measure of statistical inference. Why might our sample produce inaccurate results? There are two sources of bias that could result in our sample statistic being different from the true population parameter. The first form of bias is systematic bias. Systematic bias occurs when something about the procedure for generating the sample produces a systematic difference between the sample and the population. Sometimes, systematic bias results from the way the sample is drawn. For example, if I sample my friends and colleagues on their voting behavior, I will likely introduce very large systematic bias in my estimate of who will win an election because my friends and colleagues are more likely than the general population to hold similar views to my own. Systematic bias can also result from the way questions are worded, the characteristics of interviewers, the time of day interviews are conducted, etc. Systematic bias can often be minimized in well-designed and executed scientific surveys. Statistical inference cannot do anything to account for systematic bias. The second form of bias is random bias. Random bias occurs when the sample statistic is different from the population parameter, just by random chance due to the actual sample that was drawn. In other words, even if there is no systematic bias in my survey design, I can get a bad estimate simply due to the bad luck of drawing a really unusual sample. Imagine that I am interested in estimating mean wealth in the United States and I happen to draw Bill Gates in my sample. I will dramatically overestimate mean wealth in the US. Random bias affects every sample, regardless of how well-designed and executed. In practice, the sample statistic is extremely unlikely to be exactly equal to the population parameter, so some degree of random bias is always present in every sample. However, this random bias will become less important as the sample size increases. In the previous example, Bill Gates is going to bias my results much more if I draw a sample of 10 people, than if I draw a sample of 100,000 people. Our goal with statistical inference is to more precisely quantify how bad that random bias could be in our sample. Notice the word “could” in the previous sentence. The tricky part about statistical inference is that while we know that random bias could be causing our sample statistic to be very different from the population parameter, we never know for sure whether random bias had a big effect or a small effect in our particular sample, because we don’t have the population parameter with which we could compare it. Keep this issue in mind in the next sections, as it plays a key role in how we understand our procedures of statistical inference. It is also important to keep in mind that statistical inference only works when you are actually drawing a sample from a larger population that you want to draw conclusions about. In some cases, our data either constitute a unique event, as in the Titanic case, that cannot be properly considered a sample of something larger or the data actually constitute the entire population of interest, as is the case in our dataset on movies. Although you will occasionally still see people use inferential measures on such data, it is technically inappropriate because there is no larger population to make inferences about. "],
["the-concept-of-the-sampling-distribution.html", "The Concept of the Sampling Distribution", " The Concept of the Sampling Distribution Lets say that you want to know the mean years of education of US adults. You implement a well-designed representative survey that samples 100 respondents from the USA. You ask people the simple question “how many years of education do you have?” You then calculate the mean years of education in your sample. This simple example involves three different kinds of distributions. Understanding the difference between these three different distributions is the key to unlocking how statistical inference works. The Population Distribution. This is the distribution of years of education in the entire US population. The mean of this distribution is given by \\(\\mu\\) and is a population parameter. If we had data on the entire population we could show the distribution and calculate \\(\\mu\\). However, because we don’t have data on the full population, the population distribution and \\(\\mu\\) are unknown. This distribution is also static - it doesn’t fluctuate. The Sample Distribution. This is the distribution of years of education in the sample of 100 respondents that I have. The mean of this distribution is \\(\\bar{x}\\) and is a sample statistic. Since we collected this data, this distribution and \\(\\bar{x}\\) are known. We can calculate \\(\\bar{x}\\) and we can show the distribution of years of education in the sample (with a histogram or boxplot, for example). The sample distribution is an approximation of the population distribution, but because of random bias, it may be somewhat different. Also, because of this random bias, the distribution is not static - if we were to draw another sample the two sample distributions would almost certainly not be identical. The Sampling Distribution. Imagine all the possible samples of size 100 that I could have drawn from the US population. Its a tremendously large number. If I had all those samples, I could calculate the sample mean of years of education for each sample. Then, I would have the mean years of education in every possible sample of size 100 that I could have drawn from the population. The sampling distribution is the distribution of all of these possible sample means. More generally, the sampling distribution is the distribution of the desired sample statistic in all possible samples of size \\(n\\). The sampling distribution is much more abstract than the other two distributions, but is key to understanding statistical inference. When we draw a sample and calculate a sample statistic from this sample, we are in effect reaching into the sampling distribution and pulling out a value. Therefore, the sampling distribution give us information about how variable our sample statistic might be as a result of randomness in the sampling. Example: class height As an example, lets use some data on a recent class that took this course. I will treat this class of 42 students as a population that I would like to know something about. In this case, I would like to know the mean height of the class. In most cases, the population distribution is unknown but in this case, I know the height of all 42 students because of surveys they all took at the beginning of the term. Figure 34 shows the population distribution of height in the class: Figure 34: Population distribution of height in a class of students The population distribution of height is bimodal which is typical, because we are mixing the heights of men and women. The population mean of height (\\(\\mu\\)) is 66.517 inches. Lets say I lost the results of the student survey and I wanted to know the mean height of the class. I could take a random sample of two students in order to calculate the mean height. Lets say I drew a sample of two people who were 68 and 74 inches respectively in height. I would estimate a sample mean of 71 inches which in this case would be too high. Lets say I took another sample of two students and ended up with a mean height of 66 which would be too low. Lets say I repeat this procedure until I had sampled all possible combinations of two students out of the twenty in the class. How many samples would this be? On the first draw from the population of 42 students there are 42 possible results. On the second draw, there are 41 possible results, because I won’t re-sample the student I selected the first time. This gives me 42*41=1722 possible combinations of 42 students in two draws. However, half of these draws are just mirror images of the other draws where I swap the first and second draw. Since I don’t care about the order, I actually have 1722/2=861 possible samples. I have used a computer routine to actually calculate the sample means in all 861 of those possible samples. The distribution of these sample means then gives us the sampling distribution of mean height for samples of size 2. Figure 35 shows a histogram of that distribution. Figure 35: The sampling distribution of class height for samples of size 2 When I randomly draw one sample of size 2 and calculate its mean, I am effectively reaching into this distribution and pulling out one of these values. Note that many of the means are clustered around the true population mean of 66.5 inches, but in a few cases I can get extreme overestimates or extreme underestimates. What if I were to increase the sample size? I have used the same computer routine to calculate the sampling distribution for samples of size 3, 4, and 5. Figure 36 shows the results. Figure 36: The sampling distribution of class height for samples of various sizes Clearly, the shape of these distributions is changing. Another way to visualize this would be to draw density graphs which basically fit a curve to the histograms. We can then then overlap these density curves on the same graph. Figure 37 shows this graph. Figure 37: The sampling distribution of class height for samples of various sizes. The vertical red line shows the true population mean. There are three thing to note here. First, each sampling distribution seems to have most of the sample means clustered (where the peak is) around the true population mean of 66.5. In fact, the mean of each of these sampling distributions is exactly the true population parameter, as I will show below. Second, the spread of the distributions is shrinking as the sample size increases. You can see that the when the sample size increases, the tails of the distribution are “pulled in” and more of the sample means are closer to the center. This indicates that we are less likely to draw a sample mean that is extremely different from the population mean in larger samples. Third, the shape of the distribution at larger sample sizes is becoming more symmetric and “bell-shaped.” Lets take a look at the mean and standard deviation of these sampling distributions: Table 11: Mean and standard deviation of height for the population of students in a class as well as from several sampling distributions of that population Distribution Mean Standard Deviation Population Distribution 66.517 4.87 Sampling Distribution (n=2) 66.517 3.363 Sampling Distribution (n=3) 66.517 2.71 Sampling Distribution (n=4) 66.517 2.316 Sampling Distribution (n=5) 66.517 2.044 Note that the mean of each sampling distribution is identical to the true population mean. This is not a coincidence. The mean of the sampling distribution of sample means is always itself equal to the population mean. In statistical terminology, this is the definition of an unbiased statistic. Given that we are trying to estimate the true population mean, it is reassuring that the “average” sample mean we should get is the true population mean. Also note that the standard deviation of the sampling distributions gets smaller with increasing sample size. This is the mathematically way of seeing the shrinking of the spread that we observed graphically. In larger sample sizes, we are less likely to draw a sample mean far away from the true population mean. Central limit theorem and the normal distribution The patterns we are seeing here are well known to statisticians. In fact, they are patterns that are predicted by the most important theorem in statistics, the central limit theorem. We won’t delve into the technical details of this theorem. We can generally interpret the central limit theorem to say: As the sample size increases, the sampling distribution of a sample mean becomes a normal distribution. This normal distribution will be centered on the true population mean \\(\\mu\\) and with a standard deviation equal to \\(\\sigma/\\sqrt{n}\\), where \\(\\sigma\\) is the population standard deviation. What is this “normal” distribution? The name is somewhat misleading because there is nothing particularly normal about the normal distribution. Most real-world distributions don’t look normal, but the normal distribution is central to statistical inference because of the central limit theorem. The normal distribution is a bell-shaped, unimodal, symmetric distribution. It has two characteristics that define its exact shape. The mean of the normal distribution define where its center is and the standard deviation of the normal distribution defines its spread. Lets look at the normal sampling distribution of the mean to become familiar with it. (#fig:normal_dist)The Normal Distribution The distribution is symmetric and bell shaped. The center of the distribution is shown by the red dotted line. This center will always be at the true population mean, \\(\\mu\\). The area of the normal distribution also has a regularity that is sometimes referred to as the “68%,95%,99.7%” rule. Regardless of the actual variable, 68% of all the sample means will be within 68% of the true population mean, 95% of all the sample means will be within 95% of the true population mean, and 99.7% of all the sample means will be within three standard deviations of the mean. This regularity will become very helpful later on for making statistical inferences. The standard error It is easy to get confused by the number of standard deviations being thrown around in this section. There are three standard deviations we need to keep track of to properly understand what is going on here. Each of these standard deviations is associated with one of the types of distributions I introduced at the beginning of this section. \\(\\sigma\\): the population standard deviation. In our example, this would be the standard deviation of height for all 42 students which is 4.8702139. Typically, like other values in the population, this number is unknown. \\(s\\): the sample standard deviation. In our example, this would be the standard deviation of height from a particular sample of a given size from the class. This number can be calculated for the sample that you have, using the techniques we learned earlier in the class. \\(\\sigma/\\sqrt{n}\\): The standard deviation of the sampling distribution of the sample mean. We divide the population standard deviation \\(\\sigma\\) by the square root of the sample size. In general, we refer to the standard deviation of the sampling distribution as the standard error, for short. So remember that when I refer to the “standard error” I am using shorthand for the “standard deviation of the sampling distribution.” Other sample statistics In the example here, I have focused on the sample mean as the sample statistic of interest. However, the logic of the central limit theorem applies to several other important sample statistics of interest to us. In particular, the sampling distributions of: means proportions mean differences proportion differences correlation coefficients all become normal as the sample size increases. Thus, this normal distribution becomes critically important in making statistical inferences. Note that the standard error formula \\(\\sigma/\\sqrt{n}\\) only applies to the sampling distribution of sample means. Other sample statistics have different formulas for their standard errors, which I will introduce in the next section. What can we do with the sampling distribution? Now that we know the sampling distribution of the sample mean should be normally distributed, what can we do with that information? The sampling distribution gives us information about how we would expect the sample means to be distributed. This seems like it should be helpful in figuring out whether we got a value close to the center or not. However, there is a catch. We don’t know either \\(\\mu\\), the center of this distribution or \\(\\sigma\\) which we need to calculate its standard deviation. Thus, we know theoretically what it should look like but we have no concrete numbers to determine its actual shape. We can fix the problem with not knowing \\(\\sigma\\) fairly easily. We don’t know \\(\\sigma\\) but we do have an estimate of it in \\(s\\), the sample standard deviation. In practice, we us this value to calculate an estimated standard error of \\(s/\\sqrt{n}\\). However, this substitution has consequences. Because we are using a sample statistic subject to random bias to estimate the standard error, this creates greater uncertainty in our estimation. I will come back to this issue in the next section. We still have the more fundamental problem that we don’t know where the center of the sampling distribution should be. In order to make statistical inferences, we are going to employ two different methods that make use of what we do know about the sampling distribution: Confidence intervals: Provide a range of values within which you feel confident that the true population mean resides. Hypothesis tests: Play a game of make believe. If the true population mean was a given value, what is the probability that I would get the sample mean value that I actually did? I will discuss these two different methods in the next two sections. "],
["confidence-intervals.html", "Confidence Intervals", " Confidence Intervals As Figure 38 shows, 95% of all the possible sample means will be within 1.96 standard errors of the true population mean \\(\\mu\\). Figure 38: 95% of all sample means will be within 1.96 standard errors of the true population mean. Lets say I were to construct the following interval for every possible sample: \\[\\bar{x}\\pm1.96(\\sigma/\\sqrt{n})\\] It follows from the statement above that for 95% of all samples, this interval would contain the true population mean, \\(\\mu\\). To see how this works graphically, imagine constructing this interval for twenty different samples from the same population. Figure 39 shows the mean and interval for each of these twenty samples, relative to the true population mean. Figure 39: Intervals of sample mean plus and minus 1.96 standard errors for 20 different samples, with the true population mean shown by the blue line. Sample means shown by dots. You can see that these sample means fluctuate around the true population mean due to random sampling error. The lines give the interval outlined above. In 19 out of the 20 cases, this interval contains the true population mean (as you can see by the fact that the interval crosses the blue line). The one sample where this is not true is shown in red. On average, 95% of samples will contain the true population mean in the interval, so 5% or 1 in 20 will not. We refer to this interval as the 95% confidence interval. Of course, in practice, we only construct one interval on the sample that we have. We use this interval to give a range of values that we feel “confident” will contain the true population mean. What do we mean by “confident?” The term “confident” is a little ambiguous. Given my statements above, it might be tempting to interpret the 95% confidence interval to mean that there is a 95% probability that the true population mean is within the interval. This interpretation seems intuitive and straightforward, but that interpretation is incorrect according to the classic approach to inference. The problem here is subtle, but from the classical viewpoint, probability is an objective phenomenon that relates to the outcomes of future processes over the long run. From this viewpoint, we cannot express our subjective uncertainties about numbers in terms of probabilities.The population mean is a single static number. This leads us to a sort of yoda-like statement: The population mean is either in your interval or it is not. There is no probability. This is why we use a more neutral term like “confidence.” If we want to be long-winded about it, we might say that we are 95% confident because “in 95% of all the possible samples I could have drawn, the true population mean would be in the interval. I don’t know if I have one of those 95% or the unlucky 5%, but nonetheless, there it is.” If this all seems a bit confusing, you are perfectly normal. As I said, this is the classic view of probability. Intuitively, people often think of uncertainty in probabilistic terms (e.g. what are the odds your team will win the game?). Many contemporary statisticians would in fact agree that it is perfectly okay to express subjective uncertainty as a probability. But, I still need to let you know that from the classic approach, interpreting your confidence interval as a probability statement is a no-no. Calculating the confidence interval for the sample mean Okay, lets try calculating a confidence interval. Lets try this out for age in the politics dataset. The formula is: \\[\\bar{x}\\pm1.96(\\sigma/\\sqrt{n})\\] Oh wait, we can’t do it! We don’t know the value of the population standard deviation \\(\\sigma\\). As I explained in the last section, we are going to have to do a little “fudging” here. Instead of \\(\\sigma\\), we can use our sample standard deviation \\(s\\). However, doing so will have consequences. Here is our new formula: \\[\\bar{x} \\pm t*(s\\sqrt{n})\\] As you can see, I have replaced the 1.96 with some number \\(t\\), referred to as the t-statistic. Basically to adjust for the greater uncertainty in using a sample statistic in my calculation of the standard error, I need to increase the number here slightly from 1.96. How much I increase it will depend on the degrees of freedom which are given by the sample size minus one (\\(n-1\\)). To figure out the correct t-statistic, I can use the qt command in R. qt(0.975, nrow(politics)-1) ## [1] 1.960524 The first command to qt is the confidence you want. This is a little bit tricky because for a 95% confidence interval, we actually want to input 0.975. This is because we are basically asking for only the upper tail of that normal distribution shown at the beginning of this section. This area contains only 2.5% of the area outside, with the other 2.5% being in the lower tail. The second number is the degrees of freedom which equals \\(n-1\\). In this case, we have such a large sample, that the t-statistic we need is very close to 1.96. In smaller samples, using the t-statistic rather than 1.96 can make a bigger difference. Its not a proper sample, but lets take the case of the crime data. Here there are only 51 observations, so the t-statistic is: qt(0.975, 51-1) ## [1] 2.008559 The difference from 1.96 is a little more noticeable. Lets return to the politics data. We now have all the information we need to calculate the 95% confidence interval: xbar &lt;- mean(politics$age) sdx &lt;- sd(politics$age) n &lt;- nrow(politics) se &lt;- sdx/sqrt(n) t &lt;- qt(0.975,n-1) xbar+t*se ## [1] 50.03165 xbar-t*se ## [1] 48.97307 We are 95% confident that the mean age among all US adults is between 48.97 and 50.03 years of age. As you can see, the large sample of nearly 6,000 respondents produces a very tight confidence interval. Calculating the confidence interval for other sample statistics As noted in the previous section, the sampling distribution of other sample statistics such as proportions, mean differences, and regression slopes is also normally distributed in large enough samples. This means that we can use the same approach to construct confidence intervals for other sample statistics. The general form of the confidence interval is: \\[\\texttt{(sample statistic)} \\pm t*\\texttt{(standard error)}\\] In order to do this for any of the above sample statistics, we only need to know how to calculate that sample statistic’s standard error and the degrees of freedom used to look up the t-statistic for that sample statistic. Table 12 provides a useful cheat sheet of those formulas: Table 12: Cheat sheet of equations for calculating standard errors and degrees of freedom Type SE df for \\(t\\) Mean \\(s/\\sqrt{n}\\) \\(n-1\\) Proportion \\(\\sqrt\\frac{\\hat{p}*(1-\\hat{p})}{n}\\) \\(n-1\\) Mean Difference \\(\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}\\) min(\\(n_1-1\\),\\(n_2-1\\)) Proportion Difference \\(\\sqrt{\\frac{\\hat{p}_1*(1-\\hat{p}_1)}{n_1}+\\frac{\\hat{p}_2*(1-\\hat{p}_2)}{n_2}}\\) min(\\(n_1-1\\),\\(n_2-1\\)) Correlation Coefficient \\(\\sqrt{\\frac{1-r^2}{n-2}}\\) \\(n-2\\) I know some of that math might look intimidating but I will go through an example of each case below to show you how it works for each case. Example with proportions As an example, lets use the proportion of respondents who do not believe in anthropogenic climate change. In our politics sample, we get: table(politics$globalwarm) ## ## No Yes ## 1180 3058 n &lt;- 1180+3058 p_hat &lt;- 1180/n n ## [1] 4238 p_hat ## [1] 0.2784332 About 27.8% of the respondents in our sample are climate change deniers. What can we conclude about the proportion in the US population? First, lets figure out the t-statistic. We use the same \\(n-1\\) for degrees of freedom: t_stat &lt;- qt(0.975, n-1) t_stat ## [1] 1.960524 Our sample is large enough that we are basically using 1.96. Now we need to calculate the standard error. The formula from above is: \\[\\sqrt{\\frac{\\hat{p}*(1-\\hat{p})}{n}}\\] The term \\(hat{p}\\) is the standard way to represent the sample proportion, which in this case is 0.279. So, our formula is: \\[\\sqrt{\\frac{0.278*(1-0.278)}{4238}}\\] We can calculate this in R: se &lt;- sqrt(p_hat*(1-p_hat)/n) se ## [1] 0.006885228 We now have all the pieces to construct the confidence interval: p_hat+t_stat*se ## [1] 0.2919319 p_hat-t_stat*se ## [1] 0.2649346 We are 95% confident that the true percentage of climate change deniers in the the US population is between 26.5% and 29.2%. Example with mean differences using our Add health data, what is the mean difference in popularity (number of friend nominations) between frequent smokers and those who do not smoke frequently? tab &lt;- tapply(popularity$indegree, popularity$smoker, mean) tab ## Non-smoker Smoker ## 4.506699 4.796992 mean_diff &lt;- 4.796992 - 4.506699 mean_diff ## [1] 0.290293 In our sample data, frequent smokers had 0.290 more friends on average than those who did not smoke frequently. What is the confidence interval for that value in the population? We start by calculating the t-statistic for this confidence interval. We use the size of the smaller group minus one for the degrees of freedom. table(popularity$smoker) ## ## Non-smoker Smoker ## 3732 665 n1 &lt;- 3732 n2 &lt;- 665 t_stat &lt;- qt(.975, n2-1) t_stat ## [1] 1.963543 The value is pretty close to 1.96 but a little bigger. Now we need to calculate the standard error. The formula is: \\[\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}\\] We already have \\(n_1\\) and \\(n_2\\), so we just need to get the standard deviation of friend nominations for the two groups to get \\(s_1\\) and \\(s_2\\). We can do this with another tapply command, but changing from mean to sd in the third argument. tapply(popularity$indegree, popularity$smoker, sd) ## Non-smoker Smoker ## 3.652224 3.901188 s1 &lt;- 3.652224 s2 &lt;- 3.901188 se &lt;- sqrt(s1^2/n1+s2^2/n2) se ## [1] 0.1626661 Now we have all the pieces to put together the confidence interval: mean_diff - t_stat*se ## [1] -0.02910896 mean_diff + t_stat*se ## [1] 0.609695 We are 95% confident that in the population of US adolescents, those who smoke frequently have between 0.03 fewer to 0.61 more friend nominations, on average, than those who do not smoke frequently. Note that because our confidence interval contains both negative and positive value, we cannot be confident about whether smoking is truly associated with having more or less friends. The direction of the relationship between the two variables is uncertain. Example with proportion differences Lets continue to use the Add Health data. Do we observe a gender difference in smoking behavior in our sample? prop.table(table(popularity$smoker, popularity$sex), 2) ## ## Female Male ## Non-smoker 0.8534894 0.8435407 ## Smoker 0.1465106 0.1564593 p_hat_f &lt;- 0.1465106 p_hat_m &lt;- 0.1564593 p_hat_diff &lt;- p_hat_m-p_hat_f p_hat_diff ## [1] 0.0099487 In our sample, about 14.7% of girls were frequent smokers and about 15.6% of boys were frequent smokers. The percentage of boys who smoke is about 1% higher than the percentage of girls. Do we think this moderate difference in the sample is true in the population? Lets start again by calculating the appropriate t-statistic for our confidence interval. We use the same procedure as for mean differences above, choosing the size of the smaller group for the degrees of freedom. However, its important to note that our groups are now boys and girls, not smokers and non-smokers. table(popularity$sex) ## ## Female Male ## 2307 2090 n_f &lt;- 2307 n_m &lt;- 2090 t_stat &lt;- qt(.975, n_m-1) t_stat ## [1] 1.9611 Now we can calculate the standard error. The formula is: \\[\\sqrt{\\frac{\\hat{p}_1*(1-\\hat{p}_1)}{n_1}+\\frac{\\hat{p}_2*(1-\\hat{p}_2)}{n_2}}\\] That looks like a lot, but we just have to focus on plugging in the right values in R: se &lt;- sqrt((p_hat_f*(1-p_hat_f)/n_f)+(p_hat_m*(1-p_hat_m)/n_m)) se ## [1] 0.01083286 Now we have all the parts to calculate the confidence interval: p_hat_diff - t_stat*se ## [1] -0.01129562 p_hat_diff + t_stat*se ## [1] 0.03119302 We are 95% confident that in the population of US adolescents, between 1.1% fewer to 3.1% more boys smoke frequently than girls. As above, because our confidence interval includes both negative and positive values, we are not very confident at all about whether boys or girls smoke more frequently. Example with correlation coefficient Lets stick with the Add Health data. What is the correlation between GPA and the number of friend nominations that a student receives? r &lt;- cor(popularity$pseudoGPA, popularity$indegree) r ## [1] 0.1680881 In our sample, there is a moderately positive correlation between a student’s GPA and the number of friend nominations that a student receives. What is our confidence interval for the population? For the t-statistic, we use \\(n-2\\) for the degrees of freedom: n &lt;- nrow(popularity) t_stat &lt;- qt(.975, n-2) For the standard error, the formula is: \\[\\sqrt{\\frac{1-r^2}{n-2}}\\] This is straightforward to calculate in R: se &lt;- sqrt((1-r^2)/(n-2)) se ## [1] 0.01486952 Now we have all the parts we need to calculate the confidence interval: r - t_stat*se ## [1] 0.1389363 r + t_stat*se ## [1] 0.1972398 We are 95% confident that the true correlation coefficient between GPA and friend nominations in the population of US adolescents is between 0.139 and 0.197. While there is some difference in the strength of that relationship, we are pretty confident that the correlation is moderately positive. "],
["hypothesis-tests.html", "Hypothesis Tests", " Hypothesis Tests In social scientific practice, hypothesis testing is far more common than confidence intervals as a technique of statistical inference. Both techniques are fundamentally derived from the sampling distribution and produce similar results, but the methodology and interpretation of results is very different. In hypothesis testing, we play a game of make believe. Remember that the fundamental issue we are trying to work around is that we don’t know the value of the true population parameter and thus we don’t know where the center is for the sampling distribution of the sample statistic. In hypothesis testing, we work around this issue by boldly asserting what we think the true population parameter. We then test whether the data that we got are reasonably consistent with that assertion. Example: Coke winners Lets take a fairly straightforward example. Coca-Cola used to run promotions where a certain percentage of bottle caps were claimed to win you another free coke. In one such promotion, when I was in graduate school, Coca-Cola ran a promotion where they claimed that 1 in 12 bottles were winners. If this is true, then 8.3% (1/12=0.083) of all the coke bottles in every grocery store and mini mart should be winners. Being a grad student who needed to stay up late writing a dissertation fueled by caffeine and “sugar,” I use to drink quite a few Cokes. After only receiving a few winners after numerous attempts, I began to get suspicious of the claim. I started collecting bottle caps to see if I could statistically find evidence of fraudulent behavior. For the sake of this exercise, lets say I collected 100 coke bottle caps (I never got this high in practice, but its a nice round number) and that I only got five winners. My winning percentage is 5% which is lower than Coke’s claim of 8.3%. The critical question is whether it is likely or unlikely that I would get a winning percentage this different from the claim in a sample of 100 bottle caps. That is what a hypothesis test is all about. We are asking whether the data that we got are likely under some assumption about the true parameter. If they are unlikely, then we reject that assumption. if they are not unlikely, then we do not reject the assumption. We call that assumption the null hypothesis, \\(H_0\\). The null hypothesis is a statement about what we think the true value of the parameter is. The null hypothesis is our “working assumption” until we can be proven to be wrong. In this case, the parameter of interest is the true proportion of winners among the population of all Coke bottles in the US. Coke claims that this proportion is 0.083, so this is my null hypothesis. In mathematical terms, we write: \\[H_0: \\rho=0.083\\] I use the Greek letter \\(\\rho\\) as a symbol for the population proportion. I will use \\(\\hat{p}\\) to represent the sample proportion in my sample, which is 0.05. Some standard statistical textbooks will also claim that there is an “alternative hypothesis.” That alternative hypothesis is specified as “anything but the null hypothesis.” In my opinion, this is incorrect because vague statements about “anything else” do not constitute an actual hypothesis about the data. We are testing only whether the data are consistent with the null hypothesis. No other hypothesis is relevant. We got a sample proportion of 0.05 on a sample of 100. Assuming the null hypothesis is true, what would the sampling distribution look like from which I pulled my 0.05? Note the part in bold above. We are now playing our game of make believe. We know that on a sample of 100, the sample proportion should be normally distributed. It should also be centered on the true population proportion. Because we are assuming the null hypothesis is true, it should be centered on the value of 0.083. The standard error of this sampling distribution is given by: \\[\\sqrt{\\frac{0.083*(1-0.083)}{100}}=0.028\\] Therefore, we should have a sampling distribution that looks like: Figure 40: A game of make believe, or the sampling distribution for sample proportion of winning Coca-Cola bottle caps assuming the null hypothesis is true The blue line shows the true population proportion assumed by the null hypothesis. The red line shows my actual sample proportion. The key question of hypothesis testing is whether the observed data (or more extreme data) are reasonably likely under the assumption of the null hypothesis. Practically speaking, I want to know how far my sample proportion is from the true proportion and whether this distance is far enough to consider it unlikely. To calculate how far away I am on some standard scale, I divide the distance by the standard error of the sampling distribution to calculate how many standard errors my sample proportion is below the population parameter (assuming the null hypothesis is true). \\[\\frac{0.05-0.083}{0.028}=\\frac{-0.033}{0.028}=-1.18\\] My sample proportion is 1.18 standard errors below the center of the sampling distribution. Is this an unlikely distance? To figure this out, we need to calculate the area in the lower tail of the sampling distribution past my red line. This number will tell us the proportion of all sample proportions that would be 0.05 or lower, assuming the null hypothesis is true. This standardized measure of how far is sometimes called the test statistic for a given hypothesis test. Calculating this area is not a trivial exercise, but R provides a straightforward command called pt which is somewhat similar to the qt command above. We just need to feed in how many standard errors our estimate is away from the center (-1.18) and the degrees of freedom. These degrees of freedom are identical to the ones used in confidence intervals (in this case, \\(n-1\\), so 99). pt(-1.18, 99) ## [1] 0.1204139 There is one catch with this command. It always gives you the area in the lower tail, so if your sample statistic is above the center, you should still put in a negative value in the first command. We will see an example of this below. Our output indicates that 12% of all samples would produce a sample proportion of 0.05 or less when the true population proportion is 0.083. Graphically it looks like this: Figure 41: The proportion of all possible sample proportions that are lower than our sample proportion, assuming the null hypothesis is true The grey area is the area in the lower tail. It would seem that we are almost ready to conclude our hypothesis test. However, there is a catch and its a tricky one. Remember that I was interested in the probability of getting a sample proportion this far or farther from the true population proportion. This is not the same as getting a sample proportion this low or lower. I need to consider the possibility that I would have been equally suspicious if I had got a sample proportion much higher than 8.3%. In mathematically terms, that means I need to take the area in the upper tail as well, where I am .033 above the true population proportion. This is called a two-tailed test. Luckily, because the normal distribution is symmetric, this area will be identical to the area in the lower tail and so I can just double this percent. Figure 42: We have to also consider the possibility of getting a sample proportion as far from the population proportion but in the other direction Assuming the null hypothesis is true, there is a 24% chance of getting a sample proportion as far from the true population mean or farther, just by random chance. We call this probability the p-value. The p-value is the ultimate goal of the hypothesis test. All hypothesis tests produce a p-value and it is the p-value that we will use to make a decision about our test. What should that decision be? We have only two choices. If the p-value is low enough, then it is unlikely that we would have gotten this data or data more extreme, assuming the null hypothesis is true. Therefore, we reject the null hypothesis. If the p-value is not low enough, then it is reasonable that we would have gotten this data or data more extreme, assuming the null hypothesis is true. Therefore, we fail to reject the null hypothesis. Note that we NEVER accept or prove the null hypothesis. It is already our working assumption, so the best we can do for it is to fail to reject it and thus continue to use it as our working assumption. How low does the p-value need to be in order to reject it? There is no right answer here, because this is a subjective question. However, there is a generally agreed upon practice in the social sciences that we reject the null hypothesis when the p-value is at or below 0.05 (5%). Note that while there is general consensus around this number, it is an arbitrary cutpoint. The practical difference between a p-value of 0.049 and 0.051 is negligible, but under this arbitrary standard, we would make different decisions in each case. I would rather that you just learn to think about what the p-value represents and reach your own decision. No reasonable scientist, however, would reject the null hypothesis with a p-value of 24% as we have in our Coke case. Nearly 1 in 4 samples of size 100 would produce a sample proportion this different from the assumed true proportion of 8.3% just by random chance. I therefore do not have sufficient evidence to reject the null hypothesis that Coke is telling the truth. Note that I have not proved that Coke is telling the truth. I have only failed to produce evidence that they are lying. The general procedure of hypothesis testing The general procedure of hypothesis testing is as follows: State a null hypothesis. This null hypothesis is a claim about the true value of an unknown parameter. Calculate a test statistic that tells you how far your sample statistic is from the center of the sampling distribution, assuming the null hypothesis is true. For our purposes, this test statistic will always be the number of standard errors above or below the true population parameter, assuming the null hypothesis is true. Calculate the p-value for the test statistic. The p-value is the probability of getting a sample statistic this far or farther (in absolute value) from the true population parameter, assuming the null hypothesis is true. If the p-value is below some threshold (typically 0.05), reject the null hypothesis. Otherwise, fail to reject the null hypothesis. Interpreting p-values correctly P-values are widely misunderstood in practice. Studies have been done of practicing researchers across different disciplines where these researchers were asked to interpret a p-value from a multiple choice question and the majority get it wrong. Therefore, don’t feel bad if you are having trouble understanding a p-value. You are in good company! Nonetheless, proper interpretation of a p-value is critically important for our understanding of what a hypothesis test does. The reason many people get the interpretation of p-values wrong is that they want the p-value to express the probability of a hypothesis being correct or incorrect. People routinely misinterpret the p-value as a statement about the probability of the null hypothesis being correct. The p-value is NOT a statement about the probability of a hypothesis being correct or incorrect. For the same reason that we cannot call a confidence interval a probability statement, the classical approach dictates that we cannot characterize our subjective uncertainty about whether hypotheses are true or not by a probability statement. The hypothesis is either correct or it is not. There is no probability. Correctly interpreted, the p-value is a probability statement about the data, not about the hypothesis. Specifically, we are asking what the probability is of observing data this extreme or more extreme, assuming the null hypothesis is true. We are not making a probability statement about hypotheses. Rather we are assuming a hypothesis and then asking about the probability of the data. This difference may seem subtle, but it is in fact quite substantial in interpretation. The reason why everyone (including you and me) struggles with this is that our brains want it to be the other way around. Ultimately by rejecting or failing to reject we are making statements about whether we believe the hypothesis or not, but we are not doing that directly by a probability statement about the hypothesis but rather a probability statement about the likelihood of the data given the hypothesis. Hypothesis tests of relationships The hypothesis tests that we care the most about in the sciences are hypothesis tests about relationships between variables. We want to know whether the association we are observing in the sample is true in the population. In all of these cases, our null hypothesis is that there is no association, and we want to know whether the association we observe in the sample is strong enough to reject this null hypothesis of no association. We can do hypothesis tests of this nature for both mean differences and regression slopes. Example: mean differences Lets look at differences in mean income (measured in $1000) by religion in the politics dataset. tapply(politics$income, politics$relig, mean) ## Mainline Protestant Evangelical Protestant Catholic ## 81.83439 58.32606 77.53498 ## Jewish Non-religious Other ## 120.92958 88.62963 60.75311 I want to look at the difference between Evangelical Protestants and “Other Religions.” The mean difference here is: \\[58.32606-60.75311=-2.42705\\] Evangelical Protestants make $2,427 less than members of other religions, in my sample. Let me set up a hypothesis test where the null hypothesis is that Roman Catholics and members of other religions have the same income, or in other words, the mean difference in income is zero: \\[H_0: \\mu_c-\\mu_o=0\\] Where \\(\\mu_c\\) is the population mean income of evangelical Protestants and \\(\\mu_o\\) is the population mean income of members of other religions. In order to figure out how far my sample mean difference of -2.427 is from 0, I need to find the standard error of the mean difference. The formula for this number is: \\[\\sqrt{\\frac{s_c^2}{n_c}+\\frac{s_o^2}{n_o}}\\] I can calculate this in R: tapply(politics$income, politics$relig, sd) ## Mainline Protestant Evangelical Protestant Catholic ## 62.90763 51.23396 66.59281 ## Jewish Non-religious Other ## 89.84166 71.46392 56.37886 table(politics$relig) ## ## Mainline Protestant Evangelical Protestant Catholic ## 785 917 1015 ## Jewish Non-religious Other ## 71 567 883 se &lt;- sqrt(51.23396^2/917+56.37886^2/883) -2.42705/se ## [1] -0.9547436 The t-statistic of -0.95 here is not very large. I am only 0.44 standard errors below 0 on the sampling distribution, assuming the null hypothesis is true. Lets go ahead and calculate the p-value for this t-statistic. Remember that I need to put in the negative version of this number to the pt command. I also need to use the smaller of the two sample sizes for my degrees of freedom: 2*pt(-0.95, 883-1) ## [1] 0.3423725 In a sample of this size, there is an 34% chance of observing a mean income difference of $2,427 or more between evangelical Protestants and members of other religions, just by sampling error, assuming that there is no difference in income in the population. Therefore, I fail to reject the null hypothesis that evangelical Protestants and members of other religions make the same income. Example of proportion differences Lets look at the difference in smoking behavior between white and black students in the Add Health data. Our null hypothesis is: \\[H_0: \\rho_w-\\rho_c=0\\] In simple terms, our null hypothesis is that the same proportion of white and black adolescents smoke frequently. Lets look at the actual numbers from our sample: prop.table(table(popularity$smoker, popularity$race),2) ## ## White Black/African American Latino ## Non-smoker 0.79560106 0.94847162 0.89000000 ## Smoker 0.20439894 0.05152838 0.11000000 ## ## Asian/Pacific Islander Other ## Non-smoker 0.90123457 0.92592593 ## Smoker 0.09876543 0.07407407 ## ## American Indian/Native American ## Non-smoker 0.80769231 ## Smoker 0.19230769 p_w &lt;- 0.20439894 p_b &lt;- 0.05152838 p_diff &lt;- p_w-p_b p_diff ## [1] 0.1528706 About 20.4% of white students smoked frequently, compared to only 5.2% of black students. The difference in proportion is a large 15.3% in the sample. This would seem to contradict our null hypothesis. However, we need to confirm that a difference this large in a sample of our size is unlikely to happen by random chance. To do that we need to calculate the standard error, just as we learned to do it for proportion differences in the confidence interval section: table(popularity$race) ## ## White Black/African American ## 2637 1145 ## Latino Asian/Pacific Islander ## 400 162 ## Other American Indian/Native American ## 27 26 n_w &lt;- 2637 n_b &lt;- 1145 se &lt;- sqrt((p_w*(1-p_w)/n_w)+(p_b*(1-p_b)/n_b)) se ## [1] 0.01021531 Now many standard errors is our observed difference in proportion from zero? t_stat &lt;- p_diff/se t_stat ## [1] 14.96485 Wow, thats a lot. We can be pretty confident already without the final step of the p-value, but lets calculate it anyway. Remember ot always take the negative version of the t-statistic you calculated: 2*pt(-14.96485, n_b-1) ## [1] 2.247969e-46 The p-value is astronomically small. In a sample of this size, the probability of observing a difference in the proportion frequent smokers between whites and blacks of 15.3% or larger if there is no difference in the population is less than 0.0000001%. Therefore, I reject the null hypothesis and conclude that white students are more likely to be frequent smokers than are black students. Example of correlation coefficient Lets look at the correlation between the parental income of a student and the number of friend nominations they receive. Our null hypothesis will be that there is no relationship between parental income and student popularity in the population of US adolescents. Lets look at the data in our sample: r &lt;- cor(popularity$parentinc, popularity$indegree) r ## [1] 0.1247392 In the sample we observe a moderately positive correlation between a student’s parental income and the number of friend nominations they receive. How confident are we that we wouldn’t observe such a large correlation coefficient in our sample by random chance if the null hypothesis is true? First, we need to calculate the standard error: n &lt;- nrow(popularity) se &lt;- sqrt((1-r^2)/(n-2)) se ## [1] 0.01496633 How many standard errors are we away from the assumption of zero correlation? r/se ## [1] 8.334658 What is the probability of being that far away from zero for a sample of this size? 2*pt(-8.334658, n-2) ## [1] 1.027779e-16 The probability is very small. In a sample of this size, the probability is less than 0.00000001% of observing a correlation coefficient between parental income and friend nominations received of an absolute magnitude of 0.125 or higher when the true correlation is zero in the population. Therefore, we reject the null hypothesis and conclude that there is a positive correlation between parental income and popularity among US adolescents. Statistical Significance When a researcher is able to reject the null hypothesis of “no association,” the result is said to be statistically significant. This is a somewhat unfortunate phrase that is sometimes loosely interpreted to indicate that the result is “important” in some vague scientific sense. In practice, it is important to distinguish between substantive and statistical significance. In very large datasets, standard errors will be very small, and thus it is possible to observe associations that are very small in substantive size that are nonetheless statistically significant. On the flip side, in small datasets, standard errors will often be large, and thus it is possible to observe associations that are very large in substantive size but not statistically significant. It is important to remember that “statistical significance” is a reference to statistical inference and not a direct measure of the actual magnitude of an association. I prefer the term “statistically distinguishable” to “statistically significant” because it more clearly indicates what is going on. In the previous example, we found that the income differences in our sample between Catholics and members of other religions are not statistically distinguishable from zero. We also found that the negative association in our sample between age and sexual frequency was statistically distinguishable from zero. Establishing whether an association is worthwhile in its substantive effect is a totally different exercise from establishing whether it is statistically distinguishable from zero. It is also important to remember that a statistically insignificant finding is not evidence of no relationship because we never accept the null hypothesis. We have just failed to find sufficient evidence of a relationship. No evidence of an association is not evidence of no association. "],
["building-models-1.html", "Building Models", " Building Models Up to this point, we have learned the elementary components of a good statistical analysis. However, the typical social scientist doesn’t spend that much time with these elementary components. Instead, most social scientific analysis depends on building statistical model. A statistical model is a formal mathematical representation of how we think variables might be related to one another. By building models, we can better understand the relationships between variables and how these relationships are affected by other variables. We will focus on model building in some form or another for all the remaining modules of this course. We will begin with the simplest kind of model: we just try to fit a straight line through a set of points on a scatterplot. Although this approach may not seem very sophisticated, it forms the basis for more advanced modeling techniques we will learn later. After understanding this basic model, often called the *OLS regression model** we will move on to a variety of techniques we can use to build more complicated models that are both more realistic and more informative. Throughout this module, we will focus primarily on issues of interpretation. We are now starting to learn the techniques that you will see presented in real world social science research. Being able to interpret and understand this work is a key objective of this course. Slides for this module can be found here. "],
["the-ols-regression-line.html", "The OLS Regression Line", " The OLS Regression Line Figure 43 shows a scatterplot of the relationship between median age and violent crime rates: Figure 43: Scatterplot of median age and violent crime rates across US states, with a best-fitting straight line drawn through points I have also plotted a line through those points. When you were trying to determine the direction of the relationship many of you were probably imagining a line going through the points already. Of course, if we just tried to “eyeball” the best line, we would get many different results. The line I have graphed above, however, is the best fitting line, according to standard statistical criteria. It is the best-fitting line because it minimizes the total distance from all of the points collectively to the line. This line is called the ordinary least squares regression line ( or OLS regression line, for short). This fairly simply concept of fitting the best line to a set of points on a scatterplot is the workhorse of social science statistics and is the basis for most of the models that we will explore in this module. The Formula for a Line Remember the basic formula for a line in two-dimensional space? In algebra, you probably learned something like this: \\[y=a+bx\\] The two numbers that relate \\(x\\) to \\(y\\) are \\(a\\) and \\(b\\). The number \\(a\\) gives the y-intercept. This is the value of \\(y\\) when \\(x\\) is zero. The number \\(b\\) gives the slope of the line, sometimes referred to as the “rise over the run.” The slope indicates the change in \\(y\\) for a one-unit increase in \\(x\\). The OLS regression line above also has a slope and a y-intercept. But we use a slightly different syntax to describe this line than the equation above. The equation for an OLS regression line is: \\[\\hat{y}_i=b_0+b_1x_i\\] On the right-hand side, we have a linear equation (or function) into which we feed a particular value of \\(x\\) (\\(x_i\\)). On the left-hand side, we get not the actual value of \\(y\\) for the \\(i\\)th observation, but rather a predicted value of \\(y\\). The little symbol above the \\(y\\) is called a “hat” and it indicates the “predicted value of \\(y\\).” We use this terminology to distinguish the actual value of \\(y\\) (\\(y_i\\)) from the value predicted by the OLS regression line (\\(\\hat{y}_i\\)). The y-intercept is given by the symbol \\(b_0\\). The y-intercept tells us the predicted value of \\(y\\) when \\(x\\) is zero. The slope is given by the symbol \\(b_1\\). The slope tells us the predicted change in \\(y\\) for a one-unit increase in \\(x\\). In practice, the slope is the more important number because it tells us about the association between \\(x\\) and \\(y\\). Unlike the correlation coefficient, this measure of association is not unitless. We get an estimate of how much we expect \\(y\\) to change in terms of its units for a one-unit increase in \\(x\\). For the scatterplot in Figure 43 above, the slope is -25.6 and the y-intercept is 1343.9. We could therefore write the equation like so: \\[\\hat{\\texttt{crime rate}_i}=1343.9-25.6(\\texttt{median age}_i)\\] We would interpret our numbers as follows: The model predicts that a one-year increase in age within a state is associated with 25.6 fewer violent crimes per 100,000 population, on average. (the slope) The model predicts that in a state where the median age is zero, the violent crime rate will be 1343.9 crimes per 100,000 population, on average. (the intercept) There is a lot to digest in these interpretations and I want to return to them in detail, but first I want to address a more basic question. How did I know that these are the right numbers for the best-fitting line? Calculating the Best-Fitting Line The slope and intercept of the OLS regression line are determined based on addressing one simple criteria: minimize the distance between the actual points and the line. More formally, we choose the slope and intercept that produce the minimum sum of squared residuals (SSR). A residual is the vertical distance between an actual value of \\(y\\) for an observation and its predicted value: \\[residual_i=y_i-\\hat{y}_i\\] These residuals are also sometimes called error terms, because the larger they are in absolute value, the worse is our prediction. Take a look at the Figure 44 below which shows the residuals graphically as vertical distances between the actual point and the line. Figure 44: Scatterplot with best-fitting line shown in blue and residuals shown in red Unless the points all fall along an exact straight line, there is no way for me to eliminate these residuals altogether, but some lines will produce higher residuals than others. What I am aiming to do is minimize the sum of squared residuals which is given by: \\[SSR = \\sum_{i=1}^n(y_i-\\hat{y}_i)^2\\] I square each residual and then sum them up. By squaring, I eliminate the problem of some residuals being negative and some positive. To see how this all works, you can play around with the interactive example below which allows you to guess slope and intercept for a scatterplot and then see how well you did in minimizing the sum of squared residuals. Fortunately, we don’t have to figure out the best slope and intercept by trial and error, as in the exercise above. There are relatively straightforward formulas for calculating the slope and intercept. They are: \\[b_1=r\\frac{s_y}{s_x}\\] \\[b_0=\\bar{y}-b_1*\\bar{x}\\] The r here is the correlation coefficient. The slope is really just a re-scaled version of the correlation coefficient. We can calculate this with the example above like so: slope &lt;- cor(crimes$MedianAge, crimes$Violent)*sd(crimes$Violent)/sd(crimes$MedianAge) slope ## [1] -25.5795 I can then use that slope value to get the y-intercept: mean(crimes$Violent)-slope*mean(crimes$MedianAge) ## [1] 1343.936 Using the lm command to calculate OLS regression lines in R We could just use the given formulas to calculate the slope and intercept in R, as I showed above. However, the lm command will become particularly useful later in the term when we extend this basic OLS regression line to more advanced techniques. In order to run the lm command, you need to input a formula. The structure of this formula looks like “dependent~independent” where “dependent” and “independent” should be replaced by your specific variables. The tilde (~) sign indicates the relationship. So, if we wanted to use lm to calculate the OLS regression line we just looked at above, I would do the following: model1 &lt;- lm(crimes$Violent~crimes$MedianAge) Please keep in mind that the dependent variable always goes on the left-hand side of this equation. You will get very different answers if you reverse the ordering. In this case, I have entered in the variable names using the data$variable syntax, but lm also offers you a more streamlined way of specifying variables, by including a data option separately so that you only have to put the variable names in the formula, like so: model1 &lt;- lm(Violent~MedianAge, data=crimes) Because I have specified data=crimes, R knows that the variables “Violent” and “MedianAge” refer to variables within this dataset. The result will be the same as the previous command, but this approach makes it easier to read the formula itself. I have saved the output of the lm command into a new object that I have called “model1”. You can call this object whatever you like. This is out first real example of the “object-oriented” nature of R. I can apply a variety of functions to this object in order to extract information about the relationship. If I want to get the most information, I can run a summary on this model. summary(model1) ## ## Call: ## lm(formula = Violent ~ MedianAge, data = crimes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -381.76 -118.02 -36.25 99.28 853.41 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1343.94 434.21 3.095 0.00325 ** ## MedianAge -25.58 11.56 -2.214 0.03154 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 188.1 on 49 degrees of freedom ## Multiple R-squared: 0.09092, Adjusted R-squared: 0.07236 ## F-statistic: 4.9 on 1 and 49 DF, p-value: 0.03154 There is a lot information here and we actually don’t know what most of it means yet. All we want is the intercept and slope. These numbers are given by the two numbers in the “Estimate” column of the “Coefficients” section. The intercept is 1343.94 and the slope is -25.58. We could also run the coef command which will give us just the slope and intercept of the model. coef(model1) ## (Intercept) MedianAge ## 1343.9360 -25.5795 This result is much more compact and will do for our purposes at the moment. Adding an OLS regression line to a plot You can easily add an OLS regression line to a scatterplot in ggplot. We can do this using the geom_smooth function. However we also need to specify that our method of smoothing is “lm” (for linear model) with the method=\"lm\" argument. Here is the code for the example earlier: Figure 45: Use geom_smooth to plot an OLS regression line with or without a confidence interval band You will notice that Figure 45 also adds a band of grey. This is the confidence interval band for my line and is drawn by default. We will discuss issues of inference for the OLS regression line below. If you want to remove this you can change the se argument in geom_smooth to FALSE. The OLS regression line as a model You will note that I saved the output of my lm command above as model. The lm command itself stands for “linear model.” What do I mean by this term “model?” When we talk about “models” in statistics, we are talking about modeling the relationship between two or more variables in a formal mathematical way. In the case of the OLS regression line, we are predicting the dependent variable as a linear function of the independent variable. Just as the general term model is used to describe something that is not realistic but rather an idealized representation, the same is true of our statistical models. We certainly don’t believe that our linear function provides a correct interpretation of the exact relationship between our two variables. Instead we are trying to abstract from the details and fuzziness of the relationship to get a “big picture” of what the relationship looks like. However, we always have to consider that our model is not a very good representation of the relationship. The most obvious potential problem is if the relationship is non-linear and yet we fit the relationship by a linear model, but there can be other problems as well. I will discuss these more below and the next few sections of this module will give us techniques for building better models. However, we first need to focus on how to interpret the results we just got. Interpeting Slopes and Intercepts Learning to properly interpret slopes and intercepts (especially slopes) is the number one most important thing you will learn all term, because of how common the use of OLS regression is in social science statistics. You simply cannot pass the class unless you can interpret these numbers. So take the time to be careful in interpretation here. Interpreting Slopes In abstract terms, the slope is always the predicted change in \\(y\\) for a one unit increase in \\(x\\). However, this abstract definition will simply not do when you are dealing with specific cases. You need to think about the units of \\(x\\) and \\(y\\) and interpret the slope in concrete terms. There are also a few other caveats to consider. Take the interpretation I used above for the -25.6 slope of median age as a predictor of violent crime rates. My interpretation was: The model predicts that a one year increase in age within a state is associated with 25.6 fewer violent crimes per 100,000 population, on average. There are multiple things going on in this sentence that need to be addressed. First, lets address the phrase “model predicts.” The idea of a model is something we will explore more later, but for now I will say that when we fit a line to a set of points to predict \\(x\\) by \\(y\\), we are applying a model to the data. In this case, we are applying a model that relates \\(y\\) to \\(x\\) by a simple linear function. All of our conclusions are dependent on this being a good model. Prefacing your interpretation with “the model predicts…” highlights this point. Second, a “one year increase in age” indicates the meaning of a one unit increase in \\(x\\). Never literally say a “one unit increase in \\(x\\).” Think about the units of \\(x\\) and describe the change in \\(x\\) in these terms. Third, I use “is associated with” to indicate the relationship. This phrase is intentionally passive. We want to avoid causal language when we describe the relationship. Saying something like “when \\(x\\) increases by one \\(y\\) goes up by \\(b_1\\)” may sound more intuitive, but it also implies causation. The use of “is associated with” here indicates that the two variables are related without implicitly implying that one causes the other. Using causal language is the most common mistake in describing the slope. Fourth, “25.6 fewer violent crimes per 100,000 population” is the expected change in \\(y\\). Again, you always have to consider the unit scale of your variables. In this case, \\(y\\) is measured as the number of crimes per 100,000 population, so a decrease of 25.6 means 25.6 fewer violent crimes per 100,000 population. Fifth, I append the term “on average” to the end of my interpretation. This is because we know that our points don’t fall on a straight line and so we don’t expect a deterministic relationship between median age and violent crime. Rather, we think that if we were to take a group of states that had one year higher median age than another group of states, the average difference between the groups would be -25.6. Lets try a couple of other examples to see how this works. I will use the lm command in R to calculate the slopes and intercepts, which I explain in the section below. First, lets look at the association between age and sexual frequency (I will explain the code I use here later in this section). coef(lm(sexf~educ, data=sex)) ## (Intercept) educ ## 49.7295901 0.0266939 The slope here is 0.03. Education is measured in years and sexual frequency is measured as the number of sexual encounters per year. So, the interpretation of the slope should be: The model predicts that a one year increase in education is associated with 0.03 more sexual encounters per year, on average. There is a tiny positive effect here, but in real terms the relationship is basically zero. It would take you about 100 years more education to get laid 3 more times. Just think of the student loan debt. Now, lets take the relationship between movie runtimes and tomato meter ratings: coef(lm(TomatoMeter~Runtime, data=movies)) ## (Intercept) Runtime ## 5.1074601 0.4054953 The slope is 0.41. Runtime is measured in minutes. The tomato meter is the percent of reviews that were judged to be positive. The model predicts that a one minute increase in movie runtime length is associated with a 0.38 percentage point increase in the movie’s Tomato Meter rating, on average. Longer movies tend to have higher ratings. We may rightfully question the assumption of linearity for this relationship however. It seems likely that if a movie can become too long, so its possible the relationship here may be non-linear. We will explore ways of modeling that potential non-linearity later in the term. Interpreting Intercepts Intercepts give the predicted value of \\(y\\) when \\(x\\) is zero. Again you should never interpret an intercept in these abstract terms but rather in concrete terms based on the unit scale of the variables involved. What does it mean to be zero on the \\(x\\) variable? In our example of the relationship of median age to violent crime rates, the intercept was 1343.9. Our independent variable is median age and the dependent variable is violent crime rates, so: The model predicts that in a state where the median age is zero, the violent crime rate would be 1343.9 crimes per 100,000 population, on average. Note that I use the same “model predicts” and “on average” prefix and suffix for the intercept as I used for the slope. Beyond that I am just stating the predicted value of \\(y\\) (crime rates) when \\(x\\) is zero in the concrete terms of those variables. Is it realistic to have a state with a median age of zero? No, its not. You will never observe a US state with a median age of zero. This is a common situation that often confuses students. In cases when zero falls outside the range of the independent variable, the intercept is not a particular useful number because it does not tell us about a realistic situation. The intercept’s only “job” is to give a number that allows the line to go through the points on the scatterplot at the right level. You can see this in the interactive exercise above if you select the right slope of 148 and then vary the intercept. In general making predictions for values of \\(x\\) that fall outside the range of \\(x\\) in the observed data is problematic. This is often leads to intercepts which don’t make a lot of sense. This problem with zero being outside the range of data is also evident in the other two examples of slopes from the previous section. When looking at the relationship between education and sexual frequency, no respondents are actually at zero years of education and no movies are at zero minutes of runtime. In truth, to fit the line correctly, we only need the slope and one point along the line. It is convenient to choose the point where \\(x=0\\) but there is no reason why we could not choose a different point. It is actually quite easy to calculate a different predicted value along the line by re-centering the independent variable. To re-center the independent variable \\(x\\), we just need to to subtract some constant value \\(a\\) from all the values of \\(x\\), like so: \\[x^*=x-a\\] The zero value on our new variable \\(x^*\\) will indicates that we are at the value of \\(a\\) on the original variable \\(x\\). If we then use \\(x^*\\) in the OLS regression line rather than \\(x\\), the intercept will give us the predicted value of \\(y\\) when \\(x\\) is equal to \\(a\\). Lets try this out on the model predicting violent crimes by median age. We will create a new variable where we subtract 35 from the median age variable and use that in the regression model. crimes$MedianAge.ctr &lt;- crimes$MedianAge-35 coef(lm(Violent~MedianAge.ctr, data=crimes)) ## (Intercept) MedianAge.ctr ## 448.6533 -25.5795 The intercept now gives me the predicted violent crime rate in a state with a median age of 35. In effect, I have moved my y-intercept from zero to thirty-five as is shown in Figure 46 below. Figure 46: Re-centering the independent variable moves the intercept but does not change the slope Its also possible to re-center an independent variable in the lm command without creating a whole new variable. If you surround the re-centering in the I() function within the formula, R will interpret the result of whatever is inside the I() function as a new variable. Here is an example based on the previous example: coef(lm(Violent~I(MedianAge-35), data=crimes)) ## (Intercept) I(MedianAge - 35) ## 448.6533 -25.5795 How good is \\(x\\) as a predictor of \\(y\\)? If I selected a random observation from the dataset and asked you to predict the value of \\(y\\) for this observation, what value would you guess? Your best guess would be to guess the mean of y because this is the case where your average error would be smallest. This error is defined by the distance between the mean of y and the selected value, \\(y_i-\\bar{y}\\). Now, lets say instead of making you guess randomly I first told you the value of another variable \\(x\\) and gave you the slope and intercept predicting \\(y\\) from \\(x\\). What is your best guess now? You should guess the predicted value of \\(\\hat{y}_i\\) from the regression line because now you have some additional information. There is no way that having this information could make your guess worse than just guessing the mean. The question is how much better do you do than guessing the mean. Answering this question will give us some idea of how good \\(x\\) is as a predictor of \\(y\\). We can do this by separating, or partitioning the total possible error in our first case when we guessed the mean, into the part accounted for by \\(x\\) and the part that is unaccounted for by \\(x\\). I demonstrate this partitioning for one observation in our crime data (the state of Alaska) with the scatterplot in Figure 47 below. Figure 47: We can parition the total distance (in red) between an observation’s value of the dependent variable and the mean (the dotted horizontal line) into the part accounted for by the model (in gold) and the residual (in green) that is unaccounted for by the model The distance in red is the total distance between the observed violent crime rate in the state of Alaska and the mean violent crime rate across all states (given by the dotted line). If I were instead to use the OLS regression line predicting the violent crime rate by median age, I would predict a higher violent crime rate than average for Alaska because of its relatively low median age, but I would still predict a crime rate that is too low relative to the actual crime rate. The red line can be partitioned into th gold line which is the improvement in my estimate and the green line which is the error that remains in my prediction from the model. If I could then repeat this process for all of the states, I could calculate the percentage of the total red lines that the gold lines cover. This would give me an estimate of how much I reduce the error in my prediction by using the regression line rather than the mean to predict a state’s violent crime rate. In practice, we actually need to square those vertical distances because some are negative and some are positive and then we can sum them up over all the observations. So we get the following formulas: Total variation: \\(SSY=\\sum_{i=1}^n (y_i-\\bar{y})^2\\) Explained by model: \\(SSM=\\sum_{i=1}^n (\\hat{y}_i-\\bar{y})^2\\) Unexplained by model: \\(SSR=\\sum_{i=1}^n (y_i-\\hat{y}_i)^2\\) The proportion of the variation in \\(y\\) that is explainable or accountable by variation in \\(x\\) is given by \\(SSM/SSY\\). This looks like a kind of nasty calculation, but it turns out there is a much simpler way to calculate this proportion. If we just take our correlation coefficient \\(r\\) and square it. We will get this proportion. This measure is often called “r squared” and can be interpreted as the proportion of the variation in \\(y\\) that is explainable or accountable by variation in \\(x\\). In the example above, we can calculate R squared: cor(crimes$MedianAge, crimes$Violent)^2 ## [1] 0.09091622 About 9% of the variation in violent crime rates across states can be accounted for by variation in the median age across states. Inference for OLS Regression models When working with sample data, our usual issues of statistical inference apply to regression models. In this case, our primary concern is the estimate of the regression slope because the slope measures the relationship between \\(x\\) and \\(y\\). We can think of an underlying OLS regression model in the population: \\[\\hat{y}_i=\\beta_0+\\beta_1x_i\\] We use Greek “beta” values because we are describing unobserved parameters in the population. The null hypothesis in this case would be that the slope is zero indicating no relationship between \\(x\\) and \\(y\\): \\[H_0:\\beta_1=0\\] In our sample, we have a sample slope \\(b_1\\) that is an estimate of \\(\\beta_1\\). We can apply the same logic of hypothesis testing and ask whether our \\(b_1\\) is different enough from zero to reject the null hypothesis. We just need to find the standard error for this sample slope and the degrees of freedom to use for the test and we can do this manually. However, I have good news for you. You don’t have to do any of this by hand because the lm function does it for you automatically. Lets look at the full output of the model predicting violent crime rates from median age again using the summary command: model &lt;- lm(Violent~MedianAge, data=crimes) summary(model) ## ## Call: ## lm(formula = Violent ~ MedianAge, data = crimes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -381.76 -118.02 -36.25 99.28 853.41 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1343.94 434.21 3.095 0.00325 ** ## MedianAge -25.58 11.56 -2.214 0.03154 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 188.1 on 49 degrees of freedom ## Multiple R-squared: 0.09092, Adjusted R-squared: 0.07236 ## F-statistic: 4.9 on 1 and 49 DF, p-value: 0.03154 The “Coefficients” table in the middle gives us all the information we need. The first column gives us the sample slope of -25.58. The second column gives us the standard error for this slope of 11.56. The third column gives us the t-statistic derived by dividing the first column by the second column. The final column gives us the p-value for the hypothesis test. In this case, there is about a 3.2% chance of getting a sample slope this large on a sample of 51 cases if the true value in the population is zero. Of course, in this case its nonsensical because we don’t have a sample, but the numbers here will be valuable in cases with real sample data. Regression Line Cautions OLS regression models can be very useful for understanding relationships, but they do have some important limitations that you should be aware of when you are doing statistical analysis. There are three major limitations/cautions to be aware of when using OLS regression: OLS regression only works for linear relationships. Outliers can sometimes exert heavy influence on estimates of the relationship Don’t extrapolate beyond the scope of the data. Linearity By definition, an OLS regression line is a straight line. If the underlying relationship between x and y is non-linear, then the OLS regression line will do a poor job of measuring that relationship. One common case of non-linearity is the case of diminishing returns in which the slope gets weaker at higher values of x. Figure 48 demonstrates a class case of non-linearity in the relationship between a country’s life expectancy and GDP per capita. Figure 48: Scatterplot of GDP per capita and life expectancy across countries, 2007 The relationship is clearly a strongly positive one, but also one of diminishing returns where the positive relationship seems to plateau at higher levels of GDP per capita. This makes sense because the same absolute increase in country wealth at low levels of life expectancy can be used to reduce the incidence of well-understood infectious and parasitic diseases, whereas the same absolute increase in country wealth at high levels of life expectancy must try to reduce the risk of less understood and treatable diseases like cancer. You get more bang for your buck when life expectancy is low. Figure 49 shows what happens if we try to fit a line to this data. ## Warning: Removed 3 rows containing missing values (geom_smooth). Figure 49: Fitting a line to a non-linear relationship will cause systematic errors in your prediction Clearly a straight line is a poor fit. We systematically overestimate life expectancy at low and high GDP per capita and underestimate life expectancy in the middle. Its possible, in some circumstances, to correct for this problem of non-linearity but we will not explore those options in this module. For now, its just important to be aware of the problem and if you see clear non-linearity then you should question the use of an OLS regression line. Outliers and Influential Points An outlier is an influential point if removing this observation from the dataset substantially changes the slope of the OLS regression line. You can try the interactive exercise below to see how removing points changes the slope of your line (click on a point a second time to add it back). Can you identify any influential points? For the case of median age, Utah and DC both have fairly strong influences on the shape of the line. Removing DC makes the relationship weaker, while removing Utah makes the relationship stronger. Outliers will tend to have the strongest influence when their placement is inconsistent with the general pattern. In this case, Utah is very inconsistent with the overall negative effect because it has both low median age and low crime rates. Lets say that you have identified an influential point. What then? In truth there is only so much you can do. You cannot remove a valid data point just because it is an influential point. There are two cases where it would be legitimate to exclude the point. First, if you have reason to believe that the observation is an outlier because of a data error, then it would be acceptable to remove it. Second, if you have a strong argument that the observation does not belong with the rest of the cases, because it is logically different, then it might be OK to remove it. In our case, there is no legitimate reason to remove Utah, but there probably is a legitimate reason to remove DC. Washington DC is really a city and the rest of our observations are states that contain a mix of urban and rural population. Because crime rates are higher in urban areas, DC’s crime rates look very exaggerated compared to states. Because of this “apples and oranges” problem, it is probably better to remove DC. If our unit of analysis was cities, on the other hand, then DC should remain. In large datasets (1000+ observations), its unusual that a single point or even a small cluster of points will exert much influence on the shape of the line. The concern about influential points is mostly a concern in small datasets like the crime dataset. Thou Doth Extrapolate Too Much Its dangerous enough to assume that a linear relationship holds for your data (see the first point in this module). Its doubly dangerous to assume that this linear relationship holds beyond the scope of your data. Lets take the relationship between sexual frequency and age. We saw in the previous module that the slope here is -1.3 and the intercept is 108. The intercept itself is outside the scope of the data because we only have data on the population 18 years and older. It would be problematic to make predictions about the sexual frequency of 12 year olds, let alone zero-year olds. Another trivial example would be to look at the growth rate of children 5-15 years of age by correlating age with height. It would be acceptable to use this model to predict the height of a 14 year old, but not a 40 year old. We expect that this growth will eventually end sometime outside the range of our data when individuals reach their final adult height. If we extrapolated the data, we would predict that 40 year olds would be very tall. "],
["the-power-of-controlling-for-other-variables.html", "The Power of Controlling for Other Variables", " The Power of Controlling for Other Variables In the previous module, I showed that the OLS regression line predicting sexual frequency by years of education was 0.03. So in my dataset, there is a very small positive association between sexual frequency and years of education. Its possible that this is a causal effect. We could even spin stories about why we think such a positive association (a very small one) might exist. Maybe more educated people appear sexier to the opposite sex. Maybe more educated people take better care of themselves and thus are healthier and more able to have sex. Maybe more educated people are just more sexually liberated. Before we get carried away however its important to consider whether our results might be spurious. Its possible that the positive association between years of education and sexual frequency is driven by a third variable that we haven’t accounted for. This is a common problem in research using observational data. Association does not necessarily mean causation because of the potential for other variables to account for our observed association (and because of the possibility of reverse causation). We refer to such variables as lurking or confounding variables. In this case, the potentially confounding variable that we need to consider is age. Lets look at the association between age and each of our other variables (sexual frequency and education). cor(sex$sexf,sex$age) ## [1] -0.3974668 cor(sex$educ,sex$age) ## [1] -0.06018569 Age is negatively correlated with sexual frequency. We have observed this relationship before and it is not terribly surprising. Older people have less sex, on average. The negative correlation between age and years of education is perhaps a little more surprising. Older people have less education than younger people, on average. This may seem surprising to you because as you get older you have more opportunity to complete more education. However, you have to remember that the data we have are a snapshot in time. We are not tracking individuals over time as they age, but rather looking at differences between older and younger people at a single point in time. This kind of data is often called a cross-sectional dataset. Because we are looking at a single point in time, the age differences really reflect differences in birth cohorts or what people often loosely call “generations.” Remember that this dataset is from 2004. The difference between a 20 year old and a 60 year old is that the 20 year old was born in 1984 and the 60 year old was born in 1944. Because we are comparing birth cohorts, the differences in educational attainment reflect history more than life cycle. Older cohorts were less educated than younger birth cohorts. On average, you will be more educated than your parents and your parents were more educated than your grandparents. Thus, the correlation between age and education is negative. These two negative correlations suggest a spurious reason why we might observe a positive association between sexual frequency and education. Younger people have more education and younger people have more sex. Thus, when we look at the relationship between sexual frequency and education, we see a positive association but that positive association is indirectly driven by youth and the association of youth with both education and sex. How can we examine whether this potential spurious explanation is accurate? It turns out that we can add more than one independent variable to an OLS regression model at the same time. The mathematical structure of such a model would be: \\[\\hat{frequency}_i=b_0+b_1(education_i)+b_2(age_i)\\] We now have two different “slopes”, \\(b_1\\) and \\(b_2\\). These two slopes give the association of education and age, respectively, on sexual frequency, while controlling for the other independent variable. We now have what is called a multivariate OLS regression model. This “controlling” concept is a key point that I will return to below, but first I want to try to think graphically about what this model is doing. In the case of bivariate regression, we thought of fitting a line to a scatterplot in two-dimensional space. We are doing something similar here, but since we now have three variables, our scatterplot is in three dimensions. Figure 50 shows an interactive three-dimensional plot of the three variables. Figure 50: Interactive 3d scatterplot of years of education, age, and sexual frequency The dependent variable is shown on the vertical (z) axis and the two independent variables are shown on the “width” and “depth” axes (x and y). The flat plane shown is defined by the OLS regression model equation above. So, rather than fitting a straight line through the data, I am fitting a plane. Note however that if you rotate the 3d scatterplot to hide the age “depth” dimension, it will then look like a two-dimensional scatterplot between years of education and sexual frequency. In this case, the the edge of the plane would indicate the slope between years of education and sexual frequency. Similarly, I could rotate it the other way to look at the relationship between age and sexual frequency. How do I know what are the best values for \\(b_0\\), \\(b_1\\), and \\(b_2\\) that define my plane? The logic is the same as for bivariate OLS regression: I choose values that minimize the sum of squared residuals (SSR): \\[SSR=\\sum_{i=1}^n (\\hat{y}_i-y_i)^2\\] SSR is a measure of how far the predicted values of the dependent variable are from the actual values, so we want the intercept and slopes that minimizes this error. Unlike the bivariate case, however, there is no simple formula that I can give you for the slope and intercept, without some knowledge of matrix algebra. However, R can calculate the correct numbers for you easily. I am not concerned with your technical ability to calculate these numbers by hand, but I do want you to understand why those are the “best” numbers. They are the best numbers because they minimize the sum of the squared residuals for the model. We can calculate this model in R just by adding another variable to our model in the lm command: model &lt;- lm(sexf~educ+I(age-18), data=sex) coef(model) ## (Intercept) educ I(age - 18) ## 91.062080 -0.427747 -1.303385 Note that as I did in the previous module, I am re-centering age on 18 years so that I have reasonable value for the interpretation of the intercept. In equation form, our model will look like: \\[\\hat{frequency}_i=91.06-0.43(education_i)-1.30(age_i-18)\\] Interpreting results in a multivariate OLS regression models How do we interpret the results? Intercept: The model predicts that 18-year old individuals at with no education will have 91.06 sexual encounters per year, on average. Education Slope: The model predicts that, holding age constant, an additional year of education is associated with 0.43 fewer sexual encounters per year, on average. Age Slope: The model predicts that, holding education constant, an additional year of age is associated with 1.3 fewer sexual encounters per year, on average. The intercept is now the predicted value when all independent variables are zero. My interpretation of the slopes is almost identical to the bivariate case, except for one very important addition. I am now estimating the effect of each independent variable on the dependent variable while holding constant all other independent variables. You could also say “controlling for all other independent variables.” What does it mean to “hold other variables constant?” It means that when we look at the effect of one independent variable, we are looking at how the predicted value of the dependent variable changes while keeping all the other variables the same. For instance, the education effect above is the effect of a one year increase in education among individuals of the same age. Because we are looking at the effect of education among individuals of the same age, age should no longer have a confounding effect on our estimate of the effect of education. Thus holding constant/controlling for other variables helps to remove the potential spurious effect of those variables as confounders. Note how the effect of education on sexual frequency changed once I included age as a control variable. Before controls, I estimated a slightly positive slope (0.03) but now I am estimating a substantial negative slope (-0.43). So my understanding of the relationship between education and sexual frequency is completely reversed. When you compare individuals of the same age, more educated individuals have less sex, on average, than less educated individuals. Crime example Lets build a regression model where we predict the property crime rate in a state by the percent of adults in the state without a high school diploma and the median age of the state’s residents. summary(lm(Property~PctLessHS+MedianAge, data=crimes)) ## ## Call: ## lm(formula = Property ~ PctLessHS + MedianAge, data = crimes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1080.38 -376.78 12.19 346.82 1600.56 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5136.66 1411.21 3.640 0.000666 *** ## PctLessHS 69.47 24.79 2.803 0.007286 ** ## MedianAge -83.31 35.30 -2.360 0.022368 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 567.2 on 48 degrees of freedom ## Multiple R-squared: 0.2495, Adjusted R-squared: 0.2182 ## F-statistic: 7.977 on 2 and 48 DF, p-value: 0.001021 Note that I am giving you the full output of summary now, but we can find the slopes and intercept by looking at the Estimate column of the “Coefficients” table. “Coefficients” is another term for slopes and intercepts because that it the technical term for these values in the regression model equation. The model is: \\[\\hat{\\texttt{crime}}_i=5137+69(\\texttt{pct less hs}_i)-83(\\texttt{median age}_i)\\] The model predicts that, comparing two states with the same median age of residents, a one percent increase in the percent of the state with less than a high school diploma is associated with an increase of 69 property crimes per 100,000, on average. The model predicts that, comparing two states with the same percentage of adults without a high school diploma, a one year increase in the median age of a state’s residents is associated with a decrease of 83 property crimes per 100,000, on average. Note that we also get the \\(R^2\\) value from the summary command. In multivariate models, the \\(R^2\\) value always tells you what proportion of the variation in the dependent variable is accountable for by variation in all of the independent variables combined. In this case \\(R^2\\) is 0.2495. About 25% of the variation in property crime rates across states is accountable for by variation in the percent of adults without a high school diploma and the median age of residents across states. Including more than two independent variables If we can include two independent variables in a regression model, why stop there? Why not include three or four or more? The number of independent variables you can include is only limited by the sample size (you can never have more independent variables than the sample size minus one), although in practice we generally stop well short of this limit for pragmatic reasons. Lets take the model above predicting property crime rates by percent of adults with less than a high school diploma and the median age of residents. Lets add the poverty rate as another predictor: summary(lm(Property~PctLessHS+MedianAge+Poverty, data=crimes)) ## ## Call: ## lm(formula = Property ~ PctLessHS + MedianAge + Poverty, data = crimes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1174.43 -236.69 -30.96 286.41 1218.77 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4240.38487 1383.49276 3.065 0.0036 ** ## PctLessHS 0.04504 36.07642 0.001 0.9990 ## MedianAge -73.27098 33.68882 -2.175 0.0347 * ## Poverty 97.67933 38.52145 2.536 0.0146 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 537.6 on 47 degrees of freedom ## Multiple R-squared: 0.3398, Adjusted R-squared: 0.2976 ## F-statistic: 8.063 on 3 and 47 DF, p-value: 0.0001947 The model predicts: A one percent increase in the percent of adults in a state without a high school diploma is associated with 0.05 more property crimes per 100,000, on average, holding constant the median age of residents and the poverty rate in a state. This result is about as close to zero as you will find. A one year increase in the median age of a state’s residents is associated with 73 fewer property crimes per 100,000, on average, holding constant the percent of adults without a high school diploma and the poverty rate in a state. A one percent increase in a state’s poverty rate is associated with 98 more crimes per 100,000, on average, holding constant the percent of adults without a high school diploma and the median age of residents in a state. 34% of the variation in property crime rates across states can be accounted for by variation in the percent of adults without a high school diploma, residents’ median age, and the poverty rates across states. When I interpret the models now, I am holding constant the other two variables when I estimate the effect of each. Note that controlling for the poverty rate has a huge effect on the education variable whose effect goes from a substantial positive effect to basically zero effect. What does this tell us? Poverty rates and high school dropout rates are positively correlated and so when you don’t control for poverty rates, it looks like the high school dropout rate predicts crime because states with high high school dropout rates have high poverty rates and high poverty rates predict property crime rates. Once you control for the poverty rate, you see that it is economic deprivation not educational deprivation that is driving the crime rate. In general, the form of the multivariate regression model is: \\[\\hat{y}_i=b_0+b_1x_{i1}+b_2x_{i2}+b_3x_{i3}+\\ldots+b_px_{ip}\\] The intercept is given by \\(b_0\\). This is the predicted value of \\(y\\) when all of the independent variables are zero. The remaining \\(b\\)’s give the slopes for all of the variables up through the \\(p\\)th variable. Each of these gives the predicted change in \\(y\\) for a unit increase in that independent variable, holding all other independent variables constant. How to read a table of regression results In academic journal articles and books, the results of OLS regression models are represented in a fairly standard way. In order to understand how to read these articles, you need to understand this presentation style. Its not immediately intuitive for everyone. Table 13 below shows the typical style. In this table, I am reporting three regression models with the property crime rates as the dependent variable and three different independent variables. Table 13: OLS regression models predicting violent crime rates for US states Model 1 Model 2 Model 3 Intercept 1892.85*** 5136.66*** 4240.38** (335.33) (1411.21) (1383.49) Percent Less than HS 78.83** 69.47** 0.05 (25.58) (24.79) (36.08) Median Age -83.31* -73.27* (35.30) (33.69) Poverty Rate 97.68* (38.52) R-squared 0.16 0.25 0.34 N 51 51 51 ***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05. Standard errors in parenthesis. When reading this table and others like it, keep the following issues in mind: The first question you should ask is “what is the dependent variable?” This is the outcome that we are trying to predict. Typically, the dependent variable will be listed in the title of the table. In this case, the title tells you that the dependent variable is property crime rates and the unit of analysis is US states. The independent variables are listed on the rows of the table. In this case, I have independent variables of percent less than HS, median age, and the poverty rate. As I will explain below, just because an independent variable is listed here does not mean that it is actually included in all models. Models are listed in each column of the table. If numbers are listed for the row of a particular independent variable then that variable is included in that particular model. In this case, I have three different models. The first model only has numbers listed for Percent less than HS, so that is the only independent variable in the first model. The second model has numbers listed for Percent less than HS and Median Age, so both of these variables are included in the model. The third model includes all three variables in the model. Remember that in each case the dependent variable is the property crime rate. Within each cell with numbers listed there is a lot going on. We are primarily interested in the main number listed at the top. This number is the slope (or intercept in the case of the “Constant” row). The number in parenthesis is the standard error for each slope in the model. You could use this standard error and the slope estimate above it to calculate t-statistics and p-values exactly. However, the asterisks give you an easy visual short cut to determine the rough size of the p-value. These asterisks indicate if the p-value is below a certain level, as shown in the notes at the bottom. The cut-offs of 0.05, 0.01, and 0.001 used here are pretty standard for the discipline. So an asterisks generally means that the result is “statistically significant.” However, its important to keep in mind as noted above that these cut-offs are ultimately arbitrary and should never be confused with the substantive size of the effect itself. At the bottom, you typically get a number of summary measures of the model. The only two we care about are the number of observations and the \\(R^2\\) of the model. The advantage of organizing the table in this fashion is that we can easily see how the relationship between a given independent variable and the dependent variable changes as we add in other control variables by just looking at the numbers across a row. For example, we can see from Model 1 that the percent of the population with less than a high school diploma is initially pretty strongly positively related to violent crime rates. A one percentage point increase in this variable is associated with 78.83 more violent crimes per 100,000 population, on average. Controlling for the median age of the population in Model 2reduces this effect slightly but we still see a strong relationship. However, once we control for the poverty rate in a state the percent less than high school diploma effect completely vanishes (it becomes 0.05 which is effectively zero in this case). The poverty rate, on the other hand, has a big positive association with violent crime rates. What is going on here? It seems that the percent of the population with less than a high school diploma is only indirectly related to violent crime rates by its positive association with the poverty rate. But high poverty rates are much more directly responsible for high violent crime rates. In other words, a low high school completion rate predicts higher poverty rates and higher poverty rates predict more violent crimes, but a low high school completion rate does not directly predict more violent crimes. "],
["including-categorical-variables-as-predictors.html", "Including Categorical Variables as Predictors", " Including Categorical Variables as Predictors To this point, we only know how to include quantitative variables into OLS regression models. However, it turns out you can use a fairly easy trick to include categorical variables as independent variables in OLS regression models. By including categorical variables as independent variables, we expand considerably the range of things that we can do with OLS regression models. The most difficult part of this trick is correctly interpreting your results. Indicator variables As an example, I am going to look at the relationship between religious affiliation and sexual frequency. To keep our example simple I am going to dichotomize the religious affiliation variable, which means I am going to collapse it into two categories, rather than the six categories in the dataset. I will use a simply dichotomy of “Not Religious/Religious.” In R, I can create this variable like so: sex$norelig &lt;- sex$relig==&quot;None&quot; This is technically a boolean variable, which means it takes a TRUE or FALSE value. For our purposes, TRUE is a non-religious person. We already know how to look at the relationship between sexual frequency and this dichotomized religious affiliation variable. We can look at the mean differences in sexual frequency across our two categories: tapply(sex$sexf, sex$norelig, mean) ## FALSE TRUE ## 48.33671 59.84862 59.84862-48.33671 ## [1] 11.51191 The non-religious have sex 11.5 more times per year than the religious, on average. Hallelujah? We can represent this same mean difference in a regression model framework by using an indicator variable. An indicator variable is a variable that only takes a value of zero or one. It takes a value of one when the observation is in the indicated category and a zero otherwise. Mathematically, we would say: \\[nonrelig_i=\\begin{cases} 1 &amp; \\text{if non-religious}\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] The indicated category is the category which gets a one on the indicator variable. In this case the indicated category is non-religious. The reference category is the category that gets no indicator variable. In this case, that is just the religious group. Later on, we will see that this can become slightly more complicated. You can think of the indicator variable as an on/off switch where 1 indicates that it is “on” (i.e. the observation belongs to the indicated category) and 0 indicates that it is “off” (i.e. the observation does not belong to the indicated category). What would happen if we put this indicator variable into a regression model predicting sexual frequency like so: \\[\\hat{frequency}_i=b_0+b_1(nonrelig_i)\\] How would we interpret the slope and intercept for such a model? Figure 51 shows a scatterplot of this relationship. Figure 51: A scatterplot of the religious indicator variable by sexual frequency. Points are jittered to avoid overplotting. The mean for each group is plotted in red. Notice that all of the points align vertically either at the 0 or 1 on the x-axis. This is because the indicator variable can only take these two values. I have jittered points slightly to avoid overplotting. I have also plotted the means of the two groups in red dots and the OLS regression line for the scatterplot in blue. It turns out, that in order to be the best-fitting line, this OLS regression line must connect the two dots that represent the mean of each group. What will the slope of this line be? If we go up “one unit” on the non-religious indicator variable we have gone from a religious person to a non-religious person and the change in predicted sexual frequency is equal to the mean difference of 11.5 between the groups. The intercept is given by the value at zero which is just given by the mean sexual frequency among the religious of 48.3. So, the OLS regression line should look like: \\[\\hat{frequency}_i=48.3+11.5(nonrelig_i)\\] I can calculate these same numbers in R with the lm command: coef(lm(sexf~norelig, data=sex)) ## (Intercept) noreligTRUE ## 48.33671 11.51191 The numbers are the same. More important than the numbers, however, is the interpretation of the numbers. The intercept is the mean of the dependent variable for the reference category. The slope is the mean difference between the reference category and the indicated category. In this case, I would say: Religious individuals have sex 48.3 times per year, on average. Non-religious individuals have sex 11.5 times more per year than non-religious individuals, on average. Note that I can derive the sexual frequency of the non-religious from these two numbers by taking the value for the non-religious and adding the mean difference to find out that non-religious individuals have sex 59.8 times per year, on average. Reversing the indicator variable What if I switched my indicator variable so that the religious were indicated and the non-religious were the reference category? \\[relig_i=\\begin{cases} 1 &amp; \\text{if religious}\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] Lets try it out in R and see (the != below is computer lingo for “not equal to”): sex$religious &lt;- sex$relig!=&quot;None&quot; coef(lm(sexf~religious, data=sex)) ## (Intercept) religiousTRUE ## 59.84862 -11.51191 Lets compare the two models: \\[\\hat{frequency}_i=48.3+11.5(nonrelig_i)\\] \\[\\hat{frequency}_i=59.8-11.5(relig_i)\\] Both models give me the exact same information, but from the perspective of a different reference group. The first model tells me the mean sexual frequency of the religious (48.3) and how much more sex the non-religious have on average (11.5). The second model tells me the mean sexual frequency of the non-religious (59.8) and how much less sex the religious have (-11.5). I can easily derive one model from the other, without actually having to calculate it in R. Therefore, which category you set as the reference category is really a matter of taste, rather than one of consequence. The results are the same either way. Categorical variables with more than two categories What if I have a categorical variable that has more than two categories? Lets expand the religious variable that I dichotomized back to its original scale. There are six different categories: Fundamentalist Protestant, Mainline Protestant, Catholic, Jewish, Other, and None: summary(sex$relig) ## Fund Protestant Mainline Protestant Catholic ## 556 529 507 ## Jewish Other None ## 39 145 327 Lets look at the mean sexual frequency for each of these groups. round(tapply(sex$sexf, sex$relig, mean),1) ## Fund Protestant Mainline Protestant Catholic ## 49.6 44.2 49.2 ## Jewish Other None ## 39.8 57.9 59.8 Figure 52 plots these means on a number line to get a visual display of the differences: Figure 52: The mean sexual frequency of each group arrayed on a vertical number line. The red lines indicate the distance between each group and the reference category of fundamentalist Protestant. Nones and others clearly have much higher mean sexual frequency than the remaining religious groups and Jews have much lower mean sexual frequency. The three Christian groups cluster in the middle, although mainline protestants have a lower mean sexual frequency than the other two. This plot also shows the mean differences between the groups, with fundamentalist Protestants set as the reference category. The vertical distances from the dotted red line (the mean of fundamentalist Protestants) give the mean differences between each religious group and fundamentalist Protestants. So we can see that “Nones” have sex 10.2 more times per year than fundamentalist Protestants, on average, and mainline Protestants have sex 5.4 fewer times per year, on average, than fundamentalist Protestants. We can use the same logic of indicator variables we developed above to represent the mean differences between groups observed here in a regression model framework. However, because we now have six categories, we will need five indicator variables. You always need one less indicator variable than the number of categories. The category which doesn’t get an indicator variable is your reference category. As per the graph above, I will make Fundamentalist Protestants my reference category. Therefore, I need one indicator variable for each of the other five categories: \\[main_i=\\begin{cases} 1 &amp; \\text{if main}\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] \\[catholic_i=\\begin{cases} 1 &amp; \\text{if catholic}\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] \\[jewish_i=\\begin{cases} 1 &amp; \\text{if jewish}\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] \\[other_i=\\begin{cases} 1 &amp; \\text{if other religion}\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] \\[none_i=\\begin{cases} 1 &amp; \\text{if no religion}\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] Now lets put these variables into an OLS regression model: \\[\\hat{frequency}_i=b_0+b_1(main_i)+b_2(catholic_i)+b_3(jewish_i)+b_4(other_i)+b_5(none_i)\\] We can figure out how all this works by getting the predicted value for the member of a specific group. That respondent should get a 1 for the variable where they are a member and a zero on all other variables. For example, a fundamentalist protestant should get a zero on all of these variables: \\[\\hat{frequency}_i=b_0+b_1(0)+b_2(0)+b_3(0)+b_4(0)+b_5(0)=b_0\\] So, the intercept is the predicted value for fundamentalist Protestants. Similarly we could calculate the predicted value for mainline Protestants: \\[\\hat{frequency}_i=b_0+b_1(1)+b_2(0)+b_3(0)+b_4(0)+b_5(0)=b_0+b_1\\] The difference between the two is \\(b_1\\), so this “slope” gives the mean difference between mainline and fundamentalist Protestants. We could do the same thing for Catholics: \\[\\hat{frequency}_i=b_0+b_1(0)+b_2(1)+b_3(0)+b_4(0)+b_5(0)=b_0+b_2\\] The mean difference between Catholics and fundamentalist Protestants is given by \\(b_2\\). In general, each of the “slopes” is the mean difference between the indicated category and the reference category. In this case, the reference category is fundamentalist Protestants so each of the slopes gives the mean difference between that religious category and fundamentalist Protestant, just like the graph above. R is fairly intelligent about handling all of these indicator variables and you don’t actually have to create these five different variables. If you put a categorical variable into your regression formula, R will know to treat it as a set of indicator categories. The only catch is that R will already have a default category set as the reference. It just so happens that in our GSS data, fundamentalist Protestants are already set as the reference. So I can run this model by: model &lt;- lm(sexf~relig, data=sex) round(coef(model),2) ## (Intercept) religMainline Protestant religCatholic ## 49.60 -5.44 -0.36 ## religJewish religOther religNone ## -9.84 8.30 10.25 You can tell which category is the reference by which category is left out here. Note how the coefficients (given by the estimates column) match the mean differences I calculated above in the graph. We are simply reproducing these mean differences in a regression model framework. Categorical and quantitative variables combined in a single model If all we are doing is reproducing mean differences between categories, what good is this method? After all, we already know how to do that. The major advantage of putting these mean differences into a regression model framework is that we can control for other potentially confounding variables. These sexual frequency differences by religious affiliation are a prime example. Lets take a look at the age differences between religious affiliations: round(tapply(sex$age, sex$relig, mean),1) ## Fund Protestant Mainline Protestant Catholic ## 45.4 47.3 44.9 ## Jewish Other None ## 53.7 38.6 39.5 Notice how closely these age differences mirror the differences in sexual frequency. Others and nones are the youngest, while Jews are the oldest. Among Christians, mainline Protestants are older than fundamentalist Protestants and Catholics. We also know from prior work that age has a negative effect on sexual frequency. This should make us suspicious that some (or all) of the observed differences in sexual frequency between religious groups simply reflect age differences between those groups. We can easily address this issue by simply including age as a control variable in our model: model &lt;- lm(sexf~relig+age, data=sex) round(coef(model),2) ## (Intercept) religMainline Protestant religCatholic ## 107.90 -3.04 -0.96 ## religJewish religOther religNone ## 0.88 -0.43 2.69 ## age ## -1.28 We now interpret those slopes as the mean difference in sexual frequency between fundamentalist Protestants and the indicated category, among individuals of the same age. So for example, we would interpret the 2.69 on “None” as: The model predicts that, among individuals of the same age, those with no religious preference have sex 2.69 more times per year than fundamentalist protestants, on average. We would also interpret the age effect controlling for religious affiliation like so: The model predicts, that holding religious affiliation constant, a one year increase in age is associated with 1.28 fewer instances of sex per year, on average. Table 14 below helps to highlight the change in the effects once age is controlled. Table 14: OLS regression models predicting sexual frequency Model 1 Model 2 Intercept 49.60*** 107.90*** (2.26) (3.68) Mainline Protestant -5.44 -3.04 (3.24) (2.99) Catholic -0.36 -0.96 (3.28) (3.02) Jewish -9.84 0.88 (8.84) (8.17) Other 8.30 -0.43 (4.98) (4.61) None 10.25** 2.69 (3.72) (3.45) Age -1.28*** (0.07) R-squared 0.01 0.16 N 2103 2103 ***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05. Standard errors in parenthesis. Reference category is fundamentalist Protestant All of the coefficients (except for Catholic, which was tiny anyway) have declined substantially in size. The mean differences from fundamentalist Protestants for both Jews and other religions have basically disappeared and the “None” effect has been severely reduced. In other words, almost all or all of the observed differences in sexual frequency by religious affiliation were indirectly a product of underlying age differences between religious affiliations. If you were just about ready to convert to a different religion to get laid more often, you may want to hold off for the moment. "],
["interaction-terms.html", "Interaction Terms", " Interaction Terms By definition, a linear model is an additive model. As you increase or decrease the value of one independent variable you increase or decrease the predicted value of the dependent variable by a set amount, regardless of the other values of the independent variable. This is an assumption built into the linear model by its additive form, and it may misrepresent some relationships where independent variables interact with one another to produce more complicated effects. In particular, in this section, we want to know whether the effect (i.e. the slope) of one independent variable varies by the value of another independent variable. The nature of additive models As an example for this section, I am going to look at the relationship between movie genre, runtime, and tomato meter ratings. To simplify things, I am going to only look at these relationships for two genres: action and comedy. I can limit my movies dataset to these two genres with the following command: movies.short &lt;- subset(movies, Genre==&quot;Comedy&quot; | movies$Genre==&quot;Action&quot;) Now lets look at a simple model where genre and runtime both predict Tomato Meter ratings. round(coef(lm(TomatoMeter~Genre+Runtime, data=movies.short)),2) ## (Intercept) GenreComedy Runtime ## -1.75 4.43 0.41 Genre is a categorical variable and action movies are set as the reference category. In equation form, the model looks like: \\[\\hat{meter}_i=-1.75+4.43(comedy_i)+0.31(runtime_i)\\] I can interpret my slopes as follows: The model predicts that when comparing movies of the same runtime, comedies have Tomato Meter ratings 4.43 percentage points higher than action movies, on average. The model predicts that, holding constant movie genre, a one minute increase in movie runtime is associated with a 0.31 percentage point increase in the Tomato Meter rating, on average. This is an additive model. If we move from an action movie to a comedy of the same runtime, our predicted Tomato Meter rating goes up by 4.43, regardless of the actual value of runtime. If we increase movie runtime by one minute while keeping genre the same, our predicted Tomato Meter rating goes up by 0.41, regardless of whether that genre is action or comedy. It may help to graphically visualize the nature of this additive relationship. We can do this by plotting lines showing the relationship between runtime and Tomato Meter ratings separately for our two different genres of action and comedy. The line for action movies is given by: \\[\\hat{meter}_i=-1.75+4.43(0)+0.41(runtime_i)=-1.75+0.41(runtime_i)\\] The line for comedy movies is given by: \\[\\hat{meter}_i=-1.75+4.43(1)+0.41(runtime_i)=2.68+0.41(runtime_i)\\] Each line has an intercept and a slope. Notice that the intercepts are different but the slopes are the same. That means we have two parallel lines at different levels. Figure 53 overlays these two parallel lines on top of a scatterplot of movie runtime by tomato meter for these two genres. Figure 53: Predicted Tomato Meter by runtime for two genres based on an additive OLS regression model. The lines must be parallel. The parallel lines are an assumption of the OLS regression model structure we have used. There are two consequences of this assumption. First, At every single level of runtime, the predicted Tomato Meter difference between comedy and action movies is exactly 4.62. This can be seen on the graph by the consistent gap between the blue and red line. Second, the effect of runtime on the Tomato Meter rating is assumed to be the same for action and comedy movies. This can be seen on the graph by the fact that both lines have the exact same slope. Although these may seem like two different issues, they are really the same issue from different perspectives. If we were to allow the slopes of the blue and red line to be different, then the gap between them would not be static. The questions is how can we allow the slopes of the two lines to be different. This is where the concept of the interaction term comes in. The interaction term An interaction term is a variable that is constructed from two other variables by multiplying those two variables together. In our case, we can easily construct an interaction term as follows: movies.short$comedy &lt;- movies.short$Genre==&quot;Comedy&quot; movies.short$interaction &lt;- movies.short$Runtime*movies.short$comedy In this case, I had to create a real indicator variable for comedy before I could multiply them, but then I just multiply this indicator variable by movie runtime. Now lets add this interaction term to the model: model &lt;- lm(TomatoMeter~Runtime+comedy+interaction, data=movies.short) round(coef(model), 2) ## (Intercept) Runtime comedyTRUE interaction ## -14.45 0.52 24.36 -0.19 We now have an additional “slope” for the interaction term. Lets write this model out in equation form to try to figure out what is going on here. \\[\\hat{meter}_i=-14.45+24.36(comedy_i)+0.52(runtime_i)-0.19(runtime_i*comedy_i)\\] Remember that the interaction term is just a literal multiplication of the two other variables. To figure out how this all works, lets once again separate this into two lines predicting Tomato Meter by runtime, for comedies and action movies separately. For action movies, the equation is: \\[\\hat{meter}_i=-14.45+24.36(0)+0.52(runtime_i)-0.19(runtime_i*0)=-14.45+0.52(runtime_i)\\] For comedy movies, the equation is: \\[\\hat{meter}_i=-14.45+24.36(1)+0.52(runtime_i)-0.19(runtime_i*1)=(-14.45+24.36)+(0.52-0.19)(runtime_i)=9.91+0.33(run_i)\\] We now have two lines with different intercept and different slopes. The interaction term has allowed the effect of runtime on the Tomato Meter to vary by type of genre. In this case, the interaction term tells us how much smaller the slope is for comedy movies than for action movies. We can also just plot the lines to see how it looks, as I have done in Figure 54. Figure 54: An interaction term allows for non-parallel lines, and thus different effects of runtime on tomato meter ratings by genre The pattern here is fairly clear. Short comedies get better ratings than short action movies, while long comedies get worse ratings than long action movies. Put another way, comedies get less “return” in terms of their ratings when increasing their length than do action movies. This can be seen by the much steeper slope for action movies. Interpreting interaction terms Interpreting interaction terms can be tricky, because the inclusion of an interaction term also changes the meaning of other slopes in the model. The slopes for the two variables that make up the interaction term are called the main effects. In our example, those two variables are runtime and the comedy indicator variable and the main effects of these variables are 0.52 and 24.36, respectively. The most important rule to remember is that when an interaction term is in a model, the main effects are only the expected effects when the other variable involved in the interaction is zero. This is because the interaction implies that the effects of the two variables are not constant but rather change depending on the value of the other variable in the interaction term. Therefore, we can only interpret effects at a particular value of the other variable. So I would interpret these main effects as follows: The model predicts that among action movies, a one minute increase in movie runtime is associated with a 0.52 point increase in the Tomato Meter rating, on average. The model predicts that among movies with zero minutes of runtime (outside the scope of data of course), comedies are predicted to have Tomato Meter ratings 24.36 points higher than action movies, on average. Notice that I did not have to say I was controlling for the other variable. I am doing more than controlling when I include an interaction term. I am conditioning the effect of one variable on the value of another. That is why I instead use the phrase “among observations that are zero on the other variable.” Note that you could also include other non-interacted variables in this model as well, like maturity rating, in which case you would also need to indicate that you controlled for those variables. Interpreting interaction terms themselves can also be tricky because they are the difference in the effect of on variable depending on the value of another. One approach is to interpret this difference in effect directly. In this case, we would say: The model predicts that the predicted increase in Tomato Meter ratings for a one minute increase in movie runtime is 0.19 points smaller for comedy movies than for action movies, on average. You have to be careful with this type of interpretation. In this case, both slopes were still positive so I can talk about how the effect was smaller. However, in some cases, the slopes may end up in different directions entirely which would require a somewhat different interpretation. Another approach is to actually calculate the slope for the indicated category (comedies) and interpret it directly: The model predicts that among comedy movies, a one minute increase in movie runtime is associated with a 0.33 increase in the Tomato Meter rating, on average (which is lower than for action movies). In short, you have to be careful and thoughtful when thinking about how to interpret interaction terms. Interaction terms in R In the example above, I created the interaction term manually, but I didn’t actually need to do this. R has a shortcut method for calculating interaction terms: model &lt;- lm(TomatoMeter~Runtime*Genre, data=movies.short) round(coef(model),2) ## (Intercept) Runtime GenreComedy ## -14.45 0.52 24.36 ## Runtime:GenreComedy ## -0.19 The results are exactly the same as before. To include an interaction term between two variables I just have to connect them with a * rather than a + in the lm formula. By default, R will include each variable separately as well as their interaction. Interaction terms with multiple categories In the above example, I only compared comedy and action movies in order to keep the comparison simple, but it is possible to run the same analysis on the full movie dataset to see how runtime varies across all genres. model &lt;- lm(TomatoMeter~Runtime*Genre, data=movies) round(coef(model),2) ## (Intercept) Runtime ## -14.45 0.52 ## GenreAnimation GenreComedy ## 11.96 24.36 ## GenreDrama GenreFamily ## 59.50 -29.06 ## GenreHorror GenreMusical/Music ## 2.51 21.07 ## GenreMystery GenreRomance ## 2.30 66.59 ## GenreSciFi/Fantasy GenreThriller ## 5.92 12.26 ## Runtime:GenreAnimation Runtime:GenreComedy ## 0.14 -0.19 ## Runtime:GenreDrama Runtime:GenreFamily ## -0.39 0.32 ## Runtime:GenreHorror Runtime:GenreMusical/Music ## -0.03 -0.12 ## Runtime:GenreMystery Runtime:GenreRomance ## 0.06 -0.52 ## Runtime:GenreSciFi/Fantasy Runtime:GenreThriller ## -0.03 -0.04 Thats a lot of numbers! There is a slope for each genre except action (10 in all) and an interaction between runtime and each genre except action (another 10 in all). What we are estimating here are 11 different lines (on for each genre) for the relationship between runtime and Tomato Meter rating. Because action movies are the reference, the main effect of runtime is the slope for action movies (0.52). The interaction terms show us how much larger or smaller the effect of runtime is for each given genre. So the effect is 0.39 smaller for dramas for a total effect of 0.13 (0.52-0.39). It is 0.32 larger for family movies for a total effect of 0.84 (0.52+0.32), and so forth. Similarly, the intercept is the intercept only for action movies. To get the intercept for other genres, we take the intercept value itself and add the main effect of genre. So for dramas the intercept is -14.45+59.5=45.05 and for family movies it is -14.45-29.06=-43.51. If we put all these slopes and intercepts together, we will get 11 lines as shown in Figure 55. Figure 55: Interaction terms allow each genre to get a different return from increasing runtime There is a lot going on here, but we can detect some interesting patterns. Almost all of the lines are positive indicating that longer movies tend to generally get better ratings. This is not true of Romances however, where there is a slight negative relationship between movie runtime and Tomato Meter ratings. Dramas also have a fairly flat slope and a high intercept, so they tend to outperform most other short movies but don’t fare as well compared to other genres when they are longer. The steepest slope is for family movies, which apparently are horrible when short (think “Beethoven 6: Beethoven saves Christmas, again” or something), but do much better when longer. Interaction terms with two categorical variables The examples so far have involved interacting a quantitative variable with a categorical variable which gives you a different line for each category of your categorical variable. However, we can also create an interaction term between two categorical variables. As an example, lets look at differences in wages in the earnings dataset by race and education. To simplify things, I am going to dichotimize race into white/non-white and education into less than Bachelor’s degree/Bachelor’s degree or more, as follows: earnings$nwhite &lt;- earnings$race!=&quot;White&quot; earnings$college &lt;- as.numeric(earnings$educ)&gt;3 Lets look at mean wages across these combination of categories: tapply(earnings$wages, earnings[,c(&quot;nwhite&quot;,&quot;college&quot;)], mean) ## college ## nwhite FALSE TRUE ## FALSE 19.99246 33.91417 ## TRUE 16.74181 31.82208 White college graduates make $33.91 per hour, on average, while non-white college graduates make $31.82 per hour, on average. Whites without a college degree make $19.99, on average, while non-whites without a college degree make $16.74, on average. If we put this in a table, I can show that there are four different ways to make comparisons between these numbers. Table 15: Mean wages in dollars per hour by race and education No degree Bachelor’s degree Difference White 19.99 33.91 13.92 Non-white 16.74 31.82 15.08 Difference -3.25 -2.09 1.16 If we look at the two differences along the far-right column, we are seeing the “returns” in terms of wages for a college degree separately for whites and non-whites. The return for whites is $13.92 per hour and the return for non-whites is higher at $15.08. If we look at the differences along the bottom row, we are seeing the racial inequality in wages separately for those with no degree and those with a college degree. Among those with no college degree, non-whites make $3.25 less per hour than whites. Among those with a college degree, non-whites make $2.09 less per hour than whites. The racial gap in wages gets smaller among those who have completed a college degree. Now lets look at the difference in the differences. For the racial gap in wages this is given by -3.25-(-2.09)=1.16. For the returns to a college degree this is given by 15.08-13.92=1.16. The difference in the differences is the same! This is because we are looking at the same relationship in two different ways. If non-whites get a better return to college than whites, then the racial gap in wages must get smaller among the college-educated. Similarly, if the racial gap in wages gets smaller at the college level, it tells us that non-whites must get a better return on their college education. This 1.16 number is basically an interaction term. We can interpret the number as the difference in returns to wages from a college degree between whites and non-whites. Alternatively, we can interpret the number as the difference in the racial wage gap between those with no degree and those with a college degree. Either way, we have the same information, with the same finding: greater educational attainment reduces racial inequality because minorities get a greater return on their college degrees. Lets try modeling this relationship with an OLS regression model. First lets try a model without interaction terms: model &lt;- lm(wages~nwhite+college, data=earnings) coef(model) ## (Intercept) nwhiteTRUE collegeTRUE ## 19.850655 -2.866436 14.263698 Lets put this into an equation framework: \\[\\hat{wages}_i=19.85-2.87(nwhite_i)+14.26(college_i)\\] We can use this equation to fill in the predicted valued of the same table we calculated by hand above: Table 16: Predicted wages in dollars per hour by race and education from an additive model No degree Bachelor’s degree Difference White 19.85 19.85+14.26=34.11 14.26 Non-white 19.85-2.87=16.98 19.85-2.87+14.26=31.24 14.26 Difference -2.87 -2.87 0 The predicted values do not match the exact values above. More importantly, if you look at the differences, you can see that the returns to education are assumed to be identical for whites and non-whites ($14.26) and the racial gap is assumed to be the same for those with no degree and those with a college degree (-$2.87). This is the limitation of the additive model. We assume that the effects of race and college completion are not affected by each other. If we want to determine whether returns to college are different by race, we need to model the interaction term, as follows: model &lt;- lm(wages~nwhite*college, data=earnings) coef(model) ## (Intercept) nwhiteTRUE collegeTRUE ## 19.992457 -3.250648 13.921709 ## nwhiteTRUE:collegeTRUE ## 1.158565 In equation form: \\[\\hat{income}_i=19.99-3.25(nwhite_i)+13.92(college_i)+1.16(nwhite_i*college_i)\\] Lets use this model to get predicted values in our table: Table 17: Predicted wages in dollars per hour by race and education from an interactive model No degree Bachelor’s degree Difference White 19.99 19.99+13.92=33.91 13.92 Non-white 19.99-3.25=16.74 19.99-3.25+13.92+1.16=31.82 15.08 Difference -3.25 -2.09 1.16 Our model now fits the data exactly and the differences are allowed to vary by the other category, so that we can see the differences in returns to college by race and the differences in the racial gap by education level. The interaction term itself of 1.16 is the same to what we calculated by hand. If we were to interpret the intercept and slopes from the model above, we would say: Whites with no college degree had mean wages of $19.99 per hour. Among those with no college degree, non-whites earn $3.25 less per hour than whites, on average. Among whites, those with a college degree have wages $13.92 per hour higher on average than those without a college degree. The returns to wages from a college degree are $1.16 larger for non-whites than they are for whites, on average. "],
["model-complications.html", "Model Complications", " Model Complications In this chapter, we will expand our understanding of the linear model to address many issues that the practical researcher must face. We begin with a review and reformulation of the linear model. We then move on to discuss how to address violations of assumptions such as non-linearity and heteroskedasticity (yes, this is a real word), sample design and weighting, missing values, multicollinearity, and model selection. By the end of this chapter, you will be well-supplied with the tools for conducting a real-world analysis using the linear model framework. "],
["the-linear-model-revisited.html", "The Linear Model, Revisited", " The Linear Model, Revisited Reformulating the linear model Up until now, we have used the following equation to describe the linear model mathematically: \\[\\hat{y}_i=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\ldots+\\beta_px_{ip}\\] In this formulation, \\(\\hat{y}_i\\) is the predicted value of the dependent variable for the \\(i\\)th observation and \\(x_{i1}\\), \\(x_{i2}\\). through \\(x_{ip}\\) are the values of \\(p\\) independent variables that predict \\(\\hat{y}_i\\) by a linear function. \\(\\beta_0\\) is the y-intercept which is the predicted value of dependent variable when all the independent variables are zero. \\(\\beta_1\\) through \\(\\beta_p\\) are the slopes giving the predicted change in the dependent variable for a one unit increase in a given independent variable holding all of the other variables constant. This formulation is useful, but we can now expand and re-formulate it in a way that will help us understand some of the more advanced topics we will discuss in this and later chapters. The formula is above is only the “structural” part of the full linear model. The full linear model is given by: \\[y_i=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\ldots+\\beta_px_{ip}+\\epsilon_i\\] I have changed two things in this new formula. On the left-hand side, we have the actual value of the dependent variable for the \\(i\\)th observation. In order to make things balance on the right-hand side of the equation, I have added \\(\\epsilon_i\\) which is simply the residual or error term for the \\(i\\)th observation. We now have a full model that predicts the actual values of \\(y\\) from the actual values of \\(x\\). If we compare this to the first formula, it should become clear that every term except the residual term can be substituted for by \\(\\hat{y}_i\\). So, we can restate our linear model using two separate formulas as follows: \\[\\hat{y}_i=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\ldots+\\beta_px_{ip}\\] \\[y_i=\\hat{y}_i+\\epsilon_i\\] By dividing up the formula into two separate components, we can begin to think of our linear model as containing a structural component and a random (or stochastic) component. The structural component is the linear equation predicting \\(\\hat{y}_i\\). This is the formal relationship we are trying to fit between the dependent and independent variables. The stochastic component on the other hand is given by the \\(\\epsilon_i\\) term in the second formula. In order to get back to the actual values of the dependent variable, we have to add in the residual component that is not accounted for by the linear model. From this perspective, we can rethink our entire linear model as a partition of the total variation in the dependent variable into the structural component that can be accounted for by our linear model and the residual component that is unaccounted for by the model. This is exactly as we envisioned things when we learned to calculate \\(R^2\\) in previous chapters. Estimating a linear model Up until now, we have not discussed how R actually calculates all of the slopes and intercept for a linear model with multiple independent variables. We only know the equations for a linear model with one independent variable. Even though we don’t know the math yet behind how linear model parameters are estimated, we do know the rationale for why they are selected. We choose the parameters that minimized the sum of squared residuals given by: \\[\\sum_{i=1}^n (y_i-\\hat{y}_i)^2\\] In this section, we will learn the formal math that underlies how this estimation occurs, but first a warning: there is a lot of math ahead. In normal everyday practice, you don’t have to do any of this math, because R will do it for you. However, it is useful to know how linear model estimating works “under the hood” and it will help with some of the techniques we will learn later in the book. Matrix algebra crash course In order to learn how to estimate linear model parameters, we will need to learn a little bit of matrix algebra. In matrix algebra we can collect numbers into vectors which are single dimension arrays of numbers and matrices which are two-dimensional arrays of numbers. We can use matrix algebra to represent our linear regression model equation using one-dimensional vectors and two-dimensional matrices. We can imagine \\(y\\) below as a vector of dimension 3x1 and \\(\\mathbf{X}\\) as a matrix of dimension 3x3. \\[ \\mathbf{y}=\\begin{pmatrix} 4\\\\ 5\\\\ 3\\\\ \\end{pmatrix} \\mathbf{X}= \\begin{pmatrix} 1 &amp; 7 &amp; 4\\\\ 1 &amp; 3 &amp; 2\\\\ 1 &amp; 1 &amp; 6\\\\ \\end{pmatrix} \\] We can multiply vectors and matrices together by taking each element in the row of a matrix by the corresponding element in the vector and summing them up: \\[\\begin{pmatrix} 1 &amp; 7 &amp; 4\\\\ 1 &amp; 3 &amp; 2\\\\ 1 &amp; 1 &amp; 6\\\\ \\end{pmatrix} \\begin{pmatrix} 4\\\\ 5\\\\ 3\\\\ \\end{pmatrix}= \\begin{pmatrix} 1*4+7*5+3*4\\\\ 1*4+3*5+2*3\\\\ 1*4+1*5+6*3\\\\ \\end{pmatrix}= \\begin{pmatrix} 51\\\\ 25\\\\ 27\\\\ \\end{pmatrix}\\] You can also transpose a vector or matrix by flipping its rows and columns. My transposed version of \\(\\mathbf{X}\\) is \\(\\mathbf{X}&#39;\\) which is: \\[\\mathbf{X}= \\begin{pmatrix} 1 &amp; 1 &amp; 1\\\\ 7 &amp; 3 &amp; 1\\\\ 4 &amp; 2 &amp; 6\\\\ \\end{pmatrix}\\] You can also multiple matrices by each other using the same pattern as for multiplying vectors and matrices but now you start a new column each time you move down a row of the first matrix. So to “square” my matrix: \\[ \\begin{eqnarray*} \\mathbf{X}&#39;\\mathbf{X}&amp;=&amp; \\begin{pmatrix} 1 &amp; 1 &amp; 1\\\\ 7 &amp; 3 &amp; 1\\\\ 4 &amp; 2 &amp; 6\\\\ \\end{pmatrix} \\begin{pmatrix} 1 &amp; 7 &amp; 4\\\\ 1 &amp; 3 &amp; 2\\\\ 1 &amp; 1 &amp; 6\\\\ \\end{pmatrix}&amp;\\\\ &amp; =&amp; \\begin{pmatrix} 1*1+1*1+1*1 &amp; 1*7+1*3+1*1 &amp; 1*4+1*2+1*6\\\\ 7*1+3*1+1*1 &amp; 7*7+3*3+1*1 &amp; 7*4+3*2+1*6\\\\ 4*1+2*1+6*1 &amp; 4*7+2*3+6*1 &amp; 4*4+2*2+6*6\\\\ \\end{pmatrix}\\\\ &amp; =&amp; \\begin{pmatrix} 3 &amp; 11 &amp; 12\\\\ 11 &amp; 59 &amp; 40\\\\ 12 &amp; 40 &amp; 56\\\\ \\end{pmatrix} \\end{eqnarray*} \\] R can help us with these calculations. the t command will transpose a matrix or vector and the %*% operator will to matrix algebra multiplication. y &lt;- c(4,5,3) X &lt;- rbind(c(1,7,4),c(1,3,2),c(1,1,6)) X ## [,1] [,2] [,3] ## [1,] 1 7 4 ## [2,] 1 3 2 ## [3,] 1 1 6 t(X) ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 7 3 1 ## [3,] 4 2 6 X%*%y ## [,1] ## [1,] 51 ## [2,] 25 ## [3,] 27 t(X)%*%X ## [,1] [,2] [,3] ## [1,] 3 11 12 ## [2,] 11 59 40 ## [3,] 12 40 56 #a shortcut for squaring a matrix crossprod(X) ## [,1] [,2] [,3] ## [1,] 3 11 12 ## [2,] 11 59 40 ## [3,] 12 40 56 We can also calculate the inverse of a matrix. The inverse of a matrix (\\(\\mathbf{X}^{-1}\\)) is the matrix that when multiplied by the original matrix produces the identity matrix which is just a matrix of ones along the diagonal cells and zeroes elsewhere. Anything multiplied by the identity matrix is just itself, so the identity matrix is like 1 at the matrix algebra level. Calculating an inverse is a difficult calculation that I won’t go through here, but R can do it for us easily with the solve command: inverse.X &lt;- solve(X) inverse.X ## [,1] [,2] [,3] ## [1,] -0.8 1.9 -0.1 ## [2,] 0.2 -0.1 -0.1 ## [3,] 0.1 -0.3 0.2 round(inverse.X %*% X,0) ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 Linear model in matrix form We now have enough matrix algebra under our belt that we can re-specify the linear model equation in matrix algebra format: \\[\\mathbf{y}=\\mathbf{X\\beta+\\epsilon}\\] \\(\\begin{gather*} \\mathbf{y}=\\begin{pmatrix} y_{1}\\\\ y_{2}\\\\ \\vdots\\\\ y_{n}\\\\ \\end{pmatrix} \\end{gather*}\\), \\(\\begin{gather*} \\mathbf{X}= \\begin{pmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; \\ldots &amp; x_{1p}\\\\ 1 &amp; x_{21} &amp; x_{22} &amp; \\ldots &amp; x_{2p}\\\\ \\vdots &amp; \\vdots &amp; \\ldots &amp; \\vdots\\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; \\ldots &amp; x_{np}\\\\ \\end{pmatrix} \\end{gather*}\\), \\(\\begin{gather*} \\mathbf{\\epsilon}=\\begin{pmatrix} \\epsilon_{1}\\\\ \\epsilon_{2}\\\\ \\vdots\\\\ \\epsilon_{n}\\\\ \\end{pmatrix} \\end{gather*}\\), \\(\\begin{gather*} \\mathbf{\\beta}=\\begin{pmatrix} \\beta_{1}\\\\ \\beta_{2}\\\\ \\vdots\\\\ \\beta_{p}\\\\ \\end{pmatrix} \\end{gather*}\\) Where: \\(\\mathbf{y}\\) is a vector of known values of the independent variable of length \\(n\\). \\(\\mathbf{X}\\) is a matrix of known values of the independent variables of dimensions \\(n\\) by \\(p+1\\). This matrix is sometimes referred to as the design matrix. \\(\\mathbf{\\beta}\\) is a vector of to-be-estimated values of intercepts and slopes of length \\(p+1\\). \\(\\mathbf{\\epsilon}\\) is a vector of residuals of length \\(n\\) that will be equal to \\(\\mathbf{y-X\\beta}\\). Note that we only know \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\). We need to estimate a \\(\\mathbf{\\beta}\\) vector from these known quantities. Once we know the \\(\\mathbf{\\beta}\\) vector, we can calculate \\(\\mathbf{\\epsilon}\\) by just taking \\(\\mathbf{y-X\\beta}\\). We want to estimate \\(\\mathbf{\\beta}\\) in order to minimize the sum of squared residuals. We can represent this sum of squared residuals in matrix algebra format as a function of \\(\\mathbf{\\beta}\\): \\[\\begin{eqnarray*} SSR(\\beta)&amp;=&amp;(\\mathbf{y}-\\mathbf{X\\beta})&#39;(\\mathbf{y}-\\mathbf{X\\beta})\\\\ &amp;=&amp;\\mathbf{y}&#39;\\mathbf{y}-2\\mathbf{y}&#39;\\mathbf{X\\beta}+\\mathbf{\\beta}&#39;\\mathbf{X&#39;X\\beta} \\end{eqnarray*}\\] If you remember the old FOIL technique from high school algebra (first, outside, inside, last) that is exactly what we are doing here, in matrix algebra form. We now have a function that defines our sum of squared residuals. We want to choose the values of \\(\\mathbf{\\beta}\\) that minimize the value of this function. In order to do that, I need to introduce a teensy bit of calculus. To find the minimum (or maximum) value of a function, you calculate the derivative of the function with respect to the variable you care about and then solve for zero. Technically, you also need to calculate second derivatives in order to determine if its a minimum or maximum, but since this function has no maximum value, we know that the result has to be a minimum. I don’t expect you to learn calculus for this course, so I will just mathemagically tell you that the derivative of \\(SSR(\\beta)\\) with respect to \\(\\mathbf{\\beta}\\) is given by: \\[-2\\mathbf{X&#39;y}+2\\mathbf{X&#39;X\\beta}\\] We can now set this to zero and solve for \\(\\mathbf{\\beta}\\): \\[\\begin{eqnarray*} 0&amp;=&amp;-2\\mathbf{X&#39;y}+2\\mathbf{X&#39;X\\beta}\\\\ -2\\mathbf{X&#39;X\\beta}&amp;=&amp;-2\\mathbf{X&#39;y}\\\\ (\\mathbf{X&#39;X})^{-1}\\mathbf{X&#39;X\\beta}&amp;=&amp;(\\mathbf{X&#39;X})^{-1}\\mathbf{X&#39;y}\\\\ \\mathbf{\\beta}&amp;=&amp;(\\mathbf{X&#39;X})^{-1}\\mathbf{X&#39;y}\\\\ \\end{eqnarray*}\\] Ta-dah! We have arrived at the matrix algebra solution for the best fitting parameters for a linear model with any number of independent variables. Lets try this formula out in R: X &lt;- as.matrix(cbind(rep(1, nrow(movies)), movies[,c(&quot;Runtime&quot;,&quot;BoxOffice&quot;)])) head(X) ## rep(1, nrow(movies)) Runtime BoxOffice ## 164 1 118 47.1 ## 165 1 104 3.9 ## 166 1 122 30.7 ## 167 1 111 11.4 ## 168 1 123 39.9 ## 169 1 116 32.4 y &lt;- movies$TomatoMeter beta &lt;- solve(crossprod(X))%*%crossprod(X,y) beta ## [,1] ## rep(1, nrow(movies)) 11.09207358 ## Runtime 0.32320152 ## BoxOffice 0.05923506 model &lt;- lm(TomatoMeter~Runtime+BoxOffice, data=movies) coef(model) ## (Intercept) Runtime BoxOffice ## 11.09207358 0.32320152 0.05923506 It works! We can also estimate standard errors using this matrix algebra format. To get the standard errors, we first need to calculate the covariance matrix. \\[\\sigma^{2}(\\mathbf{X&#39;X})^{-1}\\] The \\(\\sigma^2\\) here is the variance of our residuals. Technically, this is the variance of the residuals in the population but since we usually only have a sample we actually calculate: \\[s^2=\\frac{\\sum(y_i-\\hat{y}_i)^2}{n-p-1}\\] based on the fitted values of \\(\\hat{y}_i\\) from our linear model. \\(p\\) is the number of independent variables in our model. The covariance matrix actually provides us with some interesting information about the correlation of our independent variables. For our purposes we just want the square root of the diagonal elements, which gives us the standard error for our model. Continuing with the example estimating tomato meter ratings by run time and box office returns in our movies dataset, lets calculate all the numbers we need for an inference test: #get the predicted values of y y.hat &lt;- X%*%beta df &lt;- length(y)-ncol(X) #calculate our sample variance of the residual terms s.sq &lt;- sum((y-y.hat)^2)/df #calculate the covariance matrix covar.matrix &lt;- s.sq*solve(crossprod(X)) #extract SEs from the square root of the diagonal se &lt;- sqrt(diag(covar.matrix)) ## calculate t-stats from our betas and SEs t.stat &lt;- beta/se #calculate p-values from the t-stat p.value &lt;- 2*pt(-1*abs(t.stat), df) data.frame(beta,se,t.stat,p.value) ## beta se t.stat p.value ## rep(1, nrow(movies)) 11.09207358 3.269128357 3.392976 7.019316e-04 ## Runtime 0.32320152 0.031734307 10.184609 6.617535e-24 ## BoxOffice 0.05923506 0.007978845 7.424014 1.540215e-13 #compare to the lm command summary(model)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.09207358 3.269128357 3.392976 7.019316e-04 ## Runtime 0.32320152 0.031734307 10.184609 6.617535e-24 ## BoxOffice 0.05923506 0.007978845 7.424014 1.540215e-13 And it all works. As I said at the beginning, this is a lot of math and not something you need to think about every day. I am not expecting all of this will stick the first time around, but I want it to be here for you as a handy reference for the future. Linear model assumptions Two important assumptions underlie the linear model. The first assumption is the assumption of linearity. In short, we assume that the the relationship between the dependent and independent variables is best described by a linear function. In prior chapters, we have seen examples where this assumption of linearity was clearly violated. When we choose the wrong model for the given relationship, we have made a specification error. Fitting a linear model to a non-linear relationship is one of the most common forms of specification error. However, this assumption is not as deadly as it sounds. Provided that we correctly diagnose the problem, there are a variety of tools that we can use to fit a non-linear relationship within the linear model framework. We will cover this techniques in the next section. The second assumption of linear models is that the residual or error terms \\(\\epsilon_i\\) are independent of one another and identically distributed. This is often called the i.i.d. assumption, for short. This assumption is a little more subtle. In order to understand, we need to return to this equation from earlier: \\[y_i=\\hat{y}_i+\\epsilon_i\\] One way to think about what is going on here is that in order to get an actual value of \\(y_i\\) you feed in all of the independent variables into your linear model equation to get \\(\\hat{y}_i\\) and then you reach into some distribution of numbers (or a bag of numbers if you need something more concrete to visualize) to pull out a random value of \\(\\epsilon_i\\) which you add to the end of your predicted value to get the actual value. When we think about the equation this way, we are thinking about it as a data generating process in which we get \\(y_i\\) values from completing the equation on the right. The i.i.d. assumption comes into play when we reach into that distribution (or bag) to draw out our random values. Independence assumes that what we drew previously won’t affect what we draw in the future. The most common violation of this assumption is in time series data in which peaks and valleys in the time series tend to come clustered in time, so that when you have a high (or low) error in one year, you are likely to have a similar error in the next year. Identically distributed means that you are drawn from the same distribution (or bag) each time you make a draw. One of the most common violation of this assumption is when error terms tend to get larger in absolute size as the predicted value grows in size. In later sections of this chapter, we will cover the i.i.d. assumption in more detail including the consequences of violating it, diagnostics for detecting it, and some corrections for when it does occur. "],
["modeling-non-linearity.html", "Modeling Non-Linearity", " Modeling Non-Linearity By definition, a linear model is only appropriate if the underlying relationship being modeled can accurately be described as linear. To begin, lets revisit a very clear example of non-linearity introduced in an earlier chapter. The example is the relationship between GDP per capita in a country and that country’s life expectancy. We use data from Gapminder to show this relationship with 2007 data. library(gapminder) library(ggrepel) ggplot(subset(gapminder, year==2007), aes(x=gdpPercap, y=lifeExp))+ geom_point(alpha=0.7)+ geom_smooth(method=&quot;lm&quot;, se=FALSE)+ geom_text_repel(data=subset(gapminder, year==2007 &amp; gdpPercap&gt;5000 &amp; lifeExp&lt;60), aes(label=country), size=2)+ labs(x=&quot;GDP per capita&quot;, y=&quot;Life expectancy at birth&quot;, subtitle = &quot;2007 data from Gapminder&quot;)+ scale_x_continuous(labels=scales::dollar)+ theme_bw() Figure 56: The scatterplot of life expectancy and GDP per capita from 2007 shows a clear non-linear diminishing returns relationship Figure 56 shows a scatterplot of this relationship. The relationship is so strong that its non-linearity is readily apparent. Although GDP per capita is positively related to life expectancy, the strength of this relationship diminishes substantial at higher levels of GDP. This is a particular form of non-linearity that is often called a “diminishing returns” relationship. The greater the level of GDP already, the less return you get on life expectancy for an additional dollar. Figure 56 also shows the best fitting line for this scatterplot in blue. This line is added with the geom_smooth(method=\"lm\") argument in ggplot. We can see that this line does not represent the relationship very well. How could it really, given that the underlying relationship clearly curves in a way that a straight line cannot? Even more problematic, the linear fit will produce systematic patterns in the error terms for the linear model. for countries in the middle range of GDP per capita, the line will consistently underestimate life expectancy. At the low and high ends of GDP per capita, the line will consistently overestimate life expectancy. This identifiable pattern in the residuals is an important consequence of fitting a non-linear relationship with a linear model and is in fact one of the ways we will learn to diagnose non-linearity below. Non-linearity is not always this obvious. One of the reasons that it is so clear in Figure @(fig:life-exp-non-linear) is that the relationship between GDP per capita and life expectancy is so strong. When the relationship is weaker, then points tend to be more dispersed and non-linearity might not be so easily diagnosed from the scatterplot. Figure 57 and Figure 58 show two examples that are a bit more difficult to diagnose. ggplot(movies, aes(x=TomatoRating, y=BoxOffice))+ geom_jitter(alpha=0.2)+ scale_y_continuous(labels = scales::dollar)+ labs(x=&quot;Rotten Tomatoes Rating&quot;, y=&quot;Box Office Returns (millions)&quot;)+ theme_bw() Figure 57: Is this scatterplot of tomato rating by box office returns non-linear? Figure 57 shows a scatterplot of the relationship between tomato rating and box office returns in the movies dataset. I have already made a few additions to this plot to help with the visual display. The tomato rating only comes rounded to the first decimal place, so I use geom_jitter rather than geom_point to perturb the points slightly and reduce overplotting. I also use alpha=0.2 to set the planets as semi-transparent. This is also helps with identifying areas of the scatterplot that are dense with points because they turn darker. The linearity of this plot is hard to determine. This is partially because of the heavy right-skew of the box office returns. Many of the points are densely packed along the x-axis because the scale of the y-axis is so large to deal with outliers. Nonetheless, visually it appears there might be some evidence of an increasing slope as tomato rating gets higher - this is an exponential relationship which is the inverse of the diminishing returns relationship we saw earlier. Still, its hard to know if we are really seeing it or not with the data in this form. ggplot(earnings, aes(x=age, y=wages))+ geom_jitter(alpha=0.01, width=1)+ scale_y_continuous(labels = scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;hourly wages&quot;)+ theme_bw() Figure 58: Is the relationship between age and hourly wage non-linear? Figure 58 shows the relationship between a respondent’s age and their hourly wages from the earnings data. There are so many observations here that I reduce alpha all the way to 0.01 to address overplotting. We also have a problem of a right-skewed dependent variable here. Nonetheless, you can make out a fairly clear dark band across the plot that seems to show a positive relationship. It also appears that this relationship may be of a diminishing returns type with a lower return to age after age 30 or so. However, with the wide dispersion of the data, it is difficult to feel very confident about the results. The kind of uncertainty inherent in Figures 57 and 58 is far more common in practice than the clarity in Figure 56. Scatterplots can be a first step to detecting non-linearity, but usually we need some additional diagnostics. In the next two sections, I will cover two diagnostic approaches to identifying non-linearity: smoothing and residual plots. Smoothing Smoothing is a technique for estimating the relationship in a scatterplot without the assumption that this relationship be linear. In order to understand what smoothing does, it will be first helpful to draw a non-smoothed line between all the points in a scatterplot, starting with the smallest value of the independent variable to the largest value. I do that in Figure 59. As you can see this produces a very jagged line that does not give us much sense of the relationship because it bounces around so much. movies &lt;- movies[order(movies$TomatoRating),] ggplot(movies, aes(x=TomatoRating, y=BoxOffice))+ geom_point(alpha=0.05)+ geom_line()+ scale_y_continuous(labels = scales::dollar)+ labs(x=&quot;Rotten Tomatoes Rating&quot;, y=&quot;Box Office Returns (millions)&quot;)+ theme_bw() Figure 59: Connecting the dots in our scatterplot leads to a very jagged line The basic idea of smoothing is to smooth out that jagged line. This is done by replacing each actual value of \\(y\\) with a predicted value \\(\\hat{y}\\) that is determined by that point and its nearby neighbors. The simplest kind of smoother is often called a “running average.” In this type of smoother, you simply take \\(k\\) observations above and below the given point and calculate the mean or median. For example, lets say we wanted to smooth the box office return value for the movie Rush Hour 3. We want to do this by reaching out to the two neighbors above and below this movie in terms of the tomato rating. After sorting movies by tomato rating, here are the values for Rush Hour 3 and the two movies closest to it: Title TomatoRating BoxOffice Awake 4.2 14.3 Ghost Rider 4.2 115.8 Rush Hour 3 4.2 140.1 Balls of Fury 4.2 32.8 Nobel Son 4.2 0.3 To smooth the value for Rush Hour 3, we can take either the median or the mean of the box office returns for these five values: box_office &lt;- c(14.3, 115.8, 140.1, 32.8, 0.3) mean(box_office) ## [1] 60.66 median(box_office) ## [1] 32.8 The mean smoother would give us a value of $60.66 million and the median smoother a value of $32.8 million. In either case, the value for Rush Hour 3 is pulled far away from its outlier position at $140.1 million, thus smoothing the display. To get a smoothed line, we need to repeat this process for every point in the dataset. The runmed command will do this for us once we have the data sorted correctly by the independent variable. Figure 60 shows two different smoothed lines applied to the movie scatterplot, with different windows. movies$BoxOffice.smooth1 &lt;- runmed(movies$BoxOffice, 5) movies$BoxOffice.smooth2 &lt;- runmed(movies$BoxOffice, 501) ggplot(movies, aes(x=TomatoRating, y=BoxOffice))+ geom_point(alpha=0.2)+ geom_line(col=&quot;grey&quot;, size=1, alpha=0.7)+ geom_line(aes(y=BoxOffice.smooth1), col=&quot;blue&quot;, size=1, alpha=0.7)+ geom_line(aes(y=BoxOffice.smooth2), col=&quot;red&quot;, size=1, alpha=0.7)+ scale_y_continuous(labels = scales::dollar)+ labs(x=&quot;Rotten Tomatoes Rating&quot;, y=&quot;Box Office Returns (millions)&quot;)+ theme_bw() Figure 60: Two different median smoothers are shown. The one in blue uses a window of two neighbors to each side while the one one in red uses a window of 250 neighbors on each side. It is clear here that using two neighbors on each side is not sufficient in a dataset of this size. With 250 neighbors on each side, we get something much smoother, but it also reveals a sharp uptick at the high end of the dataset, which suggests some possible non-linearity. A more sophisticated smoothing procedure is available via the LOESS (locally estimated scatterplot smoothing) smoother. The LOESS smoother uses a regression model with polynomial terms (discussed below) on a local subset of the data to estimate a predicted value for each observation. The regression model also typically weights values so that observations closer to the index observation count more in the estimation. This technique tends to produce better results than median or mean smoothing. You can easily fit a LOESS smoother to a scatterplot in ggplot by specifying method=\"loess\" in the geom_smooth function. In fact, this is the default method for geom_smooth for datasets smaller than a thousand observations. Figure 61 plots this LOESS smoother as well as a linear fit for comparison. ggplot(movies, aes(x=TomatoRating, y=BoxOffice))+ geom_jitter(alpha=0.2)+ geom_smooth(method=&quot;lm&quot;, color=&quot;red&quot;, se=FALSE)+ geom_smooth(se=FALSE, method=&quot;loess&quot;)+ scale_y_continuous(labels = scales::dollar)+ labs(x=&quot;Rotten Tomatoes Rating&quot;, y=&quot;Box Office Returns (millions)&quot;)+ theme_bw() Figure 61: A LOESS smoother to the data is shown in blue while a linear fit is shown in red. Again we see the exponential relationship in the LOESS smoother. The LOESS smoother is also much smoother than the median smoothers we calculated before. By default the LOESS smoother in geom_smooth will use 75% of the full data in its subset for calculating the smoothed value of each point. This may seem like a lot, but remember that it weights values closer to the index point. You can adjust this percentage using the span argument in geom_smooth. SHINY APP The main disadvantage of the LOESS smoother is that it becomes computationally inefficient as the number of observations increases. If I were to apply the LOESS smoother to the 145,647 observations in the earnings data, it would kill R before producing a result. For larger dataset, another option for smoothing is the general additive model (GAM). This model is complex and I won’t go into the details here, but it provides the same kinds of smoothing as LOESS but is much less computationally expensive. GAM smoothing is the default in geom_smooth for datasets larger than a thousand observations, so you do not need to include a method. Figure 62 shows A GAM smoother applied to the relationship between wages and age. The non-linear diminishing returns relationship is clearly visible in the data. ggplot(earnings, aes(x=age, y=wages))+ geom_jitter(alpha=0.01, width=1)+ geom_smooth(method=&quot;lm&quot;, color=&quot;red&quot;, se=FALSE)+ geom_smooth(se=FALSE)+ scale_y_continuous(labels = scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;hourly wages&quot;)+ theme_bw() Figure 62: A GAM smoother (shown in blue) applied to the relationship between age and wages in the earnings data. A linear fit is shown in red. Residual Plots Another technique for detecting non-linearity is the to plot the fitted values of a model by the residuals. To demonstrate how this works, lets first calculate a model predicting life expectancy by GDP per capita from the gapminder data earlier. model &lt;- lm(lifeExp~gdpPercap, data=subset(gapminder, year==2007)) If you type plot(model), you will get a series of model diagnostic plots in base R. The first of these plots is the residual vs. fitted values plot. We are going to make the same plot, but in ggplot. In order to do that, I need to introduce a new package called broom that has some nice features for summarizing results from your model. It is part of the Tidyverse set of packages. To install it type install.packages(\"broom\"). You can then load it with library(broom). The broom package has only three commands: tidy which shows the key results from the model including coefficients, standard errors and the like. augment which adds a variety of diagnostic information to the original data used to calculate the model. This includes residuals, cooks distance, fitted values, etc. glance which gives a summary of model level goodness of fit statistics. At the moment, we want to use augment. Here is what the it looks like: library(broom) augment(model) ## # A tibble: 142 x 9 ## lifeExp gdpPercap .fitted .se.fit .resid .hat .sigma .cooksd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 43.8 975. 60.2 0.973 -16.4 0.0120 8.82 2.07e-2 ## 2 76.4 5937. 63.3 0.818 13.1 0.00846 8.86 9.28e-3 ## 3 72.3 6223. 63.5 0.812 8.77 0.00832 8.90 4.11e-3 ## 4 42.7 4797. 62.6 0.848 -19.9 0.00907 8.77 2.31e-2 ## 5 75.3 12779. 67.7 0.750 7.61 0.00709 8.91 2.63e-3 ## 6 81.2 34435. 81.5 1.52 -0.271 0.0292 8.93 1.44e-5 ## 7 79.8 36126. 82.6 1.61 -2.75 0.0327 8.93 1.67e-3 ## 8 75.6 29796. 78.5 1.29 -2.91 0.0211 8.93 1.18e-3 ## 9 64.1 1391. 60.5 0.958 3.61 0.0116 8.93 9.75e-4 ## 10 79.4 33693. 81.0 1.48 -1.59 0.0278 8.93 4.71e-4 ## # … with 132 more rows, and 1 more variable: .std.resid &lt;dbl&gt; We can use the dataset returned here to build a plot of the fitted values (.fitted) by the residuals (.resid) like this: ggplot(augment(model), aes(x=.fitted, y=.resid))+ geom_point()+ geom_hline(yintercept = 0, linetype=2)+ geom_smooth(se=FALSE)+ labs(x=&quot;fitted values of life expectancy&quot;, y=&quot;model residuals&quot;)+ theme_bw() Figure 63: A residual vs. fitted values plot for a model predicting life expectancy by GDP per capita. Their is a clear trend here whigh signifies non-linearity. If a linear model fitted the data well, then we would expect to see no evidence of a pattern in the scatterplot of 63. We should just see a cloud of points centered around the value of zero. The smoothing line helps us see patterns, but in this case the non-linearity is so strong that we would be able to detect it even without the smoothing. This clear pattern indicates that we non-linearity in our model and we should probably reconsider our approach. Figures 64 and 65 show residual vs fitted value plots for the two other cases of movie box office returns and wages that we have been examining. In these cases, the patterns are somewhat less apparent without the smoothing. The movie case does not appear particularly problematic although there is some argument for exponential increase at very high values of tomato rating. The results for wages suggest a similar diminishing returns type problem. Figure 64: A residual vs. fitted values plot for a model predicting movie box office returns by tomato rating. Figure 65: A residual vs. fitted values plot for a model predicting movie box office returns by tomato rating. Both of these figures also reveal an additional problem unrelated to non-linearity. Both figures show something of a cone shape to the scatterplot in which the variance of the residuals gets larger at higher fitted values. This is the problem of heteroskedasticity that we will return to in the next section. Now that we have diagnosed the problem of non-linearity, what can we do about it? There are several ways that we can make adjustments to our models to allow for non-linearity. The most common technique is to use transformations to change the relationship between independent and dependent variables. Another approach is to include polynomial terms into the model that can simulate a parabola. A third, less common, option is to create a spline term that allows for non-linearity at specific points. I cover each of these techniques below. Transformations You transform your data when you apply a mathematical function to a variable to transform its values into different values. There are a variety of different transformations that are commonly used in statistics, but we will focus on the one transformation that is most common in the social sciences: the log transformation. Transformations can directly address issues of non-linearity. By definition if you transform the independent variable, the dependent variable, or both variables in your model, then the linear relationship assumed by the model between the transformed variables. On the original scale of the variables, the relationship is non-linear. Transformations can also have additional side benefits besides fitting non-linear relationships. The log transformation, for example, will pull in extreme values and skewness in a distribution, reducing the potential for outliers to exert a strong influence on results. Take Figure 66 for example. This figure shows the distribution of hourly wages in the earnings data. This distribution is heavily right-skewed, despite the fact that wages have been top-coded at $100. Any model of wages is likely to be influenced considerably by the relatively small but still numerically substantial number of respondents with high wages. Figure 66: Hourly wages are heavily right skewed even after top-coding the data at a wage of $100. Lets look at that distribution again, but this time on the log-scale. We will cover in more detail below what the mathematical log means, but for now you can just think of the log-scale as converting from an additive scale to a multiplicative scale. On the absolute scale, we want to have equally spaced intervals on the axes of our graph to represent a constant absolute amount difference. You can see in Figure 66 that the tick-marks for wages are at evenly spaced intervals of $25. On the log-scale we want evenly-spaced intervals on the axes of our graph to correspond to multiplicative changes. So on a log base 10 scale, we would want the numbers 1, 10, and 100, to be evenly spaced because each one represents a 10 fold increase multiplicative increase. In ggplot, you can change the axis to a log-scale with the argument scale_x_log10. Figure 67 shows the same distribution of hourly wages, but this time using the log-scale. Figure 67: On the log-scale, hourly wages is less skewed, although now we have a slight problem of a left-skew You can clearly see in Figure 67 that we have pulled in the extreme right tail of the distribution and have a somewhat more symmetric distribution. The transformation here hasn’t perfectly solved things, because now we have a slight left-skew for very low hourly wages. Nonetheless, even these values are closer in absolute value after the transformation and so will be less likely to substantially influence our model. This transformation can also help us with the cone-shaped residual issue we saw earlier (i.e. heteroskedasticity), but we will discuss that more in the next section. Because transformations can often solve multiple problems at once, they are very popular. In fact, for some common analysis like estimating wages or CO2 emissions, log transformations are virtually universal. The difficult part about transformations is understanding exactly what kind of non-linear relationship you are estimating by transforming and how to interpret the results from models with transformed variables. We will develop this understanding using the natural log transformation on the examples we have been looking at in this section. The log transformation Before getting into the mathematical details of the log transformation, lets first demonstrate its power. Figure 68 shows the relationship between Rotten Tomatoes rating and box office returns in our movies data. However, this time I have applied a log-scale to box office returns. Although we still see some evidence of non-linearity in the LOESS estimator, the relationship looks much more clearly linear than before. But the big question is how do we interpret the slope and intercept for the linear model fit to this log-transformed data? ggplot(movies, aes(x=TomatoRating, y=BoxOffice))+ geom_jitter(alpha=0.2)+ geom_smooth(method=&quot;lm&quot;, color=&quot;red&quot;, se=FALSE)+ geom_smooth(se=FALSE, method=&quot;loess&quot;)+ scale_y_log10(labels = scales::dollar)+ labs(x=&quot;Rotten Tomatoes Rating&quot;, y=&quot;Box Office Returns (millions)&quot;)+ theme_bw() Figure 68: Scatterplot of the relationship between rotten tomatoes rating and box office returns, with a log-scale applied to box office returns. In order to understand the model implied by this scatterplot, we need to delve a little more deeply into the math of the log transformation. In general, the log equation means converting some number into the exponential value of some base multiplier. So, if we wanted to know what the log base 10 value of 27 is, we need to know what exponential value applied to 10 would produce 27: \\[27=10^x\\] We can calculate this number easily in R: x &lt;- log(27, base=10) x ## [1] 1.431364 10^x ## [1] 27 Log base 10 and log base 2 are both common values used in teaching log transformations, but the standard base we use in statistics is the mathematical constant \\(e\\) (2.718282). This number is of similar importance in mathematics as \\(\\pi\\) and is based on the idea of what happens to compound interest as the period of interest accumulation approaches zero. This has a variety of applications from finance to population growth, but none of this concerns us directly. We are interested in \\(e\\) as a base for our log function because of certain other useful properties discussed further below that ease interpretation. When we use \\(e\\) as the base for our log function, then we are using what is often called the “natural log.” We can ask the same question about 27 as before but now with the natural log: x &lt;- log(27) x ## [1] 3.295837 exp(x) ## [1] 27 I didn’t have to specify a base here because R defaults to the natural log. The exp function calculates \\(e^x\\). For our purposes, the most important characteristic of the log transformation is that it converts multiplicative relationships into additive relationships.This is because of a basic mathematical relationship where: \\[e^a*a^b=e^{a+b}\\] \\[log(x*y) = log(x)+log(y)\\] You can try this out in R to see that it works: exp(2)*exp(3) ## [1] 148.4132 exp(2+3) ## [1] 148.4132 log(5*4) ## [1] 2.995732 log(5)+log(4) ## [1] 2.995732 We can use these mathematical relationships to help understand a model with log-transformed variables. First lets calculate the model implied by the scatterplot above. We can just use the log command to directly transform box office returns in the lm formula: model &lt;- lm(log(BoxOffice)~TomatoRating, data=movies) coef(model) ## (Intercept) TomatoRating ## 0.9727622 0.2406026 OK, so what does that mean? It might be tempting to interpret the results here as you normally would. We can see that tomato rating has a positive effect on box office returns. So, a one point increase in tomato rating is associated with a 0.241 increase in … what? Remember that our dependent variable here is now the natural log of box office returns, not box office returns itself. We could literally say that it is a 0.241 increase in log box office returns, but that is not a very helpful or intuitive way to think about the result. Similarly the intercept gives us the predicted log box office returns when tomato rating is zero. That is not helpful for two reasons: its outside the scope of the data, and we don’t really know how to think about a log box office returns of 0.973. In order to translate this into something meaningful, lets try looking at this in equation format. Here is what we have: \\[\\hat{\\log(y_i)}=0.973+0.241*x_i\\] What we really want is to be able to understand this equation back on the original scale of the dependent variable, which in this case is box office returns in millions of dollars. Keep in mind that taking \\(e^{\\log(y)}\\) just gives us back \\(y\\). We can use that logic here. If we “exponentiate” (take \\(e\\) to the power of the values) the left-hand side of the equation, then we can get back to \\(\\hat{y}_i\\). However, remember from algebra, that what we do to one side of the equation, we have to do to both sides. That means: \\[e^{\\hat{\\log(y_i)}}=e^{0.973+0.241*x_i}\\] \\[\\hat{y}_i=(e^{0.971})*(e^{0.241})^{x_i}\\] We now have quite a different looking equation for predicting box office returns. The good news is that we now just have our predicted box office returns back on the left-hand side of the equation. The bad news is that the right hand side looks a bit complex. Since \\(e\\) is just a constant value, we can go ahead and calculate the values in those parentheses (called “exponentiating”): exp(0.971) ## [1] 2.640584 exp(0.241) ## [1] 1.272521 That means: \\[\\hat{y}_i=(2.64)*(1.27)^{x_i}\\] We now have a multiplicative relationship rather than an additive relationship. How does this changes our understanding of the results? To see, lets plug in some values for \\(x_i\\) and see how it changes our predicted income value. A tomato rating of zero is outside the scope of our data, but lets plug it in for instructional purposes anyway: \\[\\hat{y}_i=(2.64)*(1,27)^{0}=(2.64)(1)=2.64\\] So, the predicted box office returns when \\(x\\) is zero is just given by exponentiating the intercept. Lets try increasing tomato rating by one point: \\[\\hat{y}_i=(2.64)*(1.27)^{1}=(2.64)(1.27)\\] I could go ahead and finish that multiplication, but I want to leave it here to better show how to think about the change. A one point increase in tomato rating is associated with an increase in box office returns by a multiplicative factor of 1.27. In other words, a one point increase in tomato rating is associated with a 27% increase in income, on average. What happens if I add another point? \\[\\hat{y}_i=(2.64)*(1.27)^{2}=(2.64)(1.27)(1.27)\\] Each additional point leads to a 27% increase in predicted box office returns. This is what I mean by a multiplicative increase. We are no longer talking about the predicted change in box office returns in terms of absolute numbers of dollars, but rather in relative terms of percentage increase. In general, in order to properly interpret your results when you log the dependent variable, you must exponentiate all of your slopes and the intercept. You can then interpret them as: The model predicts that a one-unit increase in \\(x_j\\) is associated with a \\(e^{b_j}\\) multiplicative increase in \\(y\\), on average while holding all other independent variables constant. The model predicts that \\(y\\) will be \\(e^{b_0}\\) on average when all independent variables are zero. Of course, just like all of our prior examples, you are responsible for converting this into sensible English. Lets try a more complex model in which we predict log box office returns by tomato rating, runtime and movie maturity rating: model &lt;- lm(log(BoxOffice)~TomatoRating+Runtime+Rating, data=movies) coef(model) ## (Intercept) TomatoRating Runtime RatingPG RatingPG-13 ## -0.81318674 0.17783703 0.03863665 -0.75833147 -1.29892956 ## RatingR ## -2.96580424 We have a couple of added complexities here. We now have results for categorical variables as well as negative numbers to interpret. In all cases, we want to exponentiate to interpret correctly, so lets go ahead and do that: exp(coef(model)) ## (Intercept) TomatoRating Runtime RatingPG RatingPG-13 ## 0.44344268 1.19463061 1.03939275 0.46844739 0.27282368 ## RatingR ## 0.05151902 The tomato rating effect can be interpreted as before but now with control variables: The model predicts that a one point increase in the tomato rating is associated with a 19% increase in box office returns, on average, holding constant movie runtime and maturity rating. The runtime effect can be interpreted in a similar way. The model predicts that a one minute increase in movie runtime is associated with a 3.9% increase in box office returns, on average, holding constant movie tomato rating and maturity rating. In both of these cases, it makes sense to take the multiplicative effect and convert that into a percentage increase. If I multiply the predicted box office returns by 1.039, then I am increasing it by 3.9%. When effects get large, however, it sometimes makes more sense to just interpret it as a multiplicative factor. For example, if the exponentiated coefficient was 3, I could interpret this as a 200% increase. However, it probably makes more sense in this case to just say something along the lines of the “predicted value of \\(y\\) triples in size for a one point increase in \\(x\\).” The categorical variables can be interpreted as we normally do in terms of the average difference between the indicated category and the reference category. However, now it is the multiplicative difference. So, taking the PG effect of 0.468, we could just say: The model predicts that, controlling for tomato rating and runtime, PG-rated movies make 46.8% as much as G-rated movies at the box office, on average. However, its also possible to talk about how much less a PG-rated movie makes than a G-rated movie. If a PG movie makes 46.8% as much, then it equivalently makes 52.2% less. We get this number by just subtracting the original percentage from 100. So I could have said: The model predicts that, controlling for tomato rating and runtime, PG-rated movies make 52.2% less than G-rated movies, on average. In general to convert any coefficient \\(\\beta_j\\) from a model with a logged dependent variable to a percentage change scale we can follow this formula: \\[(e^{\\beta_j}-1)*100\\] This will give us the percentage change and the correct direction of the relationship. You may have noted that the effect of runtime in the model above before exponentiating was 0.0386 and after we exponentiated the result, we concluded that the effect of one minute increase in runtime was 3.9%. If you move over the decimal place two, those numbers are very similar. This is not a coincidence. It follows from what is known as the Taylors series expansion for the exponential: \\[e^x=1+x-\\frac{x^2}{2!}+\\frac{x^3}{3!}-\\frac{x^4}{4!}+\\ldots\\] Any exponential value \\(e^x\\) can be calculated from this Taylor series expansion which continues indefinitely. However, when \\(x\\) is a small value less than one, note that you are squaring, cubing, and so forth, which results in an even smaller number that is divided by an increasingly large number. So for, small values of \\(x\\) the following approximation works reasonably well: \\[e^x\\approx1+x\\] In the case I just noted where \\(x=0.0386\\), the actual value of \\(e^{0.0386}\\) is 1.0394 while the approximation gives us 1.0386. That is pretty close. This approximation starts to break down around \\(x=0.2\\). The actual value of \\(e^{0.2}\\) is 1.221 or about a 22.1% increase, whereas the approximation gives us 1.2 or a straight 20% increase. You can use this approximation to get a ballpark estimate of the effect size in percentage change terms for coefficients that are not too large without having to exponentiate. So I can see that the coefficient of 0.177 for tomato rating is going to be roughly a 18-19% increase and the coefficient of 0.0386 is going to be a little under a 4% increase, all without having to exponentiate. Notice that this does not work at all well for the very large coefficients for movie maturity rating. Logging the independent variable The previous example showed how to interpret results when you log the dependent variables. Logging the dependent variable will help address exponential type relationships. However in cases of a diminishing returns type relationship, we instead need to log the independent variable. Figure 69 shows the GDP per capita by life expectancy scatterplot from before but now with GDP per capita on a log scale. Figure 69: Scatterplot of life expectancy by GDP per capita, with a log-scale applied to GDP per capita. Look how much more linear the relationship looks. It is really quite impressive how this simple transformation straightened out that very clear diminishing returns type relationship we observed before. This is because on the log-scale a one unit increase means a multiplicative change so, you have to make a much larger absolute change when you are already high in GDP per capita to get the same return in life expectancy. Models with logged independent variables are a little trickier to interpret. To understand how it works, lets go ahead and calculate the model: model &lt;- lm(lifeExp~log(gdpPercap), data=subset(gapminder, year==2007)) round(coef(model), 5) ## (Intercept) log(gdpPercap) ## 4.94961 7.20280 So our model is: \\[\\hat{y}_i=4.9+7.2\\log{x_i}\\] Because the log transformation is on the right hand side we can no longer use the trick of exponentiating its value because that would just give us \\(e^{\\hat{y}_i}\\) on the left hand side. It would also be impractical if we added other independent variables to the model. Instead, we can interpret our slope by thinking about how a 1% increase in \\(x\\) would change the predicted value of \\(y\\). For ease of interpretation, lets start with \\(x=1\\). The log of 1 is always zero, so: \\[\\hat{y}_i=4.9+7.2\\log{1}=4.9+7.2*0=\\beta_0\\] So, the intercept here is actually the predicted value of life expectancy when GDP per capita is $1. This is not a terribly realistic value for GDP per capita where the lowest value in the dataset is $241, so its not surprising that we get an unrealistically low life expectancy value. Now what happens if we raise GDP per capita by 1%? A 1% increase from a value of 1 would give us a value of 1.01. So: \\[\\hat{y}_i=4.9+7.2\\log{1.01}\\] From the Taylor series above, we can deduce that the \\(\\log{1.01}\\) is roughly equal to 0.01. So: \\[\\hat{y}_i=4.9+7.2*0.01=4.9+0.072\\] This same logic will apply to any 1% increase in GDP per capita. So, the model predicts that a 1% increase in GDP per capita is associated with a 0.072 year increase in the life expectancy. In general, you can interpret any coefficient \\(\\beta_j\\) with a logged independent variable but not logged dependent variable as: The model predicts that a a 1% increase in x is associated with a \\(\\beta_j/100\\) unit increase in y. One easy way to remember what the log transformation does is that it makes your measure of change relative rather than absolute for the variable transformed. When we logged the dependent variable before then our slope could be interpreted as the percentage change in the dependent variable (relative) for a one unit increase (absolute) in the independent variable. When we log the independent variable, the slope can be interpreted as the unit change in the dependent variable (absolute) for a 1% increase (relative) in the independent variable. Logging both independent and dependent variables: The elasticity model If logging the dependent variable will make change in the dependent variable relative, and logging the independent variable will make change in the independent variable relative, then what happens if you log transform both. Then you get a model which predicts relative change by relative change. This model is often called an “elasticity” model because the predicted slopes are equivalent to the concept of elasticity in economics: how much does of a percent change in \\(y\\) results from a 1% increase in \\(x\\). The relationship between age and wages may benefit from logging both variables. We have already seen a diminishing returns type relationship which suggests logging the independent variable. We also know that wages are highly right-skewed and so might also benefit from being logged. ggplot(earnings, aes(x=age, y=wages))+ geom_jitter(alpha=0.01, width=0.1)+ geom_smooth(se=FALSE, method=&quot;lm&quot;, color=&quot;red&quot;)+ geom_smooth(se=FALSE)+ scale_y_log10(labels = scales::dollar)+ scale_x_log10()+ labs(x=&quot;age&quot;, y=&quot;hourly wages&quot;)+ theme_bw() Figure 70: Is the relationship between age and hourly wage non-linear? Figure 70 shows the relationship between age and wages when we apply a log scale to both variables. It looks a little better although there is still some evidence of diminishing returns even on this scale. Lets go with it for now though. Lets take a look at the coefficients from this model: model &lt;- lm(log(wages)~log(age), data=earnings) coef(model) ## (Intercept) log(age) ## 1.1558386 0.5055849 One of the advantages of the elasticity model is that slopes can be easily interpreted. We can use the same techniques when logging independent variables to divide our slope by 100 to see that a one percent increase in age is associated with a 0.0051 increase in the dependent variable. However this dependent variable is the log of wages. T interpret the effect on wages, we need to exponentiate this value, subtract 1, and multiply by 100. However since the value is small, we know that this is going to be roughly a 0.51% increase. So: The model predicts that a one percent increase in age is associated with a 0.51% increase in wages, on average. In other words, we don’t have to make any changes at all. The slope is already the expected percent change in \\(y\\) for a one percent increase in \\(x\\). The STRIPAT model that Richard York helped to develop is an example of an elasticity model that is popular in environmental sociology. In this model, the dependent variable is some measure of environmental degradation, such as CO2 emissions. The independent variable can be things like GDP per capita, population size and growth, etc. All variables are logged so that model parameters can be interpreted as elasticities. The square root transformation The log transformation is very flexible and solves multiple problems at once (non-linearity, outliers, skewness), which explains its popularity. But it breaks down in one important situation: you cannot log a variable that has zero or negative values. The negative case is generally not as important although there are some exceptions (like net worth). On the other hand, there are numerous cases where a quantitative variable can be zero as well as positive. Lets run an elasticity model on box office returns, but this time lets predict returns by the Tomato Meter rather than the Tomato Rating. summary(lm(log(BoxOffice)~log(TomatoMeter), data=movies)) ## Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...): NA/NaN/Inf in &#39;x&#39; Oh no! We got an error. The problem is that the Tomato Meter has a few cases of zero values (when a movie received zero positive reviews). The log of zero is negative infinity and that simply won’t work when fitting a linear model. What can you do? It turns out that the square root transformation can do much the same work as the natural logarithm. It will pull in skewness and can make non-linear relationships more linear. Since the square root of zero is a real number (zero to be precise), it will also work on variables that have legitimate zero values. Figure 71 shows a scatterplot of tomato meter by box office returns with both variables on the log scale. Movies with a tomato meter of zero have been removed. Figure 72 shows the same figure but now with Tomato Meter on the square root scale. The figures looks pretty similar and fit a similar trend but the square root transformation can also include the zero values. ggplot(subset(movies, TomatoMeter&gt;0), aes(x=TomatoMeter, y=BoxOffice))+ geom_vline(xintercept = 0, linetype=2)+ geom_jitter(alpha=0.5)+ scale_y_log10()+ scale_x_log10()+ geom_smooth(se=FALSE)+ labs(x=&quot;Tomato Meter (square root scale)&quot;, y=&quot;Box Office Returns (log scale)&quot;)+ theme_bw() ## Warning: Transformation introduced infinite values in continuous x-axis Figure 71: Scatterplot of tomato meter by box office returns, using a log transformation of tomato meter ratings. ggplot(movies, aes(x=TomatoMeter, y=BoxOffice))+ geom_vline(xintercept = 0, linetype=2)+ geom_jitter(alpha=0.5)+ scale_y_log10()+ scale_x_sqrt()+ geom_smooth(se=FALSE)+ labs(x=&quot;Tomato Meter (square root scale)&quot;, y=&quot;Box Office Returns (log scale)&quot;)+ theme_bw() Figure 72: Scatterplot of tomato meter by box office returns, using a square root transformation of tomato meter ratings. The dotted vertical line shows cases with a zero tomato meter. We can estimate this model easily enough in R: model &lt;- lm(log(BoxOffice)~sqrt(TomatoMeter), data=movies) coef(model) ## (Intercept) sqrt(TomatoMeter) ## 1.4376939 0.1274002 There is one big downside to this transformation. Unlike log transformations, there is no clear and easy interpretation of how to intepret this effect. Since we know that the square root transformation is doing something close to a log transformation, we can think of this result as loosely close to an elasticity in this case, but that is only a loose approximation and may deteriorate as you get larger and larger values of the independent variable. Polynomial Models Another method that can be used to fit non-linear relationships is to fit polynomial terms. You may recall the the following formula from basic algebra: \\[y=a+bx+cx^2\\] This formula defines a parabola which fits not a straight line but a curve with one point of inflection. We can fit this curve in a linear model by simply including the square of a given independent variable as an additional variable in the model. Before we do this it is usually a good idea to re-center the original variable somewhere around the mean because this will reduce the correlation between the original term and its square. For example, I could fit the relationship between age and wages using polynomial terms: model &lt;- lm(wages~I(age-40)+I((age-40)^2), data=earnings) coef(model) ## (Intercept) I(age - 40) I((age - 40)^2) ## 26.92134254 0.31711167 -0.01783204 The coefficients for such a model are a little hard to interpret directly. In this case, it might be useful to start thinking about the marginal effect of \\(x\\) on \\(y\\). The marginal effect is simply how much we expect \\(y\\) to change for a one unit increase in \\(x\\) at a given value of \\(x\\). We have not had to think much about marginal effects because in a simple linear model the coefficient/slope itself is the marginal effect. But as we move into non-linear relationships, the marginal effect is more complicated. The marginal effect can be calculated by taking the derivative using calculus of the model equation with respect to \\(x\\). I don’t expect you to know how to do this, so I will just tell how it turns out. In a model with a squared term: \\[\\hat{y}=\\beta_0+\\beta_1x+\\beta_2x^2\\] The marginal effect (or slope) is given by: \\[\\beta_1+2\\beta_2x\\] Note that the slope of \\(x\\) is itself partially determined by the value of \\(x\\). This is what drives the non-linearity. If we plug in the values for the model above, the relationship will be clearer: \\[0.3171+2(-0.0178)x=0.3171-0.0356x\\] So when \\(x=0\\), which in this case means age 40, a one year increase in age is associated with a $0.32 increase in hourly wage, on average. For every increase in the year of age past 40, that effect of a one year increase in age on wages decreases in size by $0.035. For every year below age 40, the effect increases by $0.035. Because \\(\\beta_1\\) is positive and \\(\\beta_2\\) is negative in this case it corresponds to a diminishing returns kind of relationship. Unlike the log transformation however, the relationship can ultimately reverse direction from positive to negative. I can figure out the inflection point at which it will transition from a positive to a negative relationship. I won’t delve into the details here, but this point is given by setting the derivative of this equation with respect to age equal to zero and solving for age. In general, this will give me: \\[\\beta_j/(-2*\\beta_{sq})\\] Where \\(\\beta_j\\) is the coefficient for the linear effect of the variable (in this case, age), and \\(\\beta_{sq}\\) is the coefficient for the squared effect. In my case, I get: \\[0.3171/(-2*-0.0178)=0.3171/0.0356=8.91\\] Remember that we re-centered age at 40, so this means that the model predict that age will switch from a positive to a negative relationship with wages at age 48.91. Often, the easiest way to really get a sense of the parabolic relationship we are modeling is to graph it. This can be done easily with ggplot by using the geom_smooth function. In the geom_smooth function you specify lm as the method, but also specify a formula argument that specifies this linear model should be fit as a parabola rather than a straight line. ggplot(earnings, aes(x=age, y=wages))+ geom_jitter(alpha=0.01, width=1)+ scale_y_continuous(labels = scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;hourly wages&quot;)+ geom_smooth(se=FALSE, method=&quot;lm&quot;, formula=y~x+I(x^2))+ geom_vline(xintercept = 48.91, linetype=2, color=&quot;red&quot;)+ theme_bw() Figure 73: A parabolic model fit to the relationship between age and wages. The dotted red line shows the maximum value of the curve where the relationship between wages and age goes from positive to negative Figure 73 shows the relationship between age and wages after adding a squared term to our linear model. I have also added a dashed line to indicate the inflection point at which the relationship shifts from positive to negative. Polynomial models do not need to stop at a squared term. You can also add a cubic term and so on. The more polynomial terms you add, the more inflection points you are allowing for in the data, but also the more complex it will be to interpret. Figure 74 shows the predicted relationship between GDP per capita and life expectancy in the gapminder data when using a squared and cubic term in the model. You can see two different inflection points in the data that allow for more complex curves than one could get using transformations. ggplot(subset(gapminder, year==2007), aes(x=gdpPercap, y=lifeExp))+ geom_point()+ labs(x=&quot;GDP per capita&quot;, y=&quot;life expectancy&quot;)+ geom_smooth(se=FALSE, method=&quot;lm&quot;, formula=y~x+I(x^2)+I(x^3))+ theme_bw() Figure 74: A model predicting life expectancy from GDP per capita with squared and cubic terms. Splines Another approach to fitting non-linearity is to fit a spline. Splines can become quite complex, but I will focus here on a very simple spline. In a basic spline model, you allow the slope of a variable to have a different linear effect at different cutpoints or “hinge” values of \\(x\\). Within each segment defined by the cutpoints, we simply estimate a linear model (although more complex spline models allow for non-linear effects within segments). Looking at the diminishing returns to wages from age in the earnings data, it might be reasonable to fit this effect with a spline model. It looks like age 35 might be a reasonable value for the hinge. In order to fit the spline model, we need to add a spline term. This spline term will be equal to zero for all values where age is equal to or below 35 and will be equal to age minus 35 for all values where age is greater than 35. We then add this variable to the model. I fit this model below. earnings$age.spline &lt;- ifelse(earnings$age&lt;35, 0, earnings$age-35) model &lt;- lm(wages~age+age.spline, data=earnings) pander(tidy(model)) term estimate std.error statistic p.value (Intercept) -6.039 0.3107 -19.44 4.723e-84 age 0.9472 0.01035 91.52 0 age.spline -0.9549 0.01405 -67.98 0 This spline will produce two different slopes. For individuals 35 years of age or younger, the predicted change in wages for a one year increase will be given by the main effect of age, because the spline term will be zero. For individuals over 35 years of age, a one year increase in age will produce both the main age effect and the spline effect, so the total effect is given by adding the two together. So in this case, I would say the following: The model predicts that for individuals 35 years of age and younger, a one year increase in age is associated with a $0.95 increase in wages on average. The model predicts that for individuals over 35 years of age, a one year increase in age is associated with a $0.0077 (0.9742-0.9549). We can also graph this spline model, but we cannot do it simply through the formula argument in geom_smooth. In order to do that, you will need to learn a new command. The predict command can be used to get predicted values from a model for a different dataset. The predict command needs at least two arguments: (1) the model object for the model you want to predict, and (2) a data.frame that has the same variables as those used in the model. In this case, we can create a simple “toy” dataset that just has one person of every age from 18 to 65, with only the variables of age and age.spline. We can then use the predict command to get predicted wages for these individuals and add that as a variable to our new dataset. We can than plot these predicted values as a line in ggplot by feeding in the new data into the geom_line command. The r code chunk below does all of this to produce Figure 75. The spline model seems to fit pretty well here and is quite close to the smoothed line. This kind of model is sometimes called a “broken arrow” model because the two different slopes produce a visual effect much like an arrow broken at the hinge point. predict_df &lt;- data.frame(age=18:65) predict_df$age.spline &lt;- ifelse(predict_df$age&lt;35, 0, predict_df$age-35) predict_df$wages &lt;- predict(model, newdata=predict_df) ggplot(earnings, aes(x=age, y=wages))+ geom_jitter(alpha=0.01, width = 1)+ geom_smooth(se=FALSE)+ geom_line(data=predict_df, color=&quot;red&quot;, size=1.5, alpha=0.75)+ theme_bw() Figure 75: Scatterplot of age and hourly wages, with a spline fit hinged on age 35 shown in red. A smoothing line is also shown in blue. "],
["the-i-i-d-violation-and-robust-standard-errors.html", "The i.i.d. Violation and Robust Standard Errors", " The i.i.d. Violation and Robust Standard Errors "],
["sample-design-and-weighting.html", "Sample Design and Weighting", " Sample Design and Weighting "],
["missing-values.html", "Missing Values", " Missing Values Up until this point, we have analyzed data that had no missing values. In actuality, the data did have missing values, but I used an imputation technique to remove the missing values to simplify our analyses. However, we now need to think more clearly about the consequences of missing values in datasets, because the reality is that most datasets that you want to use will have some missing data. Howe you handle this missing data can have important implications for your results. Identifying Valid Skips A missing value occurs when a value is missing for an observation on a particular value. Its important to distinguish in your data between situations in which a missing value is actually a valid response, from situations in which it is invalid and must be addressed. It may seem unusual to think of some missing values as valid, but based on the structure of survey questionnaires is not unusual to run into valid missing values. In particular, valid skips are a common form of missing data. Valid skips occur when only a subset of respondents are asked a question that is a follow-up to a previous question. Typically to be asked the follow up question, respondents have to respond to the original question in a certain way. All respondents who weren’t asked the follow up question then have valid skips for their response. As an example, lets look at how the General Social Survey asks about a person’s religious affiliation. The GSS asks this using a set of three questions. First, all respondents are asked “What is your religious preference? Is it Protestant, Catholic, Jewish, some other religion, or no religion?” After 1996, the GSS also started recording more specific responses for the “some other religion” category, but for simplicity lets just look at the distribution of this variable in 1996. table(gss$relig, exclude=NULL) ## ## catholic jewish none other protestant &lt;NA&gt; ## 685 68 339 143 1664 5 On this variable, we have five cases of item nonresponse in which the respondent did not answer the question. They might have refused to answer it or perhaps they just did not know the answer. For most of the respondents, this is the only question on religious affiliation. However, those who responded as Protestant are then asked further questions to determine more information about their particular denomination. This denom question includes prompts for most of the large denominations in the US. Here is the distribution of that variable. table(gss$denom, exclude=NULL) ## ## afr meth ep zion afr meth episcopal am bapt ch in usa ## 8 11 28 ## am baptist asso am lutheran baptist-dk which ## 57 51 181 ## episcopal evangelical luth luth ch in america ## 79 33 15 ## lutheran-dk which lutheran-mo synod methodist-dk which ## 25 48 23 ## nat bapt conv of am nat bapt conv usa no denomination ## 12 8 99 ## other other baptists other lutheran ## 304 69 15 ## other methodist other presbyterian presbyterian c in us ## 12 19 34 ## presbyterian-dk wh presbyterian, merged southern baptist ## 11 13 273 ## united methodist united pres ch in us wi evan luth synod ## 190 29 11 ## &lt;NA&gt; ## 1246 Notice that we seem to have a lot of missing values here (1246). Did a lot of people just not know their denomination? No, in fact there is very little item nonresponse on this measure. The issue is that all of the people who were not asked this question because they didn’t respond as Protestant are listed as missing values here, but they are actually valid skips. We can see this more easily by crosstabbing the two variables based on whether the response was NA or not. table(gss$relig, is.na(gss$denom), exclude=NULL) ## ## FALSE TRUE ## catholic 0 685 ## jewish 0 68 ## none 0 339 ## other 0 143 ## protestant 1658 6 ## &lt;NA&gt; 0 5 You can see that almost all of the cases of NA here are respondents who did not identify as Protestants and therefore were not asked the denom question, i.e. valid skips. We have only six cases of individuals who were asked the denom question and did not respond. For Protestants who did not identify with one of the large denominations prompted in denom were identified as “other”. For this group, a third question (called other in the GSS) was asked in which the respondent’s could provide any response. I won’t show the full table here as the number of denominations is quite large, but we can use the same logic as above to identify all of the valid skips vs. item nonresponses. table(gss$denom, is.na(gss$other), exclude=NULL) ## ## FALSE TRUE ## afr meth ep zion 0 8 ## afr meth episcopal 0 11 ## am bapt ch in usa 0 28 ## am baptist asso 0 57 ## am lutheran 0 51 ## baptist-dk which 0 181 ## episcopal 0 79 ## evangelical luth 0 33 ## luth ch in america 0 15 ## lutheran-dk which 0 25 ## lutheran-mo synod 0 48 ## methodist-dk which 0 23 ## nat bapt conv of am 0 12 ## nat bapt conv usa 0 8 ## no denomination 0 99 ## other 300 4 ## other baptists 0 69 ## other lutheran 0 15 ## other methodist 0 12 ## other presbyterian 0 19 ## presbyterian c in us 0 34 ## presbyterian-dk wh 0 11 ## presbyterian, merged 0 13 ## southern baptist 0 273 ## united methodist 0 190 ## united pres ch in us 0 29 ## wi evan luth synod 0 11 ## &lt;NA&gt; 0 1246 We can see here that of the individuals who identified with an “other” Protestant denomination, only four of those respondents did not provide a write-in response. Valid skips are generally not a problem so long as we take account of them when we construct our final variable. In this example, we ultimately would want to use these three questions to create a parsimonious religious affiliation variable, perhaps by separating Protestants into evangelical and “mainline” denominations. In that case, the only item nonresponses will be the five cases for relig, the six cases for denom, and the four cases for other for a total of 15 real missing values. Kinds of Missingness In statistics, missingness is actually a word. It describes how we think values came to be missing from the data. In particular, we are concerned with whether missingness might be related to other variables that are related to the things we want to study. In this case, our results might be biased as a result of missing values. For example, income is a variable that often has lots of missing data. If missingness on income is related to education, gender, or race, then we might be concerned about how representative our results are in understanding the process. The biases that results from relationships between missingness and other variables can be hard to understand and therefore particularly challenging. Missingness is generally considered as one of three types. The best case scenario is that the data are missing completely at random (MCAR). A variable is MCAR if every observation has the same probability of missingness. In other words, the missingness of a variable has no relationship to other observed or unobserved variables. If this is true, then removing observations with missing values will not bias results. However, it pretty rare that MCAR is a reasonable assumption for missingness. Perhaps if you randomly spill coffee on a page of your data, you might have MCAR. A somewhat more realistic assumption is that the data are **missing at random* (MAR). Its totally obvious how this is different, right? A variable is MAR if the different probabilities of missingness can be fully accounted for by other observed variables in the dataset. If this is true, then various techniques can be used to produce unbiased results by imputing values for the missing values. A more honest assessment is that the data are not missing at random (NMAR). A variable is NMAR if the different probabilities of missingness depend both on observed and unobserved variables. For example, some respondents may not provide income data because they are just naturally more suspicious. This variation in suspiciousness among respondents is not likely to be observed directly even though it may be partially correlated with other observed variables. The key issue here is the unobserved nature of some of the variables that produce missingness. Because they are unobserved, I have no way to account for them in my corrections. Therefore, I can never be certain that bias from missing values has been removed from my results. In practice, although NMAR is probably the norm in most datasets, the number of missing values on a single variable may be small enough that even the incorrect assumption of MAR or MCAR might have little effect on the results. In my previous example, I had only 16 missing values on religious affiliation for 2904 respondents. So, its not likely to have a large effect on the results no matter how I deal with the missing values. How exactly does one deal with those missing values? The data analyst typically has two choices. You can either remove cases with missing values or you can do some form of imputation. These two techniques are tied to the assumptions above. Removing cases is equivalent to assuming MCAR while imputation is equivalent to MAR. To illustrate these different approaches below, we will look at a new example. We are going to examine the Add Health data again, but this time I am going to use a dataset that does not have imputed values for missing values. In particular, we are going to look at the relationship between parental income and popularity. Lets look at how many missing values we have for income. summary(addhealth$parentinc) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00 23.00 40.00 45.37 60.00 200.00 1027 Yikes! We are missing 1027 cases of parental income. That seems like a lot. Lets go ahead and calculate the percent of missing cases for all of the variables in our data. temp &lt;- round(apply(is.na(addhealth), 2, mean)*100, 2) temp &lt;- data.frame(variable=names(temp), percent_missing=temp) rownames(temp) &lt;- NULL pander(as.data.frame(temp)) variable percent_missing indegree 0 race 0.05 sex 0 grade 1.18 pseudoGPA 1.91 honorsociety 0 alcoholuse 0.66 smoker 0.93 bandchoir 0 academicclub 0 nsports 0 parentinc 23.36 cluster 0 sweight 0 For most variables, we only have a very small number of missing values, typically less than 1% of cases. however, for parental income we are missing values for nearly a quarter of respondents. Removing Cases The simplest approach to dealing with missing values is to just drop observations (cases) that have missing values. In R, this is what will either happen by default or if you give a function the argument na.rm=TRUE. For example, lets calculate the mean income in the Add Health data: mean(addhealth$parentinc) ## [1] NA If we don’t specify how R should handle the missing values, R will report back a missing value for many basic calculations like the mean, median, and standard deviation. In order to get the mean for observations with valid values of income, we need to include the na.rm=TRUE option: mean(addhealth$parentinc, na.rm=TRUE) ## [1] 45.37389 The lm command in R behaves a little differently. It will automatically drop any observations that are missing values on any of the variables in the model. Lets look at this for a model predicting popularity by parental income and number of sports played: model &lt;- lm(indegree~parentinc+nsports, data=addhealth) summary(model) ## ## Call: ## lm(formula = indegree ~ parentinc + nsports, data = addhealth) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.7920 -2.6738 -0.7722 1.8585 22.2967 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.526054 0.114310 30.846 &lt; 2e-16 *** ## parentinc 0.012309 0.001884 6.534 7.39e-11 *** ## nsports 0.451061 0.048920 9.220 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.652 on 3367 degrees of freedom ## (1027 observations deleted due to missingness) ## Multiple R-squared: 0.04164, Adjusted R-squared: 0.04107 ## F-statistic: 73.14 on 2 and 3367 DF, p-value: &lt; 2.2e-16 Note the comment in parenthesis at the bottom that tells us we lost 1027 observations due to missingness. This approach is easy to implement but comes at a significant cost to the actual analysis. First, removing cases is equivalent to assuming MCAR, which is a strong assumption. Second, since most cases that you will remove typically have valid values on most of their variables, you are throwing away a lot of valid information as well as reducing your sample size. For example, all 1027 observations that were dropped in the model above due to missing values on parental income had valid responses the variable on number of sports played. If you are using a lot of variables in your analysis, then even with moderate amounts of missingness, you may reduce your sample size substantially. In situations where only a very small number of values are missing, then removing cases may be a reasonable strategy for reasons of practicality. For example, in the Add Health data, the race variable has missing values on two observations. Regardless of whether the assumption of MCAR is violated or not, the removal of two observations in a sample of 4,397 is highly unlikely to make much difference. Rather, than use more complicated techniques that are labor intensive, it probably makes much more sense remove them. Two strategies are available when you remove cases. You can either do available-case analysis or complete-case analysis. Both strategies refer to how you treat missing values in an analysis which will include multiple measurements that may use different numbers of variables. The most obvious and frequent case here is when you run multiple nested models in which you include different number of variables. For example, lets say we want to predict a student’s popularity by in a series of nested models: - Model 1: predict popularity by number of sports played - Model 2: add smoking and drinking behavior as predictors model 1 - Model 3: add parental income as a predictor to model 2 In this case, my most complex model is model 3 and it includes five variables: popularity, number of sports played, smoking behavior, drinking behavior, and parental income. I need to consider carefully how I am going to go about removing cases. Available-case analysis In available-case analysis (also called pairwise deletion) observations are only removed for each particular component of the analysis (e.g. a model) in which they are used. This is what R will do by default when you run nested models because it will only remove cases if a variable in that particular model is missing. model1.avail &lt;- lm(indegree~nsports, data=addhealth) model2.avail &lt;- update(model1.avail, .~.+alcoholuse+smoker) model3.avail &lt;- update(model2.avail, .~.+parentinc) As a sidenote, I am introducing a new function here called update that can be very useful for model building. The update function can be used on a model object to make some changes to it and then re-run it. In this case, I am changing the model formula, but you could also use it to change the data (useful for looking at subsets), add weights, etc. When you want to update the model formula, the syntax .~.+ will include all of the previous elements of the formula and then you can make additions. I am using it to build nested models without having to repeat the entire model syntax every time. Table 18: Models predicting number of friend nominations in Add Health data, using available-case analysis Model 1 Model 2 Model 3 (Intercept) 3.997*** 3.851*** 3.391*** (0.072) (0.079) (0.120) nsports 0.504*** 0.508*** 0.454*** (0.043) (0.043) (0.049) alcoholuseDrinker 0.707*** 0.663*** (0.155) (0.180) smokerSmoker 0.194 0.383* (0.162) (0.188) parentinc 0.012*** (0.002) R2 0.031 0.037 0.048 Num. obs. 4397 4343 3332 ***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05 Table 18 shows the results of the nested models in table form. Pay particular attention to the change in the number of observations in each model. In the first model, there are no missing values on nsports or indegree so we have the full sample size of 4397. In model 2, we are missing a few values on drinking and smoking behavior, so the sample size drops to 4343. In Model 3, we are missing a large number of observations, so the sample size drops to 3332. We lose nearly a quarter of the data from Model 2 to Model 3. The changing sample sizes across models make model comparisons extremely difficult. We don’t know if changes from model to model are driven by different combinations of independent variables or changes in sample size. Note for example that the effect of smoking on popularity almost doubles from Model 2 to Model 3. Was that change driven by the fact that we controlled for parental income or was it driven by the exclusion of nearly a quarter of the sample. We have no way of knowing. For this reason, available-case analysis is generally not the best way to approach your analysis. You don’t want your sample sizes to be changing across models. There are a few special cases (I will discuss one in the next section) where it may be acceptable and sometimes it is just easier in the early exploratory phase of a project, but in general you should use complete-case analysis. Complete-case analysis In complete-case analysis (also called listwise deletion) you remove observations that are missing on any variables that you will use in the analysis even for some calculations that may not involve those variables. This techniques ensures that you are always working with the same sample throughout your analysis so any comparisons (e.g. across models) are not comparing apples and oranges. In our example, I need to restrict my sample to only observations that have valid responses for all five of the variables that will be used in my most complex model. I can do this easily with the na.omit command which will remove any observation that is missing any value in a given dataset: addhealth.complete &lt;- na.omit(addhealth[,c(&quot;indegree&quot;,&quot;nsports&quot;,&quot;alcoholuse&quot;,&quot;smoker&quot;,&quot;parentinc&quot;)]) model1.complete &lt;- lm(indegree~nsports, data=addhealth.complete) model2.complete &lt;- update(model1.complete, .~.+alcoholuse+smoker) model3.complete &lt;- update(model2.complete, .~.+parentinc) Table 19: Models predicting number of friend nominations in Add Health data, using complete-case analysis Model 1 Model 2 Model 3 (Intercept) 4.051*** 3.876*** 3.391*** (0.085) (0.091) (0.120) nsports 0.490*** 0.494*** 0.454*** (0.049) (0.049) (0.049) alcoholuseDrinker 0.717*** 0.663*** (0.181) (0.180) smokerSmoker 0.372* 0.383* (0.189) (0.188) parentinc 0.012*** (0.002) R2 0.029 0.037 0.048 Num. obs. 3332 3332 3332 ***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05 Table 19 shows the results for the models using complete-case analysis. Note that the sample size for each model is the same and is equivalent to that for the most complex model in the available-case analysis above. It is now apparent that the change in the effect of smoking on popularity above was driven by the removal of cases and not the controls for parental income. I can see this because now that I have performed the same sample restriction on Model 2, I get a similar effect size for smoking. Its worth noting that this is not really a great thing, because it suggests that by dropping cases, I am biasing the sample in some way, but at least I can make consistent comparisons across models. When only a small number of cases are missing, then complete-case analysis is a reasonable option that is practical and not labor intensive. For example, I wouldn’t worry too much about the 54 cases lost due to the inclusion of smoking and drinking behavior in the models above. However, when you are missing substantial numbers of cases, either from a single variable or from the cumulative effects of dropping a moderate number of observations for lots of variables, then both case removal techniques are suspect. In those cases, it is almost certainly better to move to an imputation strategy. Imputation Imputing a “fake” value for a missing value may seem worse than just removing it, but in many cases imputation is a better alternative than removing cases. First, imputation allows us to preserve the original sample size as well as all of the non-missing data that would be thrown way for cases with some missing values. Second, depending on methodology, imputation may move from a MCAR assumption to a MAR assumption. There are a variety of imputation procedures, but they can generally be divided up along two dimensions. First, predictive imputations use information about other variables in the dataset to impute missing values, while non-predictive imputations do not. In most cases, predictive imputation uses some form of model structure to predict missing values. Predictive imputation moves the assumption on missingness from MCAR to MAR, but is more complex (and thus prone to its own problems of model mis-specification) and more labor-intensive. Second, random imputations include some randomness in the imputation procedure, while deterministic imputations do not include randomness. Including randomness is necessary in order to preserve the actual variance in the variable that is being imputed. Deterministic imputations will always decrease this variance and thus lead to under-estimation of inferential statistics. However, random imputation identify an additional issue because random imputations will produce slightly different results each time the imputation procedure is conducted. To be accurate, the researcher must account for this additional imputation variabilty in their results. In general, the gold standard for imputation is called multiple imputation which is a predictive imputation that include randomness and adjusts for imputation variability. However, this technique is also complex and labor-intensive, so it may not be the best practical choice when the number of missing values is small. Below, I outline several of these different approaches, from simplest to most complex. Non-predictive imputation The simplest form of imputation is to just replace missing values with a single value. Since the mean is considered the “best guess” for the non-missing cases, it is customary to impute the mean value here and so this technique is generally called mean imputation. We can perform mean imputation in R simply by using the bracket-and-boolean approach to identify and replace missing values: addhealth$parentinc.meani &lt;- addhealth$parentinc addhealth$incmiss &lt;- is.na(addhealth$parentinc) addhealth$parentinc.meani[addhealth$incmiss] &lt;- mean(addhealth$parentinc, na.rm=TRUE) I create a separate variable called parentinc.meani for the mean imputation so that I can compare it to other imputation procedures later. I also create an incmiss variable that is just a boolean indicating whether income is missing for a given respondent. Figure 76: The effect of mean imputation of parental income on the scatterplot between parental income and number of friend nominations Figure 76 shows the effect of mean imputation on the scatterplot of the relationship between parental income and number of friend nominations. All of the imputed values fall along a vertical line that corresponds to the mean parental income for the non-missing values. The effect on the variance of parental income is clear. Because this imputation procedure fits a single value, its imputation does not match the spread of observations along parental income. We can also see this by comparing the standard deviations of the original variable and the mean imputed one: sd(addhealth$parentinc, na.rm=TRUE) ## [1] 33.6985 sd(addhealth$parentinc.meani) ## [1] 29.50069 Since the variance of the independent variable is a component of the standard error calculation (see the first section of this chapter for details), underestimating the variance will lead to underestimates of the standard error in the linear model. A similar method that will account for the variance in the independent variable is to just randomly sample values of parental income from the valid responses. I can do this in R using the sample function. addhealth$parentinc.randi &lt;- addhealth$parentinc addhealth$parentinc.randi[addhealth$incmiss]&lt;-sample(addhealth$parentinc[!addhealth$incmiss], sum(addhealth$incmiss), replace = TRUE) I create a new variable for this imputation. The sample function will sample from a given data vector (in this case the valid responses to parental income) a specified number of times (in this case the number of missing values). I also specify that I want to sample with replacement, which means that I replace values that I have already sampled before drawing a new value. Figure 77: The effect of random imputation of parental income on the scatterplot between parental income and number of friend nominations Figure 77 shows the same scatterplot as above, but this time with random imputation. You can see that my imputed values are now spread out across the range of parental income, thus preserving the variance in that variable. Although random imputation is preferable to mean imputation, both methods have a serious drawback. Because I am imputing values without respect to any other variables, these imputed values will by definition be uncorrelated with the dependent variable. You can see this in figure 77 where I have drawn the best fitting line for the imputed and non-imputed values. The red line for the imputed values is basically flat because there will be no association except for that produced by sampling variability. Because imputed values are determined without any prediction, non-predictive imputation will always bias estimates of association between variables downward. Table 20: Comparison of measures of assocation between parental income and number of friend nominations and variance of parental income in Add Healt data, using different imputation techniques. sample correlation slope sd Valid cases 0.132 0.0146 33.7 Valid cases + mean imputed 0.117 0.0146 29.5 Valid cases + random imputed 0.109 0.0121 33.3 Table 20 compares measures of association and variance in the Add Health data for these two techniques to the same numbers when only valid cases are used. The reduction in correlation in the two imputed samples is apparent, relative to the case with only valid responses. The reduction in variance of parental income is also clear for the case of mean imputation. Interestingly, the mean imputation does not downwardly bias the measure of slope because the reduction in correlation is perfectly offset by the reduction in variance. However, this special feature only holds when there are no other independent variables in the model and is also probably best thought of as a case of two wrongs (errors) not making a right (correct measurement). In general, because of these issues with downward bias, non-predictive imputation is not and advisable technique. However, in some cases it may be preferable as a “quick and dirty” method that is useful for initial exploratory analysis or because the number of missing cases is moderate. It may also be reasonable if the variable that needs to be imputed is intended to serve primarily as a control variable and inferences are not necessarily going to be drawn for this variable itself. In these cases where non-predictive imputation serves as a quick and dirty method, one additional technique, which I call mean imputation with dummy is advisable. In this case, the researcher performs mean imputation as above but also includes the missingness boolean variable into the model specification as above. The effect of this dummy is to estimate a slope and intercept for the imputed variable that is unaffected by the imputation, while at the same time producing a measure of how far out mean imputation is off relative to where the model expects the missing cases to fall. Since, we already have an incmiss dummy variable, we can implement this model very easily. Lets try it in a model that predicts number of friend nominations by parental income and number of sports played. model.valid &lt;- lm(indegree~parentinc+nsports, data=addhealth) model.meani &lt;- lm(indegree~parentinc.meani+nsports, data=addhealth) model.mdummyi &lt;- lm(indegree~parentinc.meani+nsports+incmiss, data=addhealth) Table 21: Models predicting number of friend nominations in Add Health data, using different methods of imputation No imputation Mean imputation Mean imputation w/dummy Parent income 0.01231*** (0.00188) Parent income, mean imputed 0.01220*** 0.01221*** (0.00186) (0.00186) Number of sports played 0.45106*** 0.47134*** 0.46945*** (0.04892) (0.04292) (0.04297) Income missing dummy -0.12163 (0.12911) Intercept 3.52605*** 3.47950*** 3.50954*** (0.11431) (0.10678) (0.11144) R2 0.04164 0.03999 0.04018 Num. obs. 3370 4397 4397 ***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05 Table 21 shows the results of these models. In this case, the results are very similar. The coefficient on the income missing dummy basically tells us how far the average number of friend nominations was off for cases with income missing relative to what was predicted by the model. In this case, the model is telling us that with a mean imputation the predicted value of friend nominations for respondents with missing income data is 0.12 nominations higher than where it is in actuality. If we believe our model, this might suggest that the parental income of those with missing income is somewhat lower than those who are not missing, on average. This quick and dirty method is often useful in practice, but keep in mind that it still has the same problems of underestimating variance and assuming MCAR. A far better solution would be to move to predictive imputation. Predictive imputation In a predictive imputation, we use other variables in the dataset to get predicted values for cases that are missing. In order to do this, an analyst must specify some sort of model to determine predicted values. A wide variety of models have been developed for this, but for this course, we will only discuss the simple case of using a linear model to predict missing values (sometimes called regression imputation). In the case of the Add Health data, we have a variety of other variables that we can consider using to predict parental income including race, GPA, alcohol use, smoking, honor society membership, band/choir membership, academic club participation, and number of sports played. We also could consider using the dependent variable (number of friend nominations) here but there is some debate on whether dependent variables should be used to predict missing values, so we will not consider it here. There is really no reason not to use as much information as possible here, so I will use all of these variables. However, in order to model this correctly, I want to consider the skewed nature of the my parental income data. To reduce the skewness and produce better predictions, I will transform parental income in my predictive model by square rooting it. I can then fit a model predicting parental income: addhealth$parentinc.regi &lt;- addhealth$parentinc model &lt;- lm(sqrt(parentinc)~race+pseudoGPA+honorsociety+alcoholuse+smoker +bandchoir+academicclub+nsports, data=addhealth) predicted &lt;- predict(model, addhealth) addhealth$parentinc.regi[addhealth$incmiss] &lt;- predicted[addhealth$incmiss]^2 summary(addhealth$parentinc.regi) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00 25.19 39.83 43.87 54.20 200.00 39 After fitting the model, I just use the predict command to get predicted parental income values (square rooted) for all of my observations. I remember to square these predicted values when I impute them back. One important thing to note is that my imputed parental income still has 39 missing values. That is a lot less than 1027, but how come we didn’t remove all the missing values? The issue here is that some of my predictor variables (particularly smoking, alcohol use, and GPA) also have missing values. Therefore, for observations that are missing on those values as well as parental income, I will get missing values in my prediction. I will address how to handle this problem further below. Figure 78: The relationship between predicted values of imputed parental income and actual or imputed values, using regression imputation This regression imputation only accounts for variability in parental income that is accountable for in the model and ignores residual variation. This can be clearly seen in Figure 78 where the actual values of parental income are widely dispersed around the predicted values, except for cases where missing values were imputed. This imputation procedure therefore underestimates the total variance in parental income with similar consequences as above. To address this problem, we can use random regression imputation which basically adds a random “tail” onto each predicted value based on the variance of our residual terms. It is also necessary to define a distribution from which to sample our random tails. We will use the normal distribution. addhealth$parentinc.rregi &lt;- addhealth$parentinc model &lt;- lm(sqrt(parentinc)~race+pseudoGPA+honorsociety+alcoholuse+smoker +bandchoir+academicclub+nsports, data=addhealth) predicted &lt;- predict(model, addhealth) addhealth$parentinc.rregi[addhealth$incmiss] &lt;- (predicted[addhealth$incmiss]+ rnorm(sum(addhealth$incmiss), 0, sigma(model)))^2 The rnorm function samples from a normal distribution with a given mean and standard deviation. We use the sigma function to pull the standard deviation of the residuals from our model. Figure 79: The relationship between predicted values of imputed parental income and actual or imputed values, using random regression imputation Figure 79 shows that the variation of our imputed values now mimics that of the actual values. We can also compare the standard deviations across imputations to see that we have preserved the variation in our valid data: sd(addhealth$parentinc, na.rm=TRUE) ## [1] 33.6985 sd(addhealth$parentinc.regi, na.rm=TRUE) ## [1] 30.10024 sd(addhealth$parentinc.rregi, na.rm=TRUE) ## [1] 33.07079 Chained equations As we saw above, one downside of using a model to predict missing values is that missing values on other variables in the dataset can cause the imputation procedure to not impute all of the missing values. There are a few different ways of handling this but the most popular technique is to use an iterative procedure called **multiple imputation by chained equations* or MICE for short. The basic procedure is as follows: All missing values are given some placeholder value. This might be the mean value, for example. For one variable, the placeholder values are removed and missing values put back in. These missing values are then predicted and imputed by some model for this variable. Step 2 is repeated for all variables with missing values. When all variables have been imputed, we have completed one iteration. Steps 2-3 are then repeated again for some number of iterations. The number of iterations necessary may vary by data, but five iterations is typical. The purpose of the iterations is to improve the imputation by moving further and further away from the mean imputation that was used to get an initial fit. This procedure is complicated to implement in practice, but luckily the mice library in R already provides an excellent implementation. Furthermore, the mice library has a variety of modeling options that include the ability to handle missing values in both quantitative and categorical models. For quantitative variables, it does not by default use regression imputation, but rather a technique called predictive mean matching that is more flexible and resistant to mis-specification. The basic command in the mice library to run MICE is … mice: library(mice) addhealth.complete &lt;- mice(addhealth[,c(&quot;indegree&quot;,&quot;race&quot;,&quot;sex&quot;,&quot;grade&quot;,&quot;pseudoGPA&quot;, &quot;honorsociety&quot;,&quot;alcoholuse&quot;,&quot;smoker&quot;,&quot;bandchoir&quot;, &quot;academicclub&quot;,&quot;nsports&quot;,&quot;parentinc&quot;)], m=5) ## ## iter imp variable ## 1 1 race grade pseudoGPA alcoholuse smoker parentinc ## 1 2 race grade pseudoGPA alcoholuse smoker parentinc ## 1 3 race grade pseudoGPA alcoholuse smoker parentinc ## 1 4 race grade pseudoGPA alcoholuse smoker parentinc ## 1 5 race grade pseudoGPA alcoholuse smoker parentinc ## 2 1 race grade pseudoGPA alcoholuse smoker parentinc ## 2 2 race grade pseudoGPA alcoholuse smoker parentinc ## 2 3 race grade pseudoGPA alcoholuse smoker parentinc ## 2 4 race grade pseudoGPA alcoholuse smoker parentinc ## 2 5 race grade pseudoGPA alcoholuse smoker parentinc ## 3 1 race grade pseudoGPA alcoholuse smoker parentinc ## 3 2 race grade pseudoGPA alcoholuse smoker parentinc ## 3 3 race grade pseudoGPA alcoholuse smoker parentinc ## 3 4 race grade pseudoGPA alcoholuse smoker parentinc ## 3 5 race grade pseudoGPA alcoholuse smoker parentinc ## 4 1 race grade pseudoGPA alcoholuse smoker parentinc ## 4 2 race grade pseudoGPA alcoholuse smoker parentinc ## 4 3 race grade pseudoGPA alcoholuse smoker parentinc ## 4 4 race grade pseudoGPA alcoholuse smoker parentinc ## 4 5 race grade pseudoGPA alcoholuse smoker parentinc ## 5 1 race grade pseudoGPA alcoholuse smoker parentinc ## 5 2 race grade pseudoGPA alcoholuse smoker parentinc ## 5 3 race grade pseudoGPA alcoholuse smoker parentinc ## 5 4 race grade pseudoGPA alcoholuse smoker parentinc ## 5 5 race grade pseudoGPA alcoholuse smoker parentinc In this case, I specified all of the variables that I wanted to go into my multiple imputation. If you want all of the variables in your dataset to be used, you can just feed in the dataset with no list of variable names. You can see that mice went through five iterations of imputation, with a list of the variables imputed and the order in which they were imputed. You can also see that it cycled through something called “imp.” This value corresponds to the m=5 argument I specified. This is the number of times that mice performed the entire imputation procedure. In this case, I now have five complete datasets (datasets with no missing values) in the object addhealth.complete. To access any one of them, I need to use the complete function with an id number: summary(complete(addhealth.complete, 1)) ## indegree race ## Min. : 0.000 White :2638 ## 1st Qu.: 2.000 Black/African American :1145 ## Median : 4.000 Latino : 400 ## Mean : 4.551 Asian/Pacific Islander : 161 ## 3rd Qu.: 6.000 Other : 27 ## Max. :30.000 American Indian/Native American: 26 ## sex grade pseudoGPA honorsociety ## Length:4397 Min. : 7.000 Min. :1.000 Mode :logical ## Class :character 1st Qu.: 8.000 1st Qu.:2.250 FALSE:3990 ## Mode :character Median :10.000 Median :3.000 TRUE :407 ## Mean : 9.509 Mean :2.829 ## 3rd Qu.:11.000 3rd Qu.:3.500 ## Max. :12.000 Max. :4.000 ## alcoholuse smoker bandchoir academicclub ## Non-drinker:3670 Non-smoker:3732 Mode :logical Mode :logical ## Drinker : 727 Smoker : 665 FALSE:3340 FALSE:3578 ## TRUE :1057 TRUE :819 ## ## ## ## nsports parentinc ## Min. :0.000 Min. : 0.00 ## 1st Qu.:0.000 1st Qu.: 23.00 ## Median :1.000 Median : 40.00 ## Mean :1.098 Mean : 45.13 ## 3rd Qu.:2.000 3rd Qu.: 60.00 ## Max. :6.000 Max. :200.00 I can run a model on my first complete dataset like so: model &lt;- lm(indegree~parentinc, data=complete(addhealth.complete,1)) summary(model) ## ## Call: ## lm(formula = indegree ~ parentinc, data = complete(addhealth.complete, ## 1)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.1241 -2.6237 -0.7437 1.7815 25.2263 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.873450 0.091761 42.212 &lt;2e-16 *** ## parentinc 0.015004 0.001625 9.233 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.657 on 4395 degrees of freedom ## Multiple R-squared: 0.01903, Adjusted R-squared: 0.01881 ## F-statistic: 85.25 on 1 and 4395 DF, p-value: &lt; 2.2e-16 Addressing imputation variability Why did I run five different imputations using the mice function in the example above? To explore this question, lets first look at the slope predicting the number of friend nominations by parental income across all five complete imputations: coefs &lt;- NULL for(i in 1:5) { coefs &lt;- c(coefs, coef(lm(indegree~parentinc, data=complete(addhealth.complete,i)))[2]) } coefs ## parentinc parentinc parentinc parentinc parentinc ## 0.01500401 0.01339734 0.01357633 0.01489504 0.01389638 I get different values on each complete imputation. This is because there is an element of randomness in my imputation procedure. That randomness is needed to correctly impute while preserving the variance in the underlying variables, but it adds an additional layer of uncertainty to understanding my results. Which of these values should I use for my results? This additional layer of uncertainty is called imputation variability. Imputation variability adds additional uncertainty above and beyond sampling variability when attempting to make statistical inferences. To accurately account for imputation variability, we need to use multiple imputation. The multiple imputation process is as follows: Use imputation process with random component to impute missing values and repeat this process to produce m separate complete datasets. Each of these datasets will be somewhat different due to the randomization of imputation. Usually m=5 is sufficient. Run m separate parallel models on each imputed dataset. As a result, you will have m sets of regression coefficients and standard errors. Pool the regression coefficients across datasets by taking the mean across all m datasets. Pool standard errors by taking the average standard errors across all m datasets plus the between model standard deviation in coefficients. The formula for the overall standard error is: \\[SE_{\\beta}=\\sqrt{W+B+\\frac{B}{m}}\\] Where \\(W\\) is the squared average standard error across all \\(m\\) datasets and \\(B\\) is the variance in coefficient estimates across all \\(m\\) models. The mice library provides some nice utilities for accounting for multiple imputation in basic linear models. However, it is worth learning how to to do this procedure by hand, so I will first show you how to do it without using built-in functions. The first step is to loop through each imputation, calculate the model and extract the coefficients and standard errors: b &lt;- se &lt;- NULL for(i in 1:5) { model &lt;- lm(indegree~parentinc+smoker+alcoholuse+nsports, data=complete(addhealth.complete,i)) b &lt;- cbind(b, coef(model)) se &lt;- cbind(se, summary(model)$coef[,2]) } The b and se objects are just matrices of the regression coefficients and standard errors across imputations. For example: round(b,3) ## [,1] [,2] [,3] [,4] [,5] ## (Intercept) 3.353 3.418 3.421 3.375 3.399 ## parentinc 0.012 0.011 0.011 0.012 0.011 ## smokerSmoker 0.180 0.219 0.185 0.151 0.211 ## alcoholuseDrinker 0.643 0.630 0.633 0.663 0.645 ## nsports 0.462 0.468 0.467 0.460 0.468 With the application of the apply command we can estimate averages for these values across imputations as well as the variability in regression coefficients across models. b.pool &lt;- apply(b,1,mean) between.var &lt;- apply(b,1,var) within.var &lt;- apply(se^2,1,mean) se.pool &lt;- sqrt(within.var+between.var+between.var/5) t.pool &lt;- b.pool/se.pool pvalue.pool &lt;- (1-pnorm(abs(t.pool)))*2 results &lt;- data.frame(b.pool, se.pool, t.pool, pvalue.pool) round(results,4) ## b.pool se.pool t.pool pvalue.pool ## (Intercept) 3.3934 0.1072 31.6469 0.000 ## parentinc 0.0114 0.0019 6.1500 0.000 ## smokerSmoker 0.1891 0.1623 1.1650 0.244 ## alcoholuseDrinker 0.6428 0.1547 4.1548 0.000 ## nsports 0.4651 0.0432 10.7618 0.000 We now have results that make use of a complete dataset and also adjust for imputation variability. We can also use the results above to estimate the relative share of the between and within variation to our total uncertainty. Figure 80 shows the relative contribution of between and within variability to our total uncertainty. Because most variables in this model are missing few or no values, the contribution of between sample imputation variability is quite small for most cases. However, for parental income which was missing a quarter of observations, it is a substantial share of around 30% of the total variability. Figure 80: Share of variability for each estimated coefficient that comes from between sample imputation variability vs. within sample sampling variabilty The for-loop approach above is an effective method for pooling results for multiple imputation and is flexible enough that you can utilize a wide variety of techniques within the for-loop, such as adjustments for sample design. However, if you are just running vanilla lm commands, then the mice library also has an easier approach: model.mi &lt;- pool(with(addhealth.complete, lm(indegree~parentinc+smoker+alcoholuse+nsports))) summary(model.mi) ## estimate std.error statistic df p.value ## (Intercept) 3.39336517 0.107225815 31.646905 448.90566 0.000000e+00 ## parentinc 0.01137895 0.001850239 6.149989 80.37632 8.502967e-10 ## smokerSmoker 0.18910211 0.162323257 1.164972 1991.18654 2.440992e-01 ## alcoholuseDrinker 0.64276779 0.154703542 4.154836 4016.88515 3.322776e-05 ## nsports 0.46513313 0.043220927 10.761757 4027.30338 0.000000e+00 As I said earlier, multiple imputation is the gold standard for dealing with missing values. However, it is important to realize that it is not a bulletproof solution to the problem of missing values. It still assumes MAR and if the underlying models used to predict missing values are not good, then it may not produce the best results. Ideally, when doing any imputation procedure you should also perform an analysis of your imputation results to ensure that they are reasonable. These techniques are beyond the scope of this class, but further reading links can be found on Canvas. "],
["multicollinearity-and-scale-creation.html", "Multicollinearity and Scale Creation", " Multicollinearity and Scale Creation The problem of multicollinearity arises when the independent variables in a linear model are highly correlated with one another. In such a case it is possible to predict with reasonable accuracy the value of one independent variable based on all of the other independent variables in the model. Multicollinearity creates problem for model estimation in two ways. First, multicollinearity between independent variables will increase the standard error on regression coefficients making it difficult to identify statistically significant effects. Second, in nested models with highly collinear variables, results from model to model can be highly variable and inconsistent. It is not unusual that two highly collinear variables both produce substantively large and statistically significant effects when put in a model separately, but when put in together, both variables fail to produce a statistically significant effect. The intuition behind why this happens is fairly straightforward. If two independent variables are highly correlated with one another, then when they are both included in a model, it is difficult to determine which variable is actually driving the effect, because there are relatively few cases where the two variables are different. Thus, the two variables are effectively blowing each other up in the full model. Avoid the Singularity Structural multicollinearity occurs when one variable is perfectly predicted by some combination of other variables. In this situation, a model with all of these variables cannot be estimated unless one variable is dropped. To provide a simple example of perfect multicollinearity, I have coded a variable in the crime dataset for the percent female in a state. This term is perfectly collinear with the variable percent male in a state and thus it will cause problems for model estimation if I include them both: crimes$PctFemale &lt;- 100-crimes$PctMale summary(lm(Violent~PctMale+PctFemale, data=crimes)) ## ## Call: ## lm(formula = Violent ~ PctMale + PctFemale, data = crimes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -293.57 -104.01 -54.44 88.98 783.97 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4317.26 1628.20 2.652 0.0108 * ## PctMale -79.75 33.01 -2.416 0.0195 * ## PctFemale NA NA NA NA ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 186.5 on 49 degrees of freedom ## Multiple R-squared: 0.1064, Adjusted R-squared: 0.08819 ## F-statistic: 5.836 on 1 and 49 DF, p-value: 0.01948 Note that the result for PctFemale is missing and the note above the coefficient table tells us that one variable is not defined because of a “singularity.” The singularity here is a matrix singularity which technically means that no inverse matrix exists for the design matrix of \\(X\\). However, the intuition here is more straightforward: PctFemale contains no information that is not already contained in PctMale so it is impossible to predict separate effects for each variable. A 1% increase in the percent male means a 1% increase in the percent female, and vice versa. We have already seen an example of perfect multicollinearity in the case of dummy variables for categorical variables. We always have to leave one dummy variable out of the model to serve as the reference category. This is because once we know the results for all but one category, we also know the results for that category. Lets say I have four categories of marital status: never married, married, widowed, and divorced. If I know you are not married, widowed, or divorced, then by process of elimination you must be never married. Structural multicollinearity is generally a specification error by the researcher rather than a true problem with the data. What we are more concerned about is data-based multicollinearity in which one variable can be predicted with high but not perfect accuracy by other variables in the model. Detecting Data-Based Multicollinearity As an example for this section, we will use data that I have collected with Nicole Marwell on the the contracting out of social services in New York City. The data were collected from 1997-2001 and contain information on the amount of dollars per capita going to neighborhoods in NYC for social services that were contracted out to non-profit organization. Our key concern here is with the distribution of this money and whether or not it was it going to more socioeconomically disadvantaged neighborhoods or more socioeconomically advantaged neighborhoods. The unit of analysis for the dataset is a “health area” which is a bureaucratic unit used by the city government that loosely corresponds to a neighborhood. The dataset has four variables: amtcapita: the amount of dollars received by a health area divided by population size. This variable is heavily right skewed so we log it in all of the models. poverty: the poverty rate in the health area. unemployed: the unemployment rate in the health area. income: the median household income in the health area. This is also right-skewed so we log it. Median income, the poverty rate, and the unemployment rate are all good measures of the socioeconomic status of a neighborhood, but we also have reason to be concerned that multicollinearity might be a problem here. The correlation matrix A first step to detecting data-based multicollinearity is to examine the correlation matrix between the independent variables. This can be done simply with the cor command: nyc$lincome &lt;- log(nyc$income) cor(nyc[,c(&quot;poverty&quot;,&quot;unemployed&quot;,&quot;lincome&quot;)]) ## poverty unemployed lincome ## poverty 1.0000000 0.6982579 -0.9120680 ## unemployed 0.6982579 1.0000000 -0.7419472 ## lincome -0.9120680 -0.7419472 1.0000000 Another more visual-pleasing approach to looking at these correlations is to graph a correlogram, which is a figure that shows the correlations between a set of variables. The excellent corrgram package in R will do this for us, and it also includes a variety of customizations that allow us to visualize this correlation in different ways. library(corrgram) corrgram(nyc[,c(&quot;poverty&quot;,&quot;unemployed&quot;,&quot;lincome&quot;)], upper.panel=&quot;panel.cor&quot;, lower.panel=&quot;panel.pts&quot;) Figure 81: A correlogram showing the relationship between three measures of neighborhood socioeconomic statsu in NYC contracting out data In this case, I am showing the correlation coefficients in the upper panel and the full bivariate scatterplots in the lower panel. the numbers are also shaded somewhat by their strength of association, although in this case the correlations are so high that the shading is not very apparent. The correlation between these three variables is very high and raises immediate concerns about a potential problem of multicollinearity if we were to try to fit these models. Examining model results Examining correlations between independent variables prior to running models is advisable. However, we can also sometimes detect multicollinearity by looking at model results across nested models. To show how this works, I have run all the possible models that involve some combination of these three variables and reported them in Table 22 below. Table 22: Models predicting the amount per capita in contracted out social services (logged) by three measures of socioeconomic status, NYC health areas Model 1 Model 2 Model 3 Model 4 Model 5 Model 6 Model 7 Intercept 4.142*** 4.135*** 10.238*** 4.121*** 3.078 -24.699*** -29.597*** (0.172) (0.231) (2.052) (0.226) (3.272) (4.941) (5.309) Poverty rate 0.039*** 0.038*** 0.127*** 0.124*** (0.007) (0.010) (0.017) (0.017) Unemployment rate 0.091*** 0.005 0.100** 0.080* (0.024) (0.033) (0.036) (0.033) Median HH income (log) -0.493* 0.091 2.516*** 2.912*** (0.191) (0.282) (0.431) (0.458) R2 0.080 0.041 0.019 0.080 0.041 0.164 0.178 Num. obs. 341 341 341 341 341 341 341 ***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05 The presence of multicollinearity is most easily detectable in models with both the poverty rate and median household income. Compare the results from Model 5 to those from Models 1 and 3. The effect on poverty rate, more than triples in size and the standard error is doubled. The results for income completely reverse direction and become very large and the standard error more than doubles. These are clear signs of multicollinearity. These same patterns are visible in Model 7. Unemployment is less affected by these issues, but its standard error also increases fairly substantially in models with either of the other two variables. In particular the effects of income here seem particularly sensitive to the other variables. These are all hints that multicollinearity is a serious problem in the models. Calculating the variance inflation factor We can more formally assess the problem of multicollinearity in models by calculating the variance inflation factor or VIF. VIF measures the degree to which the variance in estimating an independent variable is inflated in a model due to the correlation with other independent variables in the model. For a given independent variable \\(i\\), the VIF is defined as: \\[VIF_i=\\frac{1}{1-R_i^2}\\] where \\(R_i^2\\) is the r-squared value from a model in which the \\(i\\) independent variable is predicted by all of the other independent variables in the model. For example, if we wanted to calculate the VIF for the poverty rate in a model that also included the unemployment rate and the median household income, we would do the following: model &lt;- lm(poverty~unemployed+lincome, data=nyc) 1/(1-summary(model)$r.squared) ## [1] 5.984485 So, the VIF is 5.98. This number is in terms of variance, so if we want to know the inflation in the standard errors, we need to square root it. In this case, the square root produces a value of 2.45 indicating that the standard errors for the poverty rate will be more than double in a model with both unemployment and log income, as a result of multicollinearity. The car library has a vif function in which you can feed in a model object and get out the VIF for all independent variables in the model: library(car) model.full &lt;- lm(log(amtcapita)~poverty+unemployed+lincome, data=nyc) vif(model.full) ## poverty unemployed lincome ## 5.984485 2.238379 6.822173 Since a VIF of four implies a doubling of the standard error, this is often used as a rule of thumb to identify problematic levels of VIF. However, even a smaller increase in the standard error can produce problems for model estimation, so this rule of thumb should be exercised with care. Addressing Multicollinearity Given that you have identified data-based multicollinearity, what do you do about it? A simple approach is to remove some of the highly collinear variables. However, this approach has some drawbacks. First, you have to decide which variable(s) to keep and which to remove. Because the effects of these variables are not easily distinguished, making this decision may not be easy. If the variables operationalize different conceptual ideas, you don’t have a strong empirical rationale for preferring one over the other. Second, because the variables are not perfectly correlated, you are throwing away some information. Another approach is to run separate models with only one of the highly collinear variables in each model. The primary advantage of this approach over removal is that you are being more honest with yourself (and potential readers) about the indeterminacy over which variable properly predicts the outcome. However, in this case you are also underestimating the total effects of these variables collectively and their effect as control variables in models with additional variables. It also can become unworkably complex if you have multiple sets of highly correlated variables. If the variables that are highly collinear are all thought to represent the same underlying conceptual variable, then another approach is to combine these variables into a single scale. In my example, for instance, all three of my independent variables are thought to represent I turn to the mechanics of such scale creation in the next section. Creating Scales We can create a simple summated scale simply by summing up the responses to a set of variables. However, there are several issues that we need to consider before doing this summation. First, we need to consider how to handle the case of categorical variables. If we have a set of yes/no questions, we might simply sum up the number of “yes” responses. For example, the GSS has a series of questions about whether people would allow certain groups (e.g. atheists, communists, racists, militarists) to (1) speak in their community, (2) teach at a university, or (3) have a book in a library. Many researchers have summed up the number of “yes” responses across all groups and questions to form an index of “tolerance.” Ordinal variables with more than two categories are more difficult. A typical example is a Likert-scale question that asks for someone’s level of agreement with a statement (e.g. “strongly disagree”, “disagree”,“neither”,“agree”,“strongly agree”). A typical approach is to score responses (e.g. from 1-5 for five categories) and then to sum up those scores. However, since we are dealing with ordinal categorical responses, we are making an assumption that a change of one on this scale is the same across all levels. Second, we need to consider that some variables might be measured on different unit scales. If we are counting up a score on a set of Likert-scale questions that are all scored 1-5, then this is not usually considered an important issue. However, if we want to combine variables measured in very different units or with different spreads, then these variables will not be represented equally in th scale unless we first standardize them. The most common approach to standardization is to take the z-score of a variable by subtracting its mean and dividing by its standard deviation: \\[z_i=\\frac{x_i-\\bar{x}}{s_x}\\] When variables are re-scaled in this way, they will all have a mean of zero and a standard deviation of one. This re-scaling is easy to do manually in R, but the scale function will also do it for you: nyc$poverty.z &lt;- scale(nyc$poverty) nyc$unemployed.z &lt;- scale(nyc$unemployed) mean(nyc$unemployed.z) ## [1] -1.45899e-16 sd(nyc$unemployed.z) ## [1] 1 Even when re-scaling is not necessary because variables are all measured on the same scale, I think re-scaling can be useful so that the final summated scale is easier to interpret. Third, we need to be careful to make sure that all of the variables are coded in the same direction relative to the underlying concept. In my case, median household income is coded in the opposite direction as poverty and unemployment. Whereas higher poverty and unemployment indicates greater disadvantage, higher household income indicates less disadvantage. in these cases, you need to reverse the coding of some of the variables so that higher values are associated with being higher on the underlying conceptual variable you are trying to measure. In my case, I can do this for income by multiplying by -1 after re-scaling: nyc$lincome.z &lt;- -1 * scale(nyc$lincome) Once all of these considerations are done, I can add up my re-scaled and properly coded variables to create a single scale of “neighborhood deprivation.” It is not necessary but I usually like to re-scale the final summated scale again so that one unit equals a one standard deviation change. nyc$deprivation.summated &lt;- scale(nyc$poverty.z+nyc$unemployed.z+nyc$lincome.z) And now lets use this summated scale in the model: model.summated &lt;- lm(log(amtcapita)~deprivation.summated, data=nyc) pander(tidy(model.summated)) term estimate std.error statistic p.value (Intercept) 4.947 0.08807 56.18 8.329e-174 deprivation.summated 0.3748 0.0882 4.249 2.775e-05 The model predicts that a one standard deviation increase in a neighborhood’s deprivation score is associated with about a 45% increase (\\(100 * (e^{0.3748}-1)\\)) in the amount of per-capita funding for social services. Assessing a scale’s internal consistency One important measure of the quality of a scale composed of multiple variables is the internal consistency (also sometimes called internal reliability) of the scale. Internal consistency is assessed by the degree to which variables within the scale are correlated with one another. Although examination of the full correlation matrix provides some measure of this internal consistency, the most common single measure used to test for internal consistency is Cronbach’s alpha. We won’t go into the details of how to calculate Cronbach’s alpha here. It ranges from a value of 0 to 1, with higher values indicating greater internal consistency. Intuitive explanations for what Cronbach’s alpha measures are somewhat lacking, but it gives something like the expected correlation between two items measuring the underlying construct. Cronbach’s alpha can be easily in R with the alpha function in the psych library. You can feed all the variables that make up the scale into Cronbach’s alpha or just the correlation matrix between these variables. The variables do not need to be re-scaled but we do not to ensure they are all coded in the same direction. library(psych) alpha(cor(nyc[,c(&quot;poverty.z&quot;,&quot;unemployed.z&quot;,&quot;lincome.z&quot;)])) ## [1] NA NA NA NA NA NA NA NA NA Cronbach’s alpha ranges from a value of 0 to 1. As a rule of thumb, many people use a value of 0.7 as a benchmark for a good Cronbach’s alpha, although as with all rules of thumb, caution should be taken in interpreting this too literally. In this case, the “raw alpha” of 0.92 is very good and suggests that the internal consistency of this deprivation measure is high. This function also returns a sensitivity analysis telling us how much Cronbach’s alpha would change if we were to remove a given variable from the scale. In this case, the results suggest that we would get even higher internal consistency if we removed the unemployment rate. However, since the alpha on all three was so high, it seems unnecessary to drop it in this case. Cronbach’s alpha is often thought of as a test of whether the individual items all measure the same “latent” construct. While, it can be thought of as a test of whether items measure a construct vs no construct at all, it does not directly address another issue which is that items used as a measure of a single scale may instead be related to multiple latent constructs. In order to address that issue, we must move to factor analysis. Factor Analysis Generally speaking, factor analysis is a broad set of techniques that are designed to test whether specific variables are related to one or more underlying latent constructs. Most of the original work on factor analysis comes out of psychology where researchers were interested in whether items though to capture underlying latent concepts like “anxiety” and “depression” were actually correlated with some underlying constructs and whether these constructs were Differentiable. So, if you give respondents a survey with several items measuring anxiety and several other items measuring depression, you want the anxiety measures to all correlate highly and the depression measures to all correlate highly, but for the two groups to be distinguishable from each other. Factor analysis is so broadly deployed across disciplines that there is some terminological confusion in the way people talk about specific techniques. There are two separate approaches methodologically to factor analysis. In the first approach, often called principal factors analysis or principal axis analysis, the researcher only uses information about the shared variance (the communality in factor analysis speak) between items to conduct a factor analysis. In the second approach, principal component analysis (PCA), the researcher uses the entire variance across all items. To add greater confusion, people often refer to factor analysis as the first technique and treat PCA as something distinct, although factor analysis should properly be thought of as encompassing both techniques. In practice, the results across these two techniques are usually quite similar. The argument for using one approach or the other is whether the researcher is interested primarily in data reduction (simplifying analyses by combining similar items) or truly measuring a latent construct. For our purposes, I will outline principal factor analysis here, but the pca command in the psych package can be used in an almost identical fashion to conduct a PCA. To begin, lets assume a simple model with three items (\\(z_1\\), \\(z_2\\), and \\(z_3\\)) which we believe are measured by a single latent construct, \\(F\\). All items are standardized as z-scores. We believe that the outcome for a given observation on these three items is a function of the latent construct and some component unique to each item (\\(Q_1\\), \\(Q_2\\), and \\(Q_3\\)). We then can construct a system of equations like so: \\[z_{i1}=b_1F_i+u_1Q_1\\] \\[z_{i2}=b_2F_i+u_2Q_2\\] \\[z_{i3}=b_3F_i+u_3Q_3\\] Each of the \\(b\\) values below is the correlation coefficient between the underlying facto (\\(F\\)) and the given item. These are called factor loadings. The \\(F_i\\) values are the factor scores underlying the common latent factor and the \\(Q\\) values are the unique component to each item that is not explained by the common factor. Factor analysis solves this system of equations to find factor loadings and factor scores. The factor loadings are particularly important because they provide information about how strongly correlated the individual items are to the underlying factor and thus the validity of the assumption that they are all represented by a single common factor. The fa command in the pysch library will conduct a factor analysis. Just as for alpha, you can feed in raw data or just the correlation matrix. However, if you want the factor analysis to estimate factor scores, you need to feed in the raw data. Lets do a factor analysis of the three measures of neighborhood deprivation in the NYC data. factor_nyc &lt;- fa(nyc[,c(&quot;poverty.z&quot;,&quot;unemployed.z&quot;,&quot;lincome.z&quot;)], nfactors=1) loadings(factor_nyc) ## ## Loadings: ## MR1 ## poverty.z 0.926 ## unemployed.z 0.754 ## lincome.z 0.984 ## ## MR1 ## SS loadings 2.396 ## Proportion Var 0.799 The loadings function lets us extract the factor loadings. We can see that the factor loadings here are all very high, although the correlation for the unemployment rate is lower than the other two. We can also see that this single factor can account for nearly 80% of the variation across the three items. We can also display what this factor analysis looks like graphically with the fa.diagram function in the psych library. fa.diagram(factor_nyc) In this case its not terribly helpful, because we only had a single factor and the factor loadings were all high. When the factor loadings are below some threshold the lines are typically not drawn which can make this diagrams really useful for sorting out relationships with cases of multiple factors. Factor analysis with multiple common factors The example above assumed only one common factor, but we can also explore models that allow for more common factors. The only hard rule is that you have to have less common factors than items. To generalize the equations above, lets say we have a set of \\(J\\) observed variables that we want to divide between \\(m\\) common factors. We then have a set of \\(J\\) equations where the equation for the \\(j\\)th observed variable is: \\[z_{ji}=b_{j1}F_{1i}+b_{j2}F_{2i}+b_{jm}F_{mi}+u_jQ_{ji}\\] This means that we will have \\(m\\) factor loadings for each item. If our variables really do split into the expected number of common factors, then we should observe high factor loadings for each item on only one of the underlying common factors. The factor loadings that are estimated in factor analysis also depend on what is called a rotation. We won’t delve into the technical details of rotation here, but the basic idea is that there are actually an infinite number of ways to display the same set of factor loadings in \\(n\\)-dimensional space for a set of \\(n\\) common factors. So, we need to choose some way to “rotate” those loadings in n-dimensional space in order to understand them better. You can either rotate them orthogonally so that the common factors are uncorrelated or obliquely so that correlation between the common factors is allowed. The “varimax” rotation is the most popular orthogonal rotation because it chooses factor loadings that either maximize or minimize the loadings on certain variables, easing interpretation. The “oblimin” rotation is popular for the oblique case and is the default setting in the fa function. Personally, I don’t see a lot of practical value in assuming orthogonal factors, so I tend to prefer “oblimin.” To illustrate how this all works, lets look at another example. This example comes from Pew data collected on Muslim respondents in thirty-five different countries between 2008-2012. One set of questions in the survey asked respondents to rate the morality of a variety of stigmatized behaviors. Respondents could either select “morally acceptable”, “depends/not a moral value”, or “morally wrong.” Respondents were asked about the following behaviors: divorce polygamy fertility restriction alcohol euthanasia suicide abortion prostitution premarital sex homosexuality I would like to explore to what extent answers to these responses reflect an underlying common factor of “social conservativeness.” In order to do that I need to show that these responses fit well for a single common factor and that they fit better for a single common factor than for multiple factors. First, I need to scale my ordinal variables into quantitative variables with a score from 1 to 3. Since they are already ordered correctly, I can just use the as.numeric to cast them to numeric values. morality &lt;- cbind(moral_divorce=as.numeric(pew$moral_divorce), moral_fertility=as.numeric(pew$moral_fertility), moral_alcohol=as.numeric(pew$moral_alcohol), moral_euthansia=as.numeric(pew$moral_euthanasia), moral_suicide=as.numeric(pew$moral_suicide), moral_abortion=as.numeric(pew$moral_abortion), moral_prostitution=as.numeric(pew$moral_prostitution), moral_premar_sex=as.numeric(pew$moral_premar_sex), moral_gay=as.numeric(pew$moral_gay)) head(morality) ## moral_divorce moral_fertility moral_alcohol moral_euthansia ## [1,] 3 2 2 2 ## [2,] 3 1 2 2 ## [3,] 3 2 3 2 ## [4,] 3 2 3 3 ## [5,] 2 1 2 2 ## [6,] 3 1 2 2 ## moral_suicide moral_abortion moral_prostitution moral_premar_sex ## [1,] 3 3 1 3 ## [2,] 3 3 2 2 ## [3,] 2 2 3 2 ## [4,] 2 2 3 3 ## [5,] 2 3 3 3 ## [6,] 2 3 3 3 ## moral_gay ## [1,] 3 ## [2,] 2 ## [3,] 2 ## [4,] 3 ## [5,] 3 ## [6,] 3 Lets start with a look at the correlation matrix: library(corrgram) corrgram(morality, upper.panel=&quot;panel.cor&quot;, lower.panel=&quot;panel.shade&quot;) Figure 82: A correlogram showing the relationship between measures of morality in Pew data We can already see here that two of these behaviors (divorce and fertility) do not look like the others. Although they are somewhat weakly correlated with one another, they have very weak correlation with the other variables. Among the remaining variables, we do see a fairly substantial pattern of intercorrelation. I also want to examine Cronbach’s alpha, but before I do that I want to take advantage of the fact that Cronbach’s alpha and the factor analysis only requires the correlation matrix rather than the dataset itself. My data has missing values and this is one area where available-case analysis is probably preferable to complete-case analysis. For the correlation matrix, I can use all of the valid data for each pairwise comparison. That will lead to different sample sizes on each correlation coefficient, but will allow me to use the data to its fullest. I can do that in R by including the use=\"pairwise.complete.obs\" argument in cor: morality_r &lt;- cor(morality, use=&quot;pairwise.complete.obs&quot;) Lets look at Cronbach’s alpha before proceeding to full-blown factor analysis. alpha(morality_r) ## [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [24] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [47] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [70] NA NA NA NA NA NA NA NA NA NA NA NA A Cronbach’s alpha of 0.71 is decent, but the results also suggest that I would improve Cronbach’s alpha by removing either divorce or fertility from my set of measures. For the factor analysis, I will try out a one factor, two factor, and a three factor solution. morality_fa1 &lt;- fa(morality_r, 1) morality_fa2 &lt;- fa(morality_r, 2) morality_fa3 &lt;- fa(morality_r, 3) Lets compare the diagrams graphically: Figure 83: Three different factor solutions for the Pew morality data As you can see, for the single factor solution, the factor loadings were so low that divorce and fertility are dropped from the common factor. The two factor solution adds them back in as a separate factor, but even here the factor loadings of 0.4 are pretty low. The three factor solution splits my main group into two separate groups where one group seems to measure death in some way (suicide, euthanasia, and abortion) while the first group measures mostly sex with the addition of alcohol - suggesting alcohol use is viewed similarly to stigmatized sexual behavior. Although the division in the three factor solution is interesting, this level of detail does not necessarily seem warranted and leads to some low factor loadings. What seems clear above all else, is that the fertility limitation and divorce questions do not belong with the others. Therefore, a summated scale that included all variables except for these two is reasonably justified. To check this, lets see how Cronbach’s alpha does if we remove the fertility and divorce questions. alpha(cor(morality[,c(-1,-2)], use=&quot;pairwise.complete.obs&quot;)) ## [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [24] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [47] NA NA NA We now get a stronger alpha of around 0.76 and little evidence that more removals would improve the internal consistency. In closing, I should note that factor analysis is a huge area of research and I am only touching on the surface of it here. On the Canvas pages, I provide links for additional information. "],
["model-selection.html", "Model Selection", " Model Selection "],
["modeling-categorical-outcomes.html", "Modeling Categorical Outcomes", " Modeling Categorical Outcomes "],
["dichotomous-outcomes-and-the-binomial-distribution.html", "Dichotomous Outcomes and The Binomial Distribution", " Dichotomous Outcomes and The Binomial Distribution "],
["linear-probability-model.html", "Linear Probability Model", " Linear Probability Model "],
["generalized-linear-model.html", "Generalized Linear Model", " Generalized Linear Model "],
["maximum-likelihood-estimation.html", "Maximum Likelihood Estimation", " Maximum Likelihood Estimation "],
["logistic-regression-model.html", "Logistic Regression Model", " Logistic Regression Model "],
["models-for-nominal-polytomous-outcomes.html", "Models for Nominal Polytomous Outcomes", " Models for Nominal Polytomous Outcomes "],
["models-for-ordinal-polytomous-outcomes.html", "Models for Ordinal Polytomous Outcomes", " Models for Ordinal Polytomous Outcomes "],
["useful-references.html", "Useful References ", " Useful References "],
["example-datasets.html", "Example Datasets", " Example Datasets We will utilize several different datasets throughout this course to develop concepts and to provide examples. You should become familiar with these datasets. Below, I provide a brief description of each of the datasets we will use. If you are taking my undergraduate course, you will have access to these datasets through RStudio Cloud. Otherwise, you can download a ZIP file of these datasets here. You should unzip this folder to your own computer and place it somewhere you can easily access it (e.g. the desktop). You should use this folder to organize all of your work for the course.The datasets are in a *.RData format. You can simply click on them in your filesystem viewer to load them into R. Crimes The crimes data contain information on crime rates and demographic variables for all fifty US states and the District of Columbia. The crime rates are for the year 2010 and come from the FBI’s Uniform Crime Reports (UCR). The UCR is a program where local law enforcement agencies all report crime statistics to the FBI and these are aggregated into final crime statistics. For our purposes, we are dividing crimes into two main categories of violent and property crime. The demographic characteristics come from the American Community Survey (ACS) between the years 2008 and 2012. The ACS is an annual sample of the US population. To get a large enough sample in each state to calculate correct statistics (with little sampling error), I combine five years of data that are “centered”\" on 2010. Here is a full description of all variables in the dataset that we will use. Violent: violent crimes per 100,000 population within each state. This includes the crimes of murder, rape, robbery, and aggravated assault. By dividing the number of crimes by the population size, we avoid the problem of larger population states having more crimes because of a larger population. This is often called the crime “rate.” Property: property crimes per 100,000 population. This includes the crimes of burglary, larceny, and motor vehicle theft.MedianAge: Median age of a state’s population. PctMale: Percent of a state population that is male. PctLessHS: The percent of the state population over the age of 25 without a high school diploma. MedianIncomeHH: Median household income in a state. This is measured in thousands of dollars (i.e. 35 means $35,000). We are taking the income of each household (meaning all members of that household combined) rather than individual level income. For most purposes, this is thought to be a better measure because consumption and savings are typically organized at the household level. Unemployment: Unemployment rate in the state. The unemployment “rate” is really just a percentage. Its the percentage of individuals who are not working but want to work among all those in the labor force (those who are working or looking for work). Poverty: Poverty rate in the state. The poverty “rate” is also really just a percentage. It is the percent of individuals living below the poverty line. The poverty line is a number developed by the federal government. It was originally developed in the 1960s and is adjusted for inflation every year. Many people critique the poverty line as being too low because it has not kept pace with increases in the consumer price index. Gini: A measure of income inequality in the state. The gini coefficient is a widely used measure of how unequally income is distributed. If gini is zero, then everyone has exactly the same income. If gini is 100, then one person makes all the money and everyone else zero. The higher the gini coefficient, the more income inequality exists. Movies The movie data contain information about 2,612 movies produced between 2001 and 2013. The data come from the Open Movie Database, which itself contains data from the Internet Movie Database and Rotten Tomatoes. To simplify our analyses, I have limited the analysis to movies that played in the US and received 10 or more reviews. I have also excluded all shorts, documentaries, foreign language films, and movies that received an NC-17 rating or were unrated. Here are the variables we have for each movie: Year: The calendar year of the film’s release. Rating: The movie’s maturity rating (G, PG, PG-13, R). Runtime: The length of the movie in minutes. Oscars: The number of Oscar awards that the movie received. This includes Oscars that go to individual actors (leading and supporting), as well as more general awards (best screenplay, editing, cinematography, etc.), and best picture overall. TomatoMeter: The tomato meter is the percent of reviews that are judged to be positive by Rotten Tomatoes staff. This metric goes from 0 to 100 percent. Note that movies are spread out pretty evenly across the range of this variable, something we call a “uniform” distribution. Note, that this method makes no distinction between how positive a positive review was or how negative a negative review was, so its perfectly possible for two movies with the same Tomato Meter to be viewed very differently by reviewers. TomatoRating: The tomato rating is a combination of all reviews where the review used some kind of numeric rating (e.g. 3 out of 4 stars, 7 out of 10). Rotten Tomatoes “normalizes” these scores so that they are all recorded on the same basis. The scale of this normalized score goes from 1 to 10. Unlike the Tomato Meter, this scale should be capable of distinguishing how strongly positive or negative the review was. BoxOffice: The box office returns for the movie in millions of US dollars. Genre: The genre of the film. This is a tricky variable to create. In actuality, movies could be listed as multiple genres in the original dataset, with twenty different genres to choose from. For example, “No Country for Old Men” is listed in the genres of crime, drama, and thriller while “Lord of the Rings: Return of the King” is listed as action, adventure, and fantasy. This is probably the best way to treat genres, but for our purposes it adds a lot of complexity. Therefore, I have recoded movies into a single “best” genre based on a decision rule where certain genres trump all others on an ordered basis. For example, comedy trumps romance, so romantic comedies will always show up in this dataset as comedies. The ordering of this system is Animation &gt; Family &gt; Musical &gt; Horror &gt; SciFi/Fantasy &gt; Comedy &gt; Romance &gt; Action &gt; Thriller &gt; Mystery &gt; Drama &gt; All Others. For the most part, this system works well, but you may notice some odd discrepancies for a few movies. Politics This data comes from the 2016 American National Election Study (ANES). The ANES is a survey of the American electorate that is conducted every two years. The study collects information on a variety of political attitudes and voting behaviors. For our purposes, we are going to primarily look at respondent’s vote for president and attitudes on three issues: (1) birthright citizenship, (2) gay marriage, and (3) global warming. The variables we will look at are: brcitizen: Respondents were asked whether they would support a proposal to change the US Constitution to remove birthright citizenship (citizenship automatically granted to individuals born in the US regardless of their parent’s citizenship status). Respondents could either favor, oppose, or neither favor or oppose. gaymarriage: Respondents were asked for their position on gay marriage and were given the choices of “no legal recognition”, “civil union (but no marriage)”, “support gay marriage.” globalwarm: A question on whether the respondent believes that anthropogenic global warming is happening. I constructed this variable from two separate questions. The first question asks whether respondents think that global warming has been happening with the options being that it “probably has” or “probably has not.” The second question asks whether respondents thought that global warming was caused by human activity (either entirely or partially). I combine these into a single dichotomous variable where individuals either think the earth is warming from human activity or that it is not warming from human activity, where the latter category includes people who think it isn’t warming at all and people who think it is warming but not because of human activity. party: The political party with which the respondent identifies. This does not necessarily mean that a respondent is officially registered with a given party. relig: The respondent’s religion. This category is based on the combination of people’s statement about the kind of services they typically attend along with several non-exclusive yes/no questions about their religion (e.g. evangelical, Pentecostal, agnostic, atheist). age: The age of the respondent. gender: The respondent’s self-reported gender, recorded as “Male”,“Female”, or “Other.” race: the racial identification of the respondent. Respondents could write in multiple races, but to keep it simple, we will combine the small number of individuals who reported multiple races with those who listed “Other” as their race. educ: The education of the respondent. This is recorded as an ordinal variable. The “Some college” response indicates individuals who have attended college (including 2-year programs) but have not earned a BA. income: The family income of the respondent in 1000s of dollars. Respondents did not give actual dollar amounts here but rather indicated which bracket of income (e.g. $20,000-30,000) they fell within. For the purposes of our class, I randomly select an actual value within this bracket for each respondent. workstatus: The work status of the respondent. Respondents could either be working, unemployed, or out of the labor force. The last category refers to people who are not employed and not currently looking for work, whereas unemployed indicates a person who is not employed an is currently looking for work. military: Whether the respondent has ever served or is currently serving in the US military. Popularity This data comes from the National Longitudinal Study of Adolescent to Adult Health (Add Health), conducted by the Carolina Population Center at UNC-Chapel Hill and supported by a grant from the National Institute of Child Health and Human Development. The first wave of the study which we are using surveyed adolescents between 7th and 12th grade in school in the 1994-95 school year. One of the particularly valuable features of the Add Health survey is that many respondents were in the “saturation sample” which sampled all students at 16 schools. In this saturation sample, students were asked about who were their friends and sexual partners, which allows researchers to construct network maps of adolescent social systems. We will use this saturation sample to look at a various basic measure of that network that estimates students’ popularity. This measure, which is called “in degree” in the network analysis literature, measures the number of times a student was nominated as a friend by other students in the school. We will treat it as a simple proxy measure of a student’s popularity. We can then look at what other student characteristics were positively or negatively associated with a student’s popularity. Here is a full description of all variables in the dataset that we will use. indegree: The number of friend nominations received by other students at the same school. This is the measure of popularity that we will use. race: A six-category nominal variable indicating the race that the student best thought described them when asked to choose a single race: white, black, Latino, Asian, American Indian, other. sex: Add Health reports this as a student’s “biological” sex. Students were only reported as male or female.grade: current grade of the student as a quantitative variable. psuedoGPA: Students were asked for the most recent letter grade in four course types: math, language arts, science, and math. This variable was constructed by calculating GPA from those responses. honorsociety: A true/false variable for whether a student was in honor society or not. alcoholuse: A true/false variable that is true if the student reported drinking at least once or twice a month in the last twelve months. smoker: A true/false variable that is true if student smoked more than 5 cigarettes in the past 30 days. bandchoir: a true/false variable that is true if the student was in band or choir. academicclub: a true/false variable that was true if the student was in an academically-oriented club such as math club, book club, etc. nsports: The number of different school sports a student reported participating in. Students who reported more than six sports were top-coded at the value of six. parentinc: Parent’s household income measured in $1000’s of dollars. Sex The sex data come from a special supplemental questionnaire that was added to the General Social Survey (GSS) in 2004. The GSS is a survey of attitudes that is conducted every two years by the National Opinion Research Council (NORC). In the 2004 supplement, respondents were asked questions about their sexual behavior. We will be looking specifically at respondents reported frequency of sexual activity and its relationship to demographic characteristics such as age, education, and marital status. Here are the variables we will look at: sexf: A quantitative variable indicating the frequency of sexual activity as the number of sexual encounters per year. The sequal frequency response was coded as an ordinal scale variable in which respondents were given a set of options from less to more sexual activity in the previous year. For our purposes, I have recoded the ordinal sexfreq variable into a quantitative variable by giving everyone the midpoint number of sexual acts per year based upon their answer. For example, individuals who said (2 or 3 times a month) were given a value of \\(2.5*12=30\\). This will allow us to use sexual frequency as a dependent variable in regression models. age: The age of the respondent. The GSS only surveys adults aged 18 years and older. gender: The gender of the respondent. educ: Years of education for the respondent. marital: Marital status of the respondent: Never married, married, divorced, widowed, separated. relig: Religious affiliation of the respondent. Protestants have been divided into “mainline” and “fundamentalist” based on a coding of specific denominations used by the GSS. Titanic The titanic data contain information on all 1,309 passengers aboard the Titanic. The data do not include information about the crew. The data primarily come from the online database, Encyclopedia Titanica. Here are the variables we will look at: survival: Did the passenger survive? sex: The reported sex of the passenger. age: The age of the passenger. This variable is reported in whole numbers for those over one year old and as a decimal (based on months of age) for infants under a year of age. agegroup: A categorical variable indicating whether the person was an adult or a child. I have constructed this variable from the age variable. The cutoff for adults is sixteen years of age. pclass: There were three passenger classes: First, second, and third (also known as steerage). To give some pop culture references, Rose was first class, and Jack was third class. Most of the passengers were in third class. fare: The fare paid for the ticket, measured in British pounds. family: The number of family members traveling with the passenger. These family members can either be parents, spouses, siblings, or children. Wages This data has information on the hourly wages of US workers in 2018. The data here are extracted from Current Population Survey data via IPUMS. I used the earning data from the outgoing rotation groups (ORG) for each month of the CPS. Each household in the CPS is is part of a rolling panel in which they are in for four months, out for eight months, and back in for four months. In the fourth and eight month of inclusion they are given additional questions as part of the outgoing rotation group. The hourly wage of salaried workers is assessed by a question on hours worked in a typical week and earnings in the prior week. I limited the data only to those individuals between the ages of 18 and 65 in order to capture the age range of the typical worker. The dataset contains the following variables: wages: The hourly wage for the respondent. For workers who report being paid hourly, this value is based on a direct question that asked for respondents’ hourly wages. For individuals in salaried positions, this value was derived by dividing the earnings from the previous week by the hours worked in the previous week. Anyone who reported a wage of less than one dollar is removed. Any wage higher than $99.99 is top-coded as $99.99. age: age of the respondent in years. gender: Male or Female. race: The respondent’s racial identification recoded from two separate questions on race and hispanicity into the following categories: White, Black, Latino, Asian, Indigenous, and Other/Multiple races. The indigenous category includes American Indians, Pacific Islanders, and Alaska Natives. marstat: The respondent’s current marital status: never married, married, divorced or separated, and widowed. education: The respondent’s highest educational attainment: no high school diploma, high school diploma, associate’s degree, bachelor’s degree, graduate degree. The last category includes master’s degrees, professional degrees, and doctoral degrees. occup: The broad occupational category of the respondent. In the actual CPS data, there are hundreds of different occupations listed. For our purposes, I have simplified this into a broader (and smaller) set of occupational categories that we will use for the analysis. Here are the categories of the occupational variable, along with some examples of specific occupations: Managers: Human resources Managers, Operations Managers Business/Finance Specialist: Claims Adjusters, Compliance Officers, Accountants, Tax Preparers STEM: Computer Programmers, Civil Engineers, Biological Scientists Doctors: Dentists, Surgeons, Optometrists Legal: Lawyers, Judges, Paralegals Education: Preschool and Kindergarten Teachers, Librarians Arts, Design, and Media: Artists, Dancers and Choreographers, Writers and Authors Other Healthcare: Registered Nurses, Physical Therapists, Dental Hygienists Social Services: Clergy, Social Workers Service: Waiters and Waitresses, Barbers, Bartenders Sales: Cashier, Telemarketer Administrative Support: Bank Tellers, Data Entry Keyers, Receptionist Manual: Carpenters, Logging Workers, Mining Machine Operators, Small Engine Mechanic nchild: Number of own children living in the household with the respondent. foreign_born: A variable indicating whether the respondent is foreign born or not. Recorded as “Yes” or “No”. earn_type: This variable indicates whether the respondent reported being paid hourly wages or by salary. earningwt: A technical weighting variable for use with any CPS analysis of earnings. "],
["common-r-commands.html", "Common R Commands", " Common R Commands Below is a list of common commands that we use for the undergraduate class, along with some examples. You can view a help file in RStudio for each command by searching for the command name in the help tab in the lower right panel. You can also just type the name of the command preceded by a “?” into the console. For example, if you wanted to understand how barplot works, type: ?barplot The list here does not contain information about making plots in R. That information is in the Plotting Cookbook appendix. Univariate Statistics mean Calculate the mean of a quantitative variable. Remember that this command will not work for categorical variables. mean(earnings$wages) ## [1] 24.27601 median Calculate the median of a quantitative variable. Remember that this command will not work for categorical variables. median(earnings$wages) ## [1] 19.21667 sd Calculate the standard deviation of a quantitative variable. Remember that this command will not work for categorical variables. sd(earnings$wages) ## [1] 16.23676 IQR Calculate the interquartile range of a quantitative variable. Remember that this command will not work for categorical variables. IQR(earnings$wages) ## [1] 17 quantile Calculate percentiles of a distribution. Remember that this command will not work for categorical variables. By default, the quantile command will return the quartiles (0,25,50,75,100 percentiles). If you want different percentiles, you will have to specify the probs argument. quantile(earnings$wages) ## 0% 25% 50% 75% 100% ## 1.00000 13.00000 19.21667 30.00000 99.99000 #get the 10th and 90th percentile instead quantile(earnings$wages, probs = c(0.1,0.9)) ## 10% 90% ## 10.000 47.596 table Calculate the absolute frequencies of the categories a categorical variable. table(movies$Genre) ## ## Action Animation Comedy Drama Family ## 207 139 781 332 155 ## Horror Musical/Music Mystery Romance SciFi/Fantasy ## 220 103 39 138 258 ## Thriller ## 181 prop.table Calculate the proportions (i.e. relative frequencies) of the categories of a categorical variable. This command must be run on the output from a table command. You can do that in one command by nesting the table command inside the prop.table command. prop.table(table(movies$Genre)) ## ## Action Animation Comedy Drama Family ## 0.08108108 0.05444575 0.30591461 0.13004309 0.06071289 ## Horror Musical/Music Mystery Romance SciFi/Fantasy ## 0.08617313 0.04034469 0.01527615 0.05405405 0.10105758 ## Thriller ## 0.07089698 summary Provide a summary of a variable, either categorical or quantitative. summary(earnings$wages) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 13.00 19.22 24.28 30.00 99.99 summary(movies$Genre) ## Action Animation Comedy Drama Family ## 207 139 781 332 155 ## Horror Musical/Music Mystery Romance SciFi/Fantasy ## 220 103 39 138 258 ## Thriller ## 181 Bivariate Statistics table Can be used to create a two-way table, although further work needs to be done to extract useful information from the two-way table. table(movies$Genre, movies$Rating) ## ## G PG PG-13 R ## Action 0 3 98 106 ## Animation 37 92 6 4 ## Comedy 1 71 352 357 ## Drama 2 36 116 178 ## Family 16 131 8 0 ## Horror 0 1 60 159 ## Musical/Music 0 7 57 39 ## Mystery 0 0 7 32 ## Romance 0 12 64 62 ## SciFi/Fantasy 0 10 186 62 ## Thriller 0 0 37 144 prop.table Calculate the conditional distributions from a two-way table. The first argument here must be a two-way table output from the table command. It is very important that you also add a second argument that indicated the way you want the conditional distributions. 1 will give you distributions conditional on the row variable and 2 will give you distributions conditional on the column variable. ## ## G PG PG-13 R ## Action 0.000000000 0.014492754 0.473429952 0.512077295 ## Animation 0.266187050 0.661870504 0.043165468 0.028776978 ## Comedy 0.001280410 0.090909091 0.450704225 0.457106274 ## Drama 0.006024096 0.108433735 0.349397590 0.536144578 ## Family 0.103225806 0.845161290 0.051612903 0.000000000 ## Horror 0.000000000 0.004545455 0.272727273 0.722727273 ## Musical/Music 0.000000000 0.067961165 0.553398058 0.378640777 ## Mystery 0.000000000 0.000000000 0.179487179 0.820512821 ## Romance 0.000000000 0.086956522 0.463768116 0.449275362 ## SciFi/Fantasy 0.000000000 0.038759690 0.720930233 0.240310078 ## Thriller 0.000000000 0.000000000 0.204419890 0.795580110 tapply Calculate a statistic (e.g. mean, median, sd, IQR) for a quantitative variable across the categories of a categorical variable. The first argument should be the quantitative variable. The second argument should be the categorical variable. The third argument should be the name of the command that will calculate the desired statistic. tapply(movies$Runtime, movies$Rating, mean) ## G PG PG-13 R ## 90.80357 99.71901 108.02321 105.25547 tapply(movies$Runtime, movies$Rating, median) ## G PG PG-13 R ## 90 96 105 102 tapply(movies$Runtime, movies$Rating, sd) ## G PG PG-13 R ## 14.63796 13.95487 17.58490 16.07108 cor Calculate the correlation coefficient between two quantitative variables. cor(crimes$Violent, crimes$Gini) ## [1] 0.5454737 Statistical Inference nrow Return the number of observations in a dataset. nrow(crimes) ## [1] 51 qt Calculate the t-value needed for a confidence interval. For a 95% confidence interval, the first argument should always be 0.975. The second argument should be the appropriate degrees of freedom for the statistic and dataset. qt(0.975, nrow(politics)-1) ## [1] 1.960524 pt Calculate the p-value for a hypothesis test. The first argument should always be the negative version of the t-statistic and the second argument should be the appropriate degrees of freedom for the statistic and dataset. 2*pt(-2.1, nrow(politics)-1) ## [1] 0.03578782 OLS Regression Models lm Run an OLS regression model. The first argument should always be a formula of the form dependent~independent1+independent2+.... To simplify the writing of variable names, it is often useful to specify a second argument data that identifies that dataset being used. Then you don’t have to include dataset_name$ in the formula. **Remember to always put the dependent (y) variable on the left hand side of the equation. #simple model with one independent variable model_simple &lt;- lm(wages~age, data=earnings) #same simple model but recenter age on 45 years of age model_recenter &lt;- lm(wages~I(age-45), data=earnings) #a model with multiple independent variables, both quantitative and qualitative model_multiple &lt;- lm(wages~I(age-45)+education+race+gender+nchild, data=earnings) #a model like the previous but also with interaction between gender and nchild model_interaction &lt;- lm(wages~I(age-45)+education+race+gender*nchild, data=earnings) Once a model object is created, information can be extracted with either the coef command which just reports the slopes and intercept, or a full summary command which gives more information. coef(model_interaction) ## (Intercept) I(age - 45) ## 17.3568021 0.2242916 ## educationHS Diploma educationAA Degree ## 4.5382688 7.4288321 ## educationBachelors Degree educationGraduate Degree ## 16.2657784 23.0187910 ## raceBlack raceLatino ## -3.4176245 -2.1133582 ## raceAsian raceIndigenous ## 0.5641751 -1.5198248 ## raceOther/Multiple genderFemale ## -0.4331997 -4.3777137 ## nchild genderFemale:nchild ## 1.2629571 -0.7490706 summary(model_interaction) ## ## Call: ## lm(formula = wages ~ I(age - 45) + education + race + gender * ## nchild, data = earnings) ## ## Residuals: ## Min 1Q Median 3Q Max ## -43.638 -7.779 -2.198 4.568 90.578 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 17.356802 0.159721 108.669 &lt; 2e-16 *** ## I(age - 45) 0.224292 0.002858 78.471 &lt; 2e-16 *** ## educationHS Diploma 4.538269 0.154451 29.383 &lt; 2e-16 *** ## educationAA Degree 7.428832 0.181143 41.011 &lt; 2e-16 *** ## educationBachelors Degree 16.265778 0.164396 98.943 &lt; 2e-16 *** ## educationGraduate Degree 23.018791 0.178161 129.202 &lt; 2e-16 *** ## raceBlack -3.417625 0.123798 -27.607 &lt; 2e-16 *** ## raceLatino -2.113358 0.109491 -19.302 &lt; 2e-16 *** ## raceAsian 0.564175 0.157602 3.580 0.000344 *** ## raceIndigenous -1.519825 0.321284 -4.730 2.24e-06 *** ## raceOther/Multiple -0.433200 0.303134 -1.429 0.152987 ## genderFemale -4.377714 0.090203 -48.532 &lt; 2e-16 *** ## nchild 1.262957 0.043476 29.049 &lt; 2e-16 *** ## genderFemale:nchild -0.749071 0.063268 -11.840 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.74 on 145633 degrees of freedom ## Multiple R-squared: 0.284, Adjusted R-squared: 0.2839 ## F-statistic: 4443 on 13 and 145633 DF, p-value: &lt; 2.2e-16 Utility functions round Used for rounding the results of numbers to a given number of decimal places. By default, it will round to whole numbers, but you can specify the number of decimal places in the second argument. 100*round(prop.table(table(movies$Genre)), 2) ## ## Action Animation Comedy Drama Family ## 8 5 31 13 6 ## Horror Musical/Music Mystery Romance SciFi/Fantasy ## 9 4 2 5 10 ## Thriller ## 7 sort Sort a vector of numbers from smallest to largest (default), or largest to smallest (with additional argument decreasing=TRUE). sort(100*round(prop.table(table(movies$Genre)), 2), decreasing = TRUE) ## ## Comedy Drama SciFi/Fantasy Horror Action ## 31 13 10 9 8 ## Thriller Family Animation Romance Musical/Music ## 7 6 5 5 4 ## Mystery ## 2 sort(100*round(prop.table(table(movies$Genre)), 2)) ## ## Mystery Musical/Music Animation Romance Family ## 2 4 5 5 6 ## Thriller Action Horror SciFi/Fantasy Drama ## 7 8 9 10 13 ## Comedy ## 31 "],
["plotting-cookbook.html", "Plotting Cookbook", " Plotting Cookbook This appendix will provide ggplot example R code and output for of all the graphs that we might use this term. For further information, I highly recommend Kieran Healy’s Data Visualization book and Hadley Wikham’s ggplot2 book. All the examples provided will use the standard example datasets that we have been working with throughout the term. Barplots Oddly, barplots are one of the trickiest graphs in ggplotif you want proportions rather than absolute frequencies for each category. To make proportions, I need to specify y=..prop.. in the aesthetics, and then use group=1 to ensure that all categories are part of a single group and thus have proportions that sum up to one. In the code below, I am also using the scale_y_continuous command and the scales::percent command to label the tickmarks on the y-axis as percents rather than proportions. ggplot(titanic, aes(x=pclass, y=..prop.., group=1))+ geom_bar()+ scale_y_continuous(labels=scales::percent)+ labs(x=&quot;passenger class&quot;, y=NULL, title=&quot;Distribution of passenger class on Titanic&quot;)+ theme_bw() Figure 84: A barplot The result of this code is shown in Figure 84. Because the tickmark labels are self-explanatory I use a value of NULL for the y-axis label. Histograms Histograms are a snap in ggplot. The binwidth argument will allow you to easily resize bin widths without having to provide every break as base plot does. ggplot(movies, aes(x=Runtime))+ geom_histogram(binwidth = 5, col=&quot;black&quot;)+ labs(x=&quot;movie runtime (minutes)&quot;, title=&quot;Histogram of movie runtime&quot;)+ theme_bw() Figure 85: A basic histogram Figure 85 shows the basic histogram from the code above. By default, ggplot will not place a border around bars, but I like to make the border a slightly different color (e.g. col=\"black\") to provide better visual definition. You can also calculate density instead of count, if you prefer, by adding the y=..density.. option to the aesthetics. Density is the proportion of cases in a bin, divided by bin width. If you plot a histogram with density, you can also overlay this figure with a smoothed line approximating the distribution called a kernel density. These results are shown in Figure 86. ggplot(movies, aes(x=Runtime, y=..density..))+ geom_histogram(binwidth = 5, col=&quot;black&quot;)+ geom_density(col=&quot;red&quot;, fill=&quot;grey&quot;, alpha=0.5)+ labs(x=&quot;movie runtime (minutes)&quot;, title=&quot;Histogram of movie runtime&quot;)+ theme_bw() Figure 86: A histogram with kernel density smoother Boxplots The only tricky thing to a boxplot in ggplot is to remember that the aesthetic is y not x. Its not required, but setting x=\"\" in the aesthetics and then setting the x label to NULL will also create a cleaner display with no x-axis. I also apply a geom_boxplot only argument here that colors outliers a different color. ggplot(movies, aes(x=&quot;&quot;, y=Runtime))+ geom_boxplot(fill=&quot;grey&quot;, outlier.color = &quot;red&quot;)+ labs(y=&quot;movie runtime (minutes)&quot;, x=NULL, title=&quot;Boxplot of movie runtime&quot;)+ theme_bw() Figure 87: A basic boxplot You can also try a geom_violin instead which basically gives you a mirrored kernel density plot. For a single variable, use x=1 because geom_violin requires an x aesthetic. Below, I show how you can extend this into a comparative violin plot. ggplot(movies, aes(x=1, y=Runtime))+ geom_violin(fill=&quot;grey&quot;)+ scale_x_continuous(labels=NULL)+ labs(x=NULL, y=&quot;movie runtime (minutes)&quot;, title=&quot;Violin plot of movie runtime&quot;)+ theme_bw() Figure 88: A violin plot Comparative Barplots Comparative barplots are definitely one of the most difficult cases to graph properly in ggplot because grouping causes all kinds of issues with calculating proportions correctly. The best approach that I have found is to use faceting to separate out the barplots of one variable by the categories of the other variable. The basic code is then identical to that for a single barplot above with the addition of a facet_wrap command to identify the second categorical variable to use for faceting. ggplot(titanic, aes(x=survival, y=..prop.., group=1))+ geom_bar()+ facet_wrap(~pclass)+ scale_y_continuous(labels = scales::percent)+ labs(x=NULL, y=NULL, title=&quot;Survival on the Titanic by class&quot;)+ theme_bw() Figure 89: A comparative barplot made by faceting Figure 89 shows the result. In this case, since survival and death are mirror images, we could convey the same information, but with much less ink by a simple dotplot of the percent surviving by passenger class. However, this would require some preparatory work to prepare the data in the way we want to display it. For the undergraduate class, you are not required to understand how to prepare this data. tab &lt;- prop.table(table(titanic$survival, titanic$pclass),2) tab &lt;- subset(as.data.frame.table(tab), Var1==&quot;Survived&quot;, select=c(&quot;Var2&quot;,&quot;Freq&quot;)) colnames(tab) &lt;- c(&quot;pclass&quot;,&quot;prop_surv&quot;) ggplot(tab, aes(x=pclass, y=prop_surv))+ geom_point()+ scale_y_continuous(labels=scales::percent, limits=c(0,0.8))+ labs(x=NULL, y=NULL, title=&quot;Percent surviving Titanic by class&quot;)+ coord_flip()+ theme_bw() Figure 90: A simple dotplot Figure 90 shows the resulting dotplot. If that is a little too spartan for you, you could try a lollipop graph, shown in Figure 91. The geom_lollipop is in the ggalt library. ggplot(tab, aes(x=pclass, y=prop_surv))+ geom_lollipop()+ scale_y_continuous(labels=scales::percent, limits=c(0,0.7))+ labs(x=NULL, y=NULL, title=&quot;Percent surviving Titanic by class&quot;)+ coord_flip()+ theme_bw() Figure 91: A lollipop plot Comparative Boxplots To construct comparative boxplots, we just need to add an x to the aesthetic of a boxplot. ggplot(movies, aes(x=Genre, y=Runtime))+ geom_boxplot(fill=&quot;grey&quot;, outlier.color = &quot;red&quot;)+ labs(y=&quot;movie runtime (minutes)&quot;, title=&quot;Boxplot of movie runtime by genre&quot;)+ theme_bw() Figure 92: A basic comparative boxplot The mediocre results are shown in Figure 92. The labels on the x-axis often run into one another, so coord_flip is a good option to avoid that. In addition, I can also use the varwidth argument in geom_boxplot to scale the width of boxplots relative to the number of observations. ggplot(movies, aes(x=Genre, y=Runtime))+ geom_boxplot(fill=&quot;grey&quot;, outlier.color = &quot;red&quot;, varwidth=TRUE)+ labs(y=&quot;movie runtime (minutes)&quot;, title=&quot;Boxplot of movie runtime by genre&quot;)+ coord_flip()+ theme_bw() Figure 93: Flipping coordinates for better display My improved comparative boxplot is shown in Figure 93. However, Its also often useful to re-order the categories by the mean or median of the quantitative variable. We can do this with the reorder command: ggplot(movies, aes(x=reorder(Genre, Runtime, median), y=Runtime))+ geom_boxplot(fill=&quot;grey&quot;, outlier.color = &quot;red&quot;, varwidth=TRUE)+ labs(y=&quot;movie runtime (minutes)&quot;, x=NULL, title=&quot;Boxplot of movie runtime by genre&quot;)+ coord_flip()+ theme_bw() Figure 94: Re-order categories by median The results are shown in Figure 94. In this case, we used the reorder command to reorder the categories of Genre by the median value of Runtime. We could also try a comparative violin plot as shown in Figure 95. ggplot(movies, aes(x=reorder(Genre, Runtime, median), y=Runtime))+ geom_violin(fill=&quot;grey&quot;)+ labs(y=&quot;movie runtime (minutes)&quot;, x=NULL, title=&quot;Boxplot of movie runtime by genre&quot;)+ coord_flip()+ theme_bw() Figure 95: A comparative violin plot Scatterplots The geom_point command gives us the the geom we want for scatterplots. We can also use geom_smooth to graph a variety of lines to our data. Setting an alpha value can help with overplotting as well by providing semi-transparency. ggplot(politics, aes(x=age, y=income))+ geom_point(alpha=0.5)+ scale_y_continuous(labels=scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;family income (in thousands USD)&quot;, title=&quot;Scatterplot between age and family income&quot;, caption=&quot;ANES 2016&quot;)+ theme_bw() Figure 96: Basic Scatterplot The results are shown in Figure 96. As more points are plotted in the same position, the overall plot darkens. However, there is still a lot of overplotting because age is only recorded in whole numbers. In cases like this, we can use geom_jitter rather than geom_point to randomly perturb each value pair a little bit, as shown in Figure 14. ggplot(politics, aes(x=age, y=income))+ geom_jitter(alpha=0.5)+ scale_y_continuous(labels=scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;family income (in thousands USD)&quot;, title=&quot;Scatterplot between age and family income&quot;, caption=&quot;ANES 2016&quot;)+ theme_bw() Figure 97: Jittering points to deal with overplotting It is often useful to apply some model to the data to draw a best-fitting line through the points. The geom_smooth function will do this for us and we can use the method argument to specify what kind of model we want to fit. You an apply non-parametric smoothers like LOESS as well as an OLS regression line. Lets first try a LOESS smoother. ggplot(politics, aes(x=age, y=income))+ geom_jitter(alpha=0.5)+ geom_smooth(method=&quot;loess&quot;)+ scale_y_continuous(labels=scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;family income (in thousands USD)&quot;, title=&quot;Scatterplot between age and family income&quot;, caption=&quot;ANES 2016&quot;)+ theme_bw() Figure 98: Plotting loess smoother to data Figure 98 shows us the smoothed line. It looks like an inverted U-shaped distribution. Family income is also highly right skewed. Lets try scale_y_log10 to put income on a log-scale (advanced). ggplot(politics, aes(x=age, y=income))+ geom_jitter(alpha=0.5)+ geom_smooth(method=&quot;loess&quot;)+ scale_y_log10(labels=scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;family income (in thousands USD)&quot;, title=&quot;Scatterplot between age and family income&quot;, caption=&quot;ANES 2016&quot;)+ theme_bw() Figure 99: Changing scale of y-axis to logarithmic Figure 99 shows the scatterplot with an exponential scale for the y-axis. Note how the values go from $1K to $10K to $100K at even intervals. Now lets try switch the smoothing to a linear model, by simply changing the method in geom_smooth: ggplot(politics, aes(x=age, y=income))+ geom_jitter(alpha=0.5)+ geom_smooth(method=&quot;lm&quot;)+ scale_y_log10(labels=scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;family income (in thousands USD)&quot;, title=&quot;Scatterplot between age and family income&quot;, caption=&quot;ANES 2016&quot;)+ theme_bw() Figure 100: Fitting an OLS regression line Figure 100 shows the OLS line predicting income. Its pretty flat which to some extent hides the underlying curvilinear relationship. In practice, I would probably want to apply a quadratic term on age to any regression model. Finally, we can add add in an aesthetic for a third categorical variable by applying color to the plots. In this case, I will color the plots by highest degree received. ggplot(politics, aes(x=age, y=income))+ geom_jitter(alpha=0.5, aes(color=educ))+ geom_smooth(method=&quot;lm&quot;)+ scale_color_brewer(palette = &quot;YlOrRd&quot;)+ scale_y_log10(labels=scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;family income (in thousands USD)&quot;, title=&quot;Scatterplot between age and family income&quot;, caption=&quot;ANES 2016&quot;, color=&quot;Highest degree&quot;)+ theme_bw() Figure 101: Adding color aesthetic Figure 101 shows the result. Because I defined the color aesthetic in geom_jitter rather than the base ggplot command, I still only get one smoothed OLS regression line for the whole dataset. If I wanted separate lines by educational level, I could add the color aesthetic to ggplot instead. ggplot(politics, aes(x=age, y=income, color=educ))+ geom_jitter(alpha=0.5)+ geom_smooth(method=&quot;lm&quot;)+ scale_color_brewer(palette = &quot;YlOrRd&quot;)+ scale_y_log10(labels=scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;family income (in thousands USD)&quot;, title=&quot;Scatterplot between age and family income&quot;, caption=&quot;ANES 2016&quot;, color=&quot;Highest degree&quot;)+ theme_bw() Figure 102: Separate OLS lines by color aesthetic Figure 102 shows the same plot but now with separate lines for each educational level. We are basically plotting interaction terms here between age and educational level. Instead of coloration, I could have used faceting to try to separate out differences by education. Figure 103 shows the results using this approach. For this graph, I use two separate geom_smooth commands to get the LOESS and linear fit for each panel. ggplot(politics, aes(x=age, y=income))+ geom_jitter(alpha=0.3)+ geom_smooth(method=&quot;lm&quot;)+ geom_smooth(method=&quot;loess&quot;, color=&quot;red&quot;)+ facet_wrap(~educ)+ scale_y_log10(labels=scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;family income (in thousands USD)&quot;, title=&quot;Scatterplot between age and family income&quot;, caption=&quot;ANES 2016&quot;)+ theme_bw() Figure 103: facet by education instead of color If you have a small number of cases, it might make sense to label your values. You can label values with geom_text but you can often run into problems with labels being truncated or overlapping. ggplot(crimes, aes(x=Unemployment, y=Property))+ geom_point(alpha=0.7)+ geom_smooth(method=&quot;lm&quot;, se=FALSE)+ geom_text(aes(label=State), size=2)+ labs(x=&quot;Unemployment rate&quot;, y=&quot;Property crimes per 100,000 population&quot;, title=&quot;Scatterplot of unemployment and property crime&quot;)+ theme_bw() Figure 104: labeling points can look really bad Figure 104 shows labeling with the default geom_text approach. You can fiddle with a variety of arguments here that let you offset the labels but it rarely produces clean labels. The library ggrepel has a function geom_text_repel that will work harder to place labels in a readable position: ggplot(crimes, aes(x=Unemployment, y=Property))+ geom_point(alpha=0.7)+ geom_smooth(method=&quot;lm&quot;, se=FALSE)+ geom_text_repel(aes(label=State), size=2)+ labs(x=&quot;Unemployment rate&quot;, y=&quot;Property crimes per 100,000 population&quot;, title=&quot;Scatterplot of unemployment rate and property crime rate&quot;)+ theme_bw() Figure 105: repel works better for labeling points, but still can get too busy "],
["r-stat-lab.html", "R Stat Lab ", " R Stat Lab "],
["using-scripts.html", "Using Scripts", " Using Scripts One of the primary advantages of using statistical software like R or Stata is the ability to perform your analysis in a script. A script is a text file of commands that can be run to reproduce a data cleaning exercise, an analysis, a simulation, etc. It is a good practice to always do your statistical work using scripts. The advantage of scripting is that you will have an easily reproducible record of exactly what you did. Lets say that I did a thorough and time-consuming statistical analysis in Excel or (god forbid) SPSS using pull-down menus and buttons and I found The Best Result Ever. I quickly put together a paper and submit it to a flagship journal. My R&amp;R comes back several months later and the reviewers say that my results look great, but they want to see if the results hold up if I limit my sample in some way. Now I have to (a) try to remember exactly what I did months ago, and (b) re-run that entire time-consuming analysis from scratch. That is horribly inefficient and is likely to lead to inconsistencies between the two analyses. Imagine that instead of cowboying my analysis, I had written everything into an R or Stata script. Now, I only have to add a line of code at the top that subsets my sample and processes the script. This will take a few minutes at most. Getting Started with Scripts Lets put together a simple script in R. In RStudio, you can go to File &gt; New File &gt; R Script to get a new script up and running. This will open a text pane in the upper left corner of RStudio. This is your R script. Lets start by writing a simple “Hello World” script. Type the following command into your script: cat(&quot;Hello World!&quot;) 2+2 You can run this script a couple of different ways: You could just copy and paste both lines of code to the console in the lower left. This is quick and dirty but is typically not the most efficient way to run code from your script. You could run your entire script by clicking the “Source” button in the upper right corner of the script pane. Note that with this button, you won’t get the output, unless you use the drop-down arrow to choose “Source with Echo”. You could run a single line of your script where the cursor is located. You can do this by either hitting the “Run” button or by clicking Ctrl+Enter on Windows/OSX or Command+Enter on OSX. When you do execute the code this way, your cursor will automatically move to the next line of code, so you can execute your way through the code simply by clicking Ctrl+Enter repeatedly. If you have saved the script to a file, you can also type in the source command in the console to source the script. Lets run this script using the “Source with Echo” button. Here is what the output looks like in my console: &gt; source(&#39;~/.active-rstudio-document&#39;, echo=TRUE) &gt; cat(&quot;Hello World&quot;) Hello World &gt; 2+2 [1] 4 Now, lets save this script as helloworld.R (“.R” is the expected suffix for R scripts) and source it from the console: source(&quot;helloworld.R&quot;, echo=TRUE) ## ## &gt; cat(&quot;Hello World&quot;) ## Hello World ## &gt; 2 + 2 ## [1] 4 Now lets try a slightly more useful script. In this script, I am going to use the politics dataset to do the following: re-code the gay marriage support variable into support for gay marriage vs. all else, and the religion variable as evangelical vs. all else Create a two-way table (crosstab) of these two new variables. Calculate the odds ratio between the two new variables. I don’t expect you to know how all of this code works yet. I just want to show you an example of a script that actually does something interesting. politics$supportgmar &lt;- politics$gaymarriage==&quot;Support gay marriage&quot; politics$evangelical &lt;- politics$relig==&quot;Evangelical Protestant&quot; tab &lt;- table(politics$supportgmar, politics$evangelical) tab prop.table(tab, 2) OR &lt;- tab[1,1]*tab[2,2]/(tab[1,2]*tab[2,1]) OR If I run this script, I will get: source(&quot;politics.R&quot;, echo=TRUE) ## ## &gt; politics$supportgmar &lt;- politics$gaymarriage == &quot;Support gay marriage&quot; ## ## &gt; politics$evangelical &lt;- politics$relig == &quot;Evangelical Protestant&quot; ## ## &gt; tab &lt;- table(politics$supportgmar, politics$evangelical) ## ## &gt; tab ## ## FALSE TRUE ## FALSE 1103 662 ## TRUE 2218 255 ## ## &gt; prop.table(tab, 2) ## ## FALSE TRUE ## FALSE 0.3321289 0.7219193 ## TRUE 0.6678711 0.2780807 ## ## &gt; OR &lt;- tab[1, 1] * tab[2, 2]/(tab[1, 2] * tab[2, 1]) ## ## &gt; OR ## [1] 0.1915562 About 49% of non-evangelicals supported gay marriage while only 24% of evangelicals supported it. That works out to an odds ratio of 0.33, meaning that the odds of gay marriage support were about a third as high for evangelicals as for non-evangelicals. Lets say I then wanted to change this script to broaden the definition of support for gay marriage to include civil unions. I could then just change the line of code creating the supportgmar variable to: politics$supportgmar &lt;- politics$gaymarriage==&quot;Support gay marriage&quot; | politics$gaymarriage==&quot;Civil unions&quot; Not Everything Goes Into Your Script You don’t need to put every command into your script. The script should provide a narrative of your analysis (the part that you want to be easily reproducible), not a log of every single command you ran. Sometimes you may try out some exploratory commands or may just need to get some basic information about a variable. For example, in the script above, I first had to remember what the names were for the categories of the gaymarriage variable. To figure this out, I typed: table(politics$gaymarriage) ## ## No legal recognition Civil unions Support gay marriage ## 773 992 2473 From there I could see that the category I wanted was called “Support gay marriage” and I could use that in my script. However, there was no need to put this command into my script because it wasn’t producing anything that I needed to know or that was necessary for later commands in the script. Commenting for Sanity There is one big thing missing from the scripts listed above: comments! Comments are crucial for good script writing. Comments are lines in your script that will not be processed by R (or Stata). In R, you can create single line comments by using the pound sign (#). Anything after the pound sign will be ignored by R until a new line. You should use these comments to explain what the code is doing. You can also use them to help visually separate the script into sections for easier readability. Comments help you remember what you were doing when you come back to a project you haven’t worked on for weeks or months. They are also useful for other people who might end up reading your code (co-authors, advisers, etc). Increasingly, academics are being asked to do more “open” research where we make our code available. As you write code, its useful to think of it as something that will eventually be seen by other people and thus needs to be well documented. Here is the script from above, but now with some helpful commenting: ################################################################# # Aaron Gullickson # Program to analyze the differences in support for gay marriage # between evangelical christians and all those of other religious # affiliations ################################################################# #### DATA ORGANIZATION #### #load the politics dataset. Note that this command will not work if #you have the dataset loaded into a different directory. load(&quot;politics.RData&quot;) #dichotomize both the support for gay marriage and the religious variable politics$supportgmar &lt;- politics$gaymarriage==&quot;Support gay marriage&quot; politics$evangelical &lt;- politics$relig==&quot;Evangelical Protestant&quot; #### ANALYSIS #### #create a crosstab tab &lt;- table(politics$supportgmar, politics$evangelical) tab #Distribution of support conditional on religion prop.table(tab, 2) #Calculate the odds ratio of support OR &lt;- tab[1,1]*tab[2,2]/(tab[1,2]*tab[2,1]) OR Even if you don’t understand what the code here is doing yet, you can at least get a sense of what it is supposed to be doing. Notice that I use multiple pound signs to draw attention to the header and larger sections. For a script this small, the division between DATA ORGANIZATION and ANALYSIS is probably overkill, but for larger scripts, this sectioning can be very useful in helping to easily distinguish different components of the analysis. One nice feature of RStudio is that if you surround your comments with at least four pound signs on either side, then the outline of your script will show these sections and you can navigate to them. You can also go to Code &gt; Insert Section to add a section with a somewhat different look. Don’t Overcomment While commenting is necessary for good script writing, more commenting is not always better. The key issue is that you need to think of commenting as documentation of your code. If you change the code, you also need to change the documentation. If you change the code, but not the documentation, then you will actually make your code more confusing to other readers. Only write documentation that you will actively maintain. The most common error that novices make is to use comments to describe the results of their code. This is very bad practice, because as you make changes to your data and methods, these results are likely to change and if you don’t update your comments, there will be a lack of alignment between your real results and your documentation of the results. Later in the term, we will learn a better way to separate out the reporting of results in a “lab notebook” fashion, from comments in scripts. "],
["object-types.html", "Object Types", " Object Types R is an object-oriented programming language. This means that within R, you can save a variety of objects to memory. These objects will show up in the upper right panel in the environment tab in RStudio when you create them. A variety of functions can be applied to objects and in many cases, the same function may produce different results when applied to different kinds of objects. We will work most commonly with the data.frame object, but it is useful to know some other basic object types to understand how R works. Objects can have different types and R will often handle them differently depending on their type. However, type is defined in two different ways. Each object has a mode and a class. The class of an object is typically what determines how it is handled by other functions. The mode is most important in terms of the “atomic” or “basic” modes of R objects. These are the basic-building blocks of everything else. Atomic Modes The three most important atomic modes for our purposes are: numeric: records a numeric value. character: records a set of characters. This is often called a “string” in computer science parlance. logical: records either a TRUE or FALSE value. In computer science parlance, this is called a “boolean.” There is also a fourth type complex for things like imaginary numbers, but we won’t really need to worry about that. Lets try assigning values to an object of each mode: a &lt;- 3 b &lt;- &quot;bob said&quot; c &lt;- TRUE a ## [1] 3 mode(a) ## [1] &quot;numeric&quot; class(a) ## [1] &quot;numeric&quot; b ## [1] &quot;bob said&quot; mode(b) ## [1] &quot;character&quot; class(b) ## [1] &quot;character&quot; c ## [1] TRUE mode(c) ## [1] &quot;logical&quot; class(c) ## [1] &quot;logical&quot; note that for these simple objects the mode is the same as the class. These objects will now show up in my upper right panel. However, programming with these simple objects with one value is not very useful. What we usually really want is an aggregation of many of these values. There are a variety of ways we can do this. Vectors and Matrices Vectors If you want to put a bunch of values of the same mode together (don’t call it a “list” because that means something else, see below), you can do that with a vector. You can create a vector with the concatenation function c. Lets do that below: name &lt;- c(&quot;Bob&quot;,&quot;Juan&quot;,&quot;Maria&quot;,&quot;Jane&quot;,&quot;Howie&quot;) age &lt;- c(15,25,19,12,21) ate_breakfast &lt;- c(TRUE,FALSE,TRUE,TRUE,FALSE) name ## [1] &quot;Bob&quot; &quot;Juan&quot; &quot;Maria&quot; &quot;Jane&quot; &quot;Howie&quot; age ## [1] 15 25 19 12 21 ate_breakfast ## [1] TRUE FALSE TRUE TRUE FALSE mode(name) ## [1] &quot;character&quot; class(name) ## [1] &quot;character&quot; mode(age) ## [1] &quot;numeric&quot; class(age) ## [1] &quot;numeric&quot; mode(ate_breakfast) ## [1] &quot;logical&quot; class(ate_breakfast) ## [1] &quot;logical&quot; Note that the mode and class of each vector is given by the mode of the values that makes up the vector. As you can see, vectors are basically equivalent to variables for the purpose of data analysis. We can even calculate values like the mean for numeric and logical vectors: mean(age) ## [1] 18.4 mean(ate_breakfast) ## [1] 0.6 How can you calculate a mean for a logical vector? R automatically converts logical values to numeric values where TRUE=1 and FALSE=0 when it seems like a numeric value is needed. Therefore, the mean is tellling us that 60% of respondents ate breakfast. You can also force vectors (and some other objects) into a different basic type: as.character(age) ## [1] &quot;15&quot; &quot;25&quot; &quot;19&quot; &quot;12&quot; &quot;21&quot; as.numeric(ate_breakfast) ## [1] 1 0 1 1 0 as.numeric(name) ## Warning: NAs introduced by coercion ## [1] NA NA NA NA NA Note that this didn’t work so well when converting a character string to a number and I ended up with a set of missing values (NA). Matrices A matrix is just an extention of a vector, but two dimensional instead of one dimensional. We can use the matrix command to turn a vector into a matrix, by speficying the number of rows and columns. x &lt;- matrix(c(4,5,3,9,7,8), 3, 2) x ## [,1] [,2] ## [1,] 4 9 ## [2,] 5 7 ## [3,] 3 8 mode(x) ## [1] &quot;numeric&quot; class(x) ## [1] &quot;matrix&quot; I created a matrix of numeric values with three rows and two columns. Notice that when putting all of these values, R tries to fill up each column before moving on to the next column. Its also worth noting that the mode and class of my matrix are no longer the same. This means that I can specify specific functions that will apply to the class of matrix. The mode tells me what kind of values I have within the matrix. I can also create a matrix by binding together vectors into different rows (rbind) or columns (cbind). a &lt;- c(4,5,3) b &lt;- c(9,7,8) cbind(a,b) ## a b ## [1,] 4 9 ## [2,] 5 7 ## [3,] 3 8 rbind(a,b) ## [,1] [,2] [,3] ## a 4 5 3 ## b 9 7 8 This might seem like a good way to create a full dataset from the variables I created above, but there is a problem: cbind(name, age, ate_breakfast) ## name age ate_breakfast ## [1,] &quot;Bob&quot; &quot;15&quot; &quot;TRUE&quot; ## [2,] &quot;Juan&quot; &quot;25&quot; &quot;FALSE&quot; ## [3,] &quot;Maria&quot; &quot;19&quot; &quot;TRUE&quot; ## [4,] &quot;Jane&quot; &quot;12&quot; &quot;TRUE&quot; ## [5,] &quot;Howie&quot; &quot;21&quot; &quot;FALSE&quot; A matrix has to have values of the same atomic mode. In most cases, if there is any character vector in the binding, then everything will get converted to characters. We will see a better way to create a dataset (spoiler: the data.frame object) below. Note there is an extension to the matrix object called the array which generalizes it to n-dimensions rather than two. However, we will not make much use of that in this class. Indexing Vectors and Matrices What if I want to know a specific value in my vector or matrix. Lets say I want to know the name of the fourth person in my name vector. You can easily get this value by indexing the vector or matrix with square brackets. In my case: name[4] ## [1] &quot;Jane&quot; Because a vector only has one dimension, I only need one index. In the case of matrices, you will need two numbers, separated by a comma. If you want to get an entire row or column, you can leave one of the indices blank, but you still need the comma. x ## [,1] [,2] ## [1,] 4 9 ## [2,] 5 7 ## [3,] 3 8 #value in 2nd row, 1st column x[2,1] ## [1] 5 #2nd row x[2,] ## [1] 5 7 #1st column x[,1] ## [1] 4 5 3 #1st and 2nd row x[c(1,2),] ## [,1] [,2] ## [1,] 4 9 ## [2,] 5 7 Factors You will note that we don’t yet have a representation for categorical variables. Lets say for example that I wanted to include highest degree received for my respondents from above. I could create this as a character variable: high_degree &lt;- c(&quot;Less than HS&quot;, &quot;College&quot;, &quot;HS Diploma&quot;, &quot;HS Diploma&quot;, &quot;College&quot;) summary(high_degree) ## Length Class Mode ## 5 character character However, this is not very satisfying because there is very little that can be done with categorical variables. As you see the summary command failed to produce anything useful. What I want to do is turn this into a factor. A factor in R is the standard object for coding categorical variables. Each value is actually recorded with a mode of “numeric” but the object also contains a set of labels that provide the meaning of each level. Almost all functions in R will know how to handle these factor objects correcly. To create a factor object, I can just apply the factor function to my vector: high_degree_fctr &lt;- factor(high_degree) levels(high_degree_fctr) ## [1] &quot;College&quot; &quot;HS Diploma&quot; &quot;Less than HS&quot; summary(high_degree_fctr) ## College HS Diploma Less than HS ## 2 2 1 mode(high_degree_fctr) ## [1] &quot;numeric&quot; class(high_degree_fctr) ## [1] &quot;factor&quot; Now the summary command gives me a table of frequencies for each category. Re-ordering categories in factors The only problem with my factor is that this is an ordinal variable and the categories are backwards with “College” first and “Less than HS” last. This is because R sorts alphabetically by default. In order to ensure a specific order to the categories in the factor, I will need to specify the levels argument in the factor function and explicitly write out the order I want: high_degree_fctr &lt;- factor(high_degree, levels=c(&quot;Less than HS&quot;,&quot;HS Diploma&quot;,&quot;College&quot;)) levels(high_degree_fctr) ## [1] &quot;Less than HS&quot; &quot;HS Diploma&quot; &quot;College&quot; summary(high_degree_fctr) ## Less than HS HS Diploma College ## 1 2 2 Another option is to use the relevel function on a factor to just change the very first level of a factor to something else: levels(high_degree_fctr) ## [1] &quot;Less than HS&quot; &quot;HS Diploma&quot; &quot;College&quot; levels(relevel(high_degree_fctr,&quot;HS Diploma&quot;)) ## [1] &quot;HS Diploma&quot; &quot;Less than HS&quot; &quot;College&quot; Logical Values and Boolean Statements One of the most important features of all computer programming languages is the ability to create statements that will either evaluate to a “boolean” value of TRUE or FALSE (a “logical” value in R parlance). These kinds of statements are called boolean statements. The following basic operators will allow you to make boolean statements in R. Operator Meaning == equal to != not equal to &lt; less than &gt; greater than &gt;= less than or equal &lt;= greater than or equal Note that the “equal to” syntax is a double-equals. This is because the single equals is used for assignment of values to objects. As a simple example, lets say that I wanted to identify all respondents from my data above that were 18 years of age or older: age&gt;=18 ## [1] FALSE TRUE TRUE FALSE TRUE You can use factor variables in boolean statements of equality as well, but you need to use the character string variables. Lets say I want to identify all respondents with a college degree: high_degree==&quot;College&quot; ## [1] FALSE TRUE FALSE FALSE TRUE A very important feature of boolean statements is the ability to string together multiple boolean statements with a &amp; (AND) or | (OR) in order to make compound statement. Lets say I wanted to identify all respondents who had either a high school diploma or a college degree: high_degree==&quot;College&quot; | high_degree==&quot;HS Diploma&quot; ## [1] FALSE TRUE TRUE TRUE TRUE Lets say I want to find all respondents who are between the ages of [20,25): age&gt;=20 &amp; age&lt;25 ## [1] FALSE FALSE FALSE FALSE TRUE You can also use parenthesis to ensure that your compound boolean statements are interpreted in the correct order. (age&gt;=20 &amp; age&lt;25) &amp; (high_degree==&quot;College&quot; | high_degree==&quot;HS Diploma&quot;) ## [1] FALSE FALSE FALSE FALSE TRUE Another useful option is the ability to put a ! sign in front of a boolean variable to indicate “not”. Lets say I wanted to find all respondents who had NOT eaten breakfast: !ate_breakfast ## [1] FALSE TRUE FALSE FALSE TRUE Missing Values Missing values are a common feature of most real-world data. They can exist for a variety of reasons, but item non-response (i.e. respondent declined to answer a specific question) is one of the most common reasons. In R, missing values are represented with the NA value. missing values can exist for any of the modes we have discussed. Lets insert a missing value into the age vector that we have been using: age[4] &lt;- NA age ## [1] 15 25 19 NA 21 Watch what happens now when we try to calculate the mean of age: mean(age) ## [1] NA The mean is missing! This is the default behavior for many functions in R. If the values you feed in have missing values, R will return a missing value. R wants to be sure that you explicitly decide how to treat missing values. In Soc 513, we will learn about other ways of dealing with missing values, but our approach this term will be to simply remove observations that have missing values (i.e. casewise deletion) and then calculate the appropriate statistics. In many of the functions in R, this can be accomplished by setting the argument na.rm to TRUE: mean(age, na.rm=TRUE) ## [1] 20 Another useful function to know is the is.na function. If you feed in a vector of values, this function will return a logical vector that evaluates to TRUE if a value is missing. is.na(age) ## [1] FALSE FALSE FALSE TRUE FALSE Combining this with the ! from the previous section, we can easily create a function that tells us which observations have non-missing values: !is.na(age) ## [1] TRUE TRUE TRUE FALSE TRUE Lists Lists are one of the most flexible types of standard objects. Lists are just collections of different objects and the objects can be of different types and dimensions. You can even put lists into lists and end up with lists of lists. Lets put our four variables into a list: my_list &lt;- list(name, age, ate_breakfast, high_degree_fctr) my_list ## [[1]] ## [1] &quot;Bob&quot; &quot;Juan&quot; &quot;Maria&quot; &quot;Jane&quot; &quot;Howie&quot; ## ## [[2]] ## [1] 15 25 19 NA 21 ## ## [[3]] ## [1] TRUE FALSE TRUE TRUE FALSE ## ## [[4]] ## [1] Less than HS College HS Diploma HS Diploma College ## Levels: Less than HS HS Diploma College mode(my_list) ## [1] &quot;list&quot; class(my_list) ## [1] &quot;list&quot; In this case, each item in the list is a vector of the same length but they don’t need to be. Accessing elements of the list You will notice a lot of brackets in the list output above. To access an object at a specific index of the list, I need to use double square brackets. Lets say, I wanted to access the third object (ate_breakfast): my_list[[3]] ## [1] TRUE FALSE TRUE TRUE FALSE If I want to access a specific element of that vector, I can follow up that double bracket indexing with single indexing: my_list[[3]][4] ## [1] TRUE My fourth respondent did eat breakfast. Good to know. There is another way to access objects within the list but to do this, I need to provide a name for each object in the list. I can do this within the initial list command by using a name=value syntax for each object: my_list &lt;- list(name=name, age=age, ate_breakfast=ate_breakfast, high_degree=high_degree_fctr) Now, I can call up any object by its name with the basic syntax list_name$object_name. Lets do that for age: my_list$age ## [1] 15 25 19 NA 21 mean(my_list$age, na.rm=TRUE) ## [1] 20 You will notice in RStudio that when you type the “$”, it brings up a list of all the names you could want. You can select the one you want and tab to complete. Thanks, RStudio! The lapply command is awesome Lets say that I just want to get a summary of each object in my list: summary(my_list) ## Length Class Mode ## name 5 -none- character ## age 5 -none- numeric ## ate_breakfast 5 -none- logical ## high_degree 5 factor numeric Well, that was not super helpful. R is giving me a summary of the list itself rather than a summary of each object in the list. What if I want to apply the same function to every object in the list? This is exactly what the lapply command does. Feed in a list and a function (even a custom function) and it will sequentally apply that function to each object in the list: lapply(my_list, summary) ## $name ## Length Class Mode ## 5 character character ## ## $age ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 15 18 20 20 22 25 1 ## ## $ate_breakfast ## Mode FALSE TRUE ## logical 2 3 ## ## $high_degree ## Less than HS HS Diploma College ## 1 2 2 Thats much better. It won’t be immediately obvious, but lapply turns out to be a very powerful and helpful tool. We will learn more about it later this term. Data Frames The list was a nice way to organize my data, but it wasn’t ideal because it didn’t represent data in the two-dimensional observations-on-the-row and variables-on-the-columns way we expect most datasets to be typical organized. This is where the data.frame object comes in. This is the object that we will mostly work directly with in this class and the one you are most likely to work with in real projects. The data.frame object is basically a special form of a list in which each object in the list is required to be a vector of the same length, but not necessarily the same mode and class. The results can be displayed like a matrix and the same kinds of options for indexing that are available for matrices can be used on data.frames. Lets put the variables into a data.frame: my_data &lt;- data.frame(name, age, ate_breakfast, high_degree=high_degree_fctr) my_data ## name age ate_breakfast high_degree ## 1 Bob 15 TRUE Less than HS ## 2 Juan 25 FALSE College ## 3 Maria 19 TRUE HS Diploma ## 4 Jane NA TRUE HS Diploma ## 5 Howie 21 FALSE College Now that looks like a dataset. Note that by default it just used the name of the object as the column name. However, I specifically changed this behavior for high_degree. We can run a summary on the whole dataset now: summary(my_data) ## name age ate_breakfast high_degree ## Bob :1 Min. :15 Mode :logical Less than HS:1 ## Howie:1 1st Qu.:18 FALSE:2 HS Diploma :2 ## Jane :1 Median :20 TRUE :3 College :2 ## Juan :1 Mean :20 ## Maria:1 3rd Qu.:22 ## Max. :25 ## NA&#39;s :1 It is now treating the name variable like a factor. By default, R will convert all character variables into factors when including them in a data.frame. I can also access any specific variable with the same “$” syntax as for lists: mean(my_data$age, na.rm=TRUE) ## [1] 20 I can also use the same indexing as for matrices to retrieve particular values: my_data[3,2] ## [1] 19 Renaming Variables in a data.frame Vectors, matrices, lists, and data.frames can all have names associated with their elements. You have already seen an example of specifying names in the creation of the list and data.frame objects. But you can also change names of elements after creation. We will focus here specifically on the case of data frames, but much of this is generalizable to other objects as well. the rownames and colnames command can be used to both retrieve and set the row and column names, respectively for a data frame. The colnames command is generally more useful because we typically care more about variable names than observation names. colnames(my_data) ## [1] &quot;name&quot; &quot;age&quot; &quot;ate_breakfast&quot; &quot;high_degree&quot; Lets say I decided that I wanted to have all my variable names capitalized. I can easily change this by just assigning a new character vector to this colnames command: colnames(my_data) &lt;- c(&quot;Name&quot;,&quot;Age&quot;,&quot;Ate_breakfast&quot;,&quot;High_degree&quot;) my_data ## Name Age Ate_breakfast High_degree ## 1 Bob 15 TRUE Less than HS ## 2 Juan 25 FALSE College ## 3 Maria 19 TRUE HS Diploma ## 4 Jane NA TRUE HS Diploma ## 5 Howie 21 FALSE College I can also use indexing of the colnames to just change specific variable names. Lets say I decided that “Ate_breakfast” was too long: colnames(my_data)[3] &lt;- &quot;Breakfast&quot; my_data ## Name Age Breakfast High_degree ## 1 Bob 15 TRUE Less than HS ## 2 Juan 25 FALSE College ## 3 Maria 19 TRUE HS Diploma ## 4 Jane NA TRUE HS Diploma ## 5 Howie 21 FALSE College Subsetting Data Frames One of the most common tasks of organizing data is slicing and dicing datat into smaller subsets. There are two cases that are common. Subsetting Observations You may want only a subset of observations by some logical characteristics (e.g. only women, people born after 1975, people who grew up in Lake Geneva WI, member-states of the OECD). One nice feature of R is that you can use the same indexing of rows as I discussed above but instead of specific index numbers, you can provide a boolean statement and only observations that evaluate to TRUE will be kept. Lets say that you want limit the dataset we have been using to only those who completed college: my_data[my_data$High_degree==&quot;College&quot;,] ## Name Age Breakfast High_degree ## 2 Juan 25 FALSE College ## 5 Howie 21 FALSE College One “gotcha” with this approach is missing values in your boolean statement. Lets try to subset the data to only those respondents 18 years and older: my_data[my_data$Age&gt;=18,] ## Name Age Breakfast High_degree ## 2 Juan 25 FALSE College ## 3 Maria 19 TRUE HS Diploma ## NA &lt;NA&gt; NA NA &lt;NA&gt; ## 5 Howie 21 FALSE College Notice we get one full row of NA values. This was because of the one missing value on age. To avoid this, we first need to tell R to only keep cases that do not have missing values on age: my_data[!is.na(my_data$Age) &amp; my_data$Age&gt;=18,] ## Name Age Breakfast High_degree ## 2 Juan 25 FALSE College ## 3 Maria 19 TRUE HS Diploma ## 5 Howie 21 FALSE College Now it works. However, we can also use the subset command to do the same thing and the subset command will be more robust to this missing value problem. The first argument to the subset command is the data frame you want to subset and the second is a boolean statement about the variables in that data frame. When this boolean statement evaluates to true for an observation it will be kept in the subset. subset(my_data, Age&gt;=18) ## Name Age Breakfast High_degree ## 2 Juan 25 FALSE College ## 3 Maria 19 TRUE HS Diploma ## 5 Howie 21 FALSE College Notice that I did not have to use the full my_data$Age specification in my boolean statement, because the subset command knows that this variable must be in the dataset that I fed in. I also did not have to deal with the whole business of missing values. In general, the subset command produces cleaner and more readable code than bracketing and indexing. Subsetting by variables Sometimes you want to drop variables. This may be because you have used those variables to construct another variable and you don’t need them any longer or maybe your data source just contains a lot more variables than you need. I am a big proponent of dropping irrelevant variables. It keeps your dataset cleaner and easier to read for others. You can use indexing of columns in your data frame to choose variables that you want to keep. Lets say that I only wanted to keep age and highest degree in the dataset that we have been looking at: my_data[,c(&quot;Age&quot;,&quot;High_degree&quot;)] ## Age High_degree ## 1 15 Less than HS ## 2 25 College ## 3 19 HS Diploma ## 4 NA HS Diploma ## 5 21 College You can also use the negative sign in front of a number index to remove it rather than keep it. my_data[,c(-1,-3)] ## Age High_degree ## 1 15 Less than HS ## 2 25 College ## 3 19 HS Diploma ## 4 NA HS Diploma ## 5 21 College You can also use the select argument in the subset command to keep only certain variables. subset(my_data, select=c(&quot;Age&quot;,&quot;High_degree&quot;)) ## Age High_degree ## 1 15 Less than HS ## 2 25 College ## 3 19 HS Diploma ## 4 NA HS Diploma ## 5 21 College You can even use the subset command to simultaneously subset observations and drop variables. Lets say that I wanted to use my Name variable as row names for my dataset and I also want to drop any cases that are missing on age: rownames(my_data) &lt;- my_data$Name my_data2 &lt;- subset(my_data, !is.na(Age), select=c(&quot;Age&quot;,&quot;Breakfast&quot;,&quot;High_degree&quot;)) my_data2 ## Age Breakfast High_degree ## Bob 15 TRUE Less than HS ## Juan 25 FALSE College ## Maria 19 TRUE HS Diploma ## Howie 21 FALSE College "],
["pretty-pictures.html", "Pretty Pictures", " Pretty Pictures One of the best features of R is the ability to make beautiful graphs. R gives the user an incredible amount of control over exactly how your graphs appear. If you know enough, you can virtually make any graph that you can imagine in your head. This flexibility comes at a cost, however. The curve in learning graphics is quite substantial, because of the number of parameters that can be controlled. Nonetheless, if you intend to do quantitative data analysis, then its well worth the effort. In my opinion, figures are almost always a better way to express a result than a table. If the figure is well-designed it will make your results much more intelligible to a broader audience because you have literally helped them to visualize it. In this lab, I am going to show you two techniques for plotting in R. First, I am going to show you how to plot things in R using the basic plotting functions, sometimes called “base plot.” Base plotting in R gives you full control over pretty much everything you could want to stick on a canvas, but sometimes it gives you too much control and you can end up fiddling around a lot to get things to look just how you want them to. So, I am also going to show you how to plot in R using the ggplot2 library. ggplot has become so popular as a method of plotting that many people prefer it to base plot. The “gg” in ggplot refers to teh “graphics of grammar.” The basic idea of ggplot is that rather than control everything about your plot, you give R a “grammar” of what data you want to graph, how you want to graph, and how you want to tie aesthetics to the different pieces of data. In future versions of this course, I plan to transition many of the graphs we use to ggplot although most slides are still written in base plot. There are two different ways you should use pictures in your work. The first method is to use figures to visualize what you data looks like. This is what we have been learning already in class. Histograms, barplots, boxplots, and scatterplots are all examples of trying to visualize your data. Most work on how to make figures is devoted to the topic of visualizing your data like this. However, there is a second way to use pictures, which I think is actually equally important and under-utilized. You can use figures to vizualize your findings. The difference here might not be immediately obvious, partly because we haven’t talked about “models” yet, which is the primary way that results are expressed in quantitative work. Let me give you an example from my own work that may help to communicate what I mean. Here is a figure from an AJS article that shows how to think about the results of a model I ran that tries to predict the number of lynchings in a county by the inequality in occupation between individuals classified as black and mulatto and the inequality between individuals classified as white and mulatto or black (i.e. “African ancestry”). This is a heat (or topological) map of risk where dark areas indicate greater risk (of lynching). The point was to show that the effect of each variable produces opposite effects depending on the level of the other variable. I am not describing data here, but rather the results of a model of lynching risk that is based on the underlying county-level data. Since we don’t know much about these types of models yet, we will focus for now on using graphs to describe data. The data we will look at come from one of my current projects. In this project, my colleague (Nicole Marwell) and I have data on social service contracts awarded by the City of New York to non-profit organizations working within the city. The data I am using here are created by aggregating all the money allocated to a particlar health area (a bureaucractic neighborhood boundary in NYC) between the years of 2009 and 2010. We combine that data with data on the poverty rate for each health areas and also divide by the population in a health area to get an estimate of the social service funding per capita. Here is a peak at what the data look like: head(nyc) ## health_area amtcapita poverty unemployed income borough popn ## 1 10110 29.168592 22.77850 11.324410 40287.02 Manhattan 27983.43 ## 2 10120 109.451055 22.62417 10.179587 43266.45 Manhattan 20235.00 ## 3 10210 216.306532 30.08883 11.616439 29015.87 Manhattan 28688.00 ## 4 10221 29.148974 27.16760 17.279031 33987.68 Manhattan 27161.08 ## 5 10222 2.422527 12.10421 8.580756 66350.84 Manhattan 13372.73 ## 6 10300 506.985063 27.39747 13.606836 32591.55 Manhattan 16035.99 ## lincome poverty.z unemployed.z lincome.z deprivation.summated ## 1 10.60378 0.1623016 0.65034173 0.2704751 0.3902136 ## 2 10.67513 0.1496155 0.34143111 0.1183988 0.2195641 ## 3 10.27560 0.7632204 0.72914069 0.9699954 0.8871098 ## 4 10.43375 0.5230918 2.25709249 0.6328925 1.2296246 ## 5 11.10271 -0.7151376 -0.08998557 -0.7929740 -0.5757443 ## 6 10.39181 0.5419873 1.26621463 0.7222968 0.9116593 We have the numeric code of the health area, the amount of funding per capita, the poverty rate as a percentage of the population, a numeric code for borough (Manhattan, Brooklyn, Bronx, Queens, Staten Island), and the population size. You can download this data in the files section of Canvas. Base Plot For this example, we are going to use a scatterplot to look at the association between funding per capita and poverty rates. Here is the ultimate figure that we want to end up with: There is a lot going on in this figure. Lets identify a few of the nifty features: dots are color-coded by borough the scale of the y-axis is logarithmic which means that an increase of one unit is actually a multiplicative increase of 10 (sort of like the richter scale). The size of dots is scaled to the population size of the health area. There is a best-fitting line drawn through the points. It looks positive. the y-axis has a dollar-sign and commas to make the numbers easier to read and the x-axis has a % marker to make clear the units. there are two different legends drawn on the outside margins of the figure. Thats a lot of stuff. Lets start from the basics and build our way up to this final model. Lets start by just running the basic plot command for a scatterplot: plot(nyc$poverty, nyc$amtcapita) Ok, thats a lot different. Lets deal with the biggest substantive problem. The amount per capita is so heavily right-skewed that it is difficult to see the relationship because almost all of the data points are “squished” down at the bottom of the y-axis. This can be resolved by using whats called a logarithmic scale such that each tick mark indicates not an additive unit increase on the y-axis but a multiplicative increase. We can do this easily by specifying “y” as an option to the log argument in plot: plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;) Ok, that looks better. This graphical approach is related to the idea of tranformations that we will discuss later in the term. Now we can better see that there is a positive effect. However, the numbers on the y-axis tick-marks look horrible. Instead of using the default tickmarks, I can specify my own axis. A quick check using summary reveals that the maximum amount per capita is $12,290. If I am going up by multiples of ten, the next tick mark above this would be 100,000 (from 10,000, to 100,000). So, I am going to do a few things in the next command. First, I am going to use yaxt=\"n\" to tell R not to draw the default y-axis. Then I am going to use ylim=c(0.1,100000) to specify the upper and lower limits on my y-axis. plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000)) Notice that there is now no y-axis tickmarks. I can define those myself with the axis command. plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000)) axis(2,at=c(0.1,1,10,100,1000,10000,100000), las=1) The 2 argument tells R to draw the axis for the y-axis and the at argument allows me to specify where I want the tick marks. The las argument specifies that I want my tickmark labels to be perpendicular to the axis, for easier reading. There are still a couple of problems with this axis. First, the label for the y-axis is now overlapping with the tickmark labels. Second, the tickmark labels are in an ugly exponential notation. These can both be fixed. First, I can fix the tickmark labels. There are a variety of functions such as paste, format, and formatC which will allow you to turn numbers into pretty well-formatted character strings. In this case, I am going to use those functions to write out the full number with a comma at the thousands place and put a dollar sign in front: plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000)) axis(2,at=c(0.1,1,10,100,1000,10000,100000), las=2, labels=paste(&quot;$&quot;,formatC(c(0.1,1,10,100,1000,10000,100000), format=&quot;d&quot;, big.mark=&quot;,&quot;), sep=&quot;&quot;)) I am not going to go over the details of these commands here, but you can see how the tickmark labels are much easier to read now. I still have the problem of the overlapping label for the y-axis and now, the left margin is also not quite large enough for the top label which is being cut-off. To adjust the margin, let me introduce the par command. The par command can be called before the plot command to set up the parameters of the plot. If you look at the help file for par you will see that there are a huge number of parameters that we can adjust. In our case, we want to adjust the mar argument which is a vector of four numbers defining the size of the bottom, left, top, and right margins, respectively. The default for these is c(5,4,4,2). I will increase the left margin slightly and reduce the right margin since we won’t use it. In addition, I am going to remove the default y label by specifying ylab=\"\". par(mar=c(5,6,3,1)) plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000), ylab=&quot;amount per capita&quot;) axis(2,at=c(0.1,1,10,100,1000,10000,100000), las=2, labels=paste(&quot;$&quot;,formatC(c(0.1,1,10,100,1000,10000,100000), format=&quot;d&quot;, big.mark=&quot;,&quot;), sep=&quot;&quot;)) The final step is to define my own y-axis label. I can do this with the title command. If you look at the help file for the title command, you will see that in addition to telling R which title you want to add, you can specify the number of lines away from the graph for the title, which allows us to control its placement. After experimenting around a bit, I discovered that line=5 looked nice. par(mar=c(5,6,3,1)) plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000), ylab=&quot;&quot;) axis(2,at=c(0.1,1,10,100,1000,10000,100000), las=2, labels=paste(&quot;$&quot;,formatC(c(0.1,1,10,100,1000,10000,100000), format=&quot;d&quot;, big.mark=&quot;,&quot;), sep=&quot;&quot;)) title(ylab=&quot;amount per capita&quot;, line=5) The y-axis now looks pretty good. I was able to override the default y-axis with the yaxt=\"n\" and ylab=\"\" arguments to plot and then use the axis and title commands to customize tick marks and labels. Now, I will do the same to the x-axis: par(mar=c(5,6,3,1)) plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000), ylab=&quot;&quot;, xaxt=&quot;n&quot;, xlim=c(0,60), xlab=&quot;Poverty rate&quot;) axis(2,at=c(0.1,1,10,100,1000,10000,100000), las=2, labels=paste(&quot;$&quot;,formatC(c(0.1,1,10,100,1000,10000,100000), format=&quot;d&quot;, big.mark=&quot;,&quot;), sep=&quot;&quot;)) title(ylab=&quot;amount per capita&quot;, line=5) axis(1,at=seq(from=0,to=60,by=10),labels=paste(seq(from=0,to=60,by=10),&quot;%&quot;,sep=&quot;&quot;)) In this case, I didn’t have to define a separate title, because the default x-axis label fits fine. I also don’t need the las command because the default has the correct alignment. Now that I have my axes well-labeled. I can focus on the actual plot. The default “dots” for scatterplots in R are prety ugly, but there are lots of options for better dots. You can specify the shape and style of the dot with the pch argument in plot. If you use ?points, the help file will give you a list of the numeric codes that correspond to different kinds of dots. I usually use pch=21 because it will give me a circle that has a separate border and fill color. The border color can be specified by the col argument an the fill color can be specified by the bg argument. I can also use the cex option to define the size of the dots (relative to the default of 1). I will use this now to create circles of half the average size with a red fill and a black border. model &lt;- lm(I(log(nyc$amtcapita))~nyc$poverty) par(mar=c(5,6,3,1)) plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000), ylab=&quot;&quot;, xaxt=&quot;n&quot;, xlim=c(0,60), xlab=&quot;Poverty rate&quot;, pch=21, bg=&quot;red&quot;, col=&quot;black&quot;, cex=0.5) axis(2,at=c(0.1,1,10,100,1000,10000,100000), las=2, labels=paste(&quot;$&quot;,formatC(c(0.1,1,10,100,1000,10000,100000), format=&quot;d&quot;, big.mark=&quot;,&quot;), sep=&quot;&quot;)) title(ylab=&quot;amount per capita&quot;, line=5) axis(1,at=seq(from=0,to=60,by=10),labels=paste(seq(from=0,to=60,by=10),&quot;%&quot;,sep=&quot;&quot;)) This is now pretty good, but the final touch here is to vary the size of the dots by the size of the health area and the fill color by the borough of the health area. R is very flexible about these types of arguments. If I give a vector of numbers for the size or a vector of colornames for the fill color, R will assume that those colors correspond to the individual dots and will allow for variation in the size and color. For example, lets just feed in population size divided by 10,000 to cex. par(mar=c(5,6,3,1)) plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000), ylab=&quot;&quot;, xaxt=&quot;n&quot;, xlim=c(0,60), xlab=&quot;Poverty rate&quot;, pch=21, bg=&quot;red&quot;, col=&quot;black&quot;, cex=nyc$popn/10000) axis(2,at=c(0.1,1,10,100,1000,10000,100000), las=2, labels=paste(&quot;$&quot;,formatC(c(0.1,1,10,100,1000,10000,100000), format=&quot;d&quot;, big.mark=&quot;,&quot;), sep=&quot;&quot;)) title(ylab=&quot;amount per capita&quot;, line=5) axis(1,at=seq(from=0,to=60,by=10),labels=paste(seq(from=0,to=60,by=10),&quot;%&quot;,sep=&quot;&quot;)) Well, that sort of worked but some of those dots are way too big and some are way too small. The variation in size between health areas is so large that any kind of linear scaling of the population size is going to result in this problem. There are a variety of potential solutions to this, but our earlier use of logarithmic scales suggests an easy one. Logging the population values will allow for differences in size but at a diminishing scale difference. After experimentation, I decided that dividing population size by 3000 and logging with a base of 5 produced good size variation. par(mar=c(5,6,1,1)) plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000), ylab=&quot;&quot;, xaxt=&quot;n&quot;, xlim=c(0,60), xlab=&quot;Poverty rate&quot;, pch=21, bg=&quot;red&quot;, col=&quot;black&quot;, cex=log(nyc$popn/3000,5)) axis(2,at=c(0.1,1,10,100,1000,10000,100000), las=2, labels=paste(&quot;$&quot;,formatC(c(0.1,1,10,100,1000,10000,100000), format=&quot;d&quot;, big.mark=&quot;,&quot;), sep=&quot;&quot;)) title(ylab=&quot;amount per capita&quot;, line=5) axis(1,at=seq(from=0,to=60,by=10),labels=paste(seq(from=0,to=60,by=10),&quot;%&quot;,sep=&quot;&quot;)) The final step is to color-code the dots. I could put in a vector of any five colors. However, it is important to think about accessibility for color-blind individuals and whether a color combination will show up well in print. There are many online resources for this sort of thing. I like ColorBrewer. Here I have specified five classes with a diverging scheme that are colorblind and print friendly. I wiould have selected qualitative scale but there are no color-blind options in that category for five classes. ColorBrewer gives me some options with hexadecimal color codes which I can feed into R. I first create a vector of the five color names and then I use the borough index to assign them in my plot command. color_choices &lt;- c(&quot;#ca0020&quot;,&quot;#f4a582&quot;,&quot;#f7f7f7&quot;,&quot;#92c5de&quot;,&quot;#0571b0&quot;) par(mar=c(5,6,3,1)) plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000), ylab=&quot;&quot;, xaxt=&quot;n&quot;, xlim=c(0,60), xlab=&quot;Poverty rate&quot;, pch=21, bg=color_choices[nyc$borough], col=&quot;black&quot;, cex=log(nyc$popn/3000,5)) axis(2,at=c(0.1,1,10,100,1000,10000,100000), las=2, labels=paste(&quot;$&quot;,formatC(c(0.1,1,10,100,1000,10000,100000), format=&quot;d&quot;, big.mark=&quot;,&quot;), sep=&quot;&quot;)) title(ylab=&quot;amount per capita&quot;, line=5) axis(1,at=seq(from=0,to=60,by=10),labels=paste(seq(from=0,to=60,by=10),&quot;%&quot;,sep=&quot;&quot;)) We are almost there but I also want to add a best-fitting line. Normally, I could do this with the abline command as discussed in the Canvas section on the OLS regression line. However, that won’t work in this case because of the the logarithmic scale on the y-axis. Instead, I can create a sequence of poverty rate values and then based on a model, I can calculate the predicted amount per capita (note that you don’t know how to do this yet, so just hang tight). I can then feed those x and y values into a lines command to draw a line on my plot. model &lt;- lm(I(log(nyc$amtcapita))~nyc$poverty) x &lt;- 0:60 y &lt;- exp(model$coef[1])*exp(model$coef[2])^x par(mar=c(5,6,3,1)) plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000), ylab=&quot;&quot;, xaxt=&quot;n&quot;, xlim=c(0,60), xlab=&quot;Poverty rate&quot;, pch=21, bg=color_choices[nyc$borough], col=&quot;black&quot;, cex=log(nyc$popn/3000,5), bty=&quot;n&quot;) axis(2,at=c(0.1,1,10,100,1000,10000,100000), las=2, labels=paste(&quot;$&quot;,formatC(c(0.1,1,10,100,1000,10000,100000), format=&quot;d&quot;, big.mark=&quot;,&quot;), sep=&quot;&quot;)) title(ylab=&quot;amount per capita&quot;, line=5) axis(1,at=seq(from=0,to=60,by=10),labels=paste(seq(from=0,to=60,by=10),&quot;%&quot;,sep=&quot;&quot;)) lines(x,y, lwd=3, col=&quot;grey20&quot;) I don’t want you to worry to much about the model part. The important feature I want to highlight here is that lines is one of a number of commands that include points, text, and mtext that you can use to later add other stuff to a plot that you have already made. In this case, I have added a straight line. The lwd argument defines the width of the line and the col argument defines the color of the line. I snuck in one other argument here that made a noticeable change. The argument bty defines how the border is drawn around the overall plot area. By setting this to “n”, I removed the border altogether, which I think gives it a cleaner look. The last step is to add some legends. Legends can be tricky. The first thing you have to figure out is where to place the legend. In my case, I would rather have the legend in the margin than in the main plot area, but R won’t do this by default. In order to do that, I need to specify an xpd=TRUE argument in the par command to allow writing output to the margins and not just the main plot area. In the legend command itself, I need to make a label for each component of the legend and then I need to specify how each component is identified. Lets start with the legend for boroughs. model &lt;- lm(I(log(nyc$amtcapita))~nyc$poverty) x &lt;- 0:60 y &lt;- exp(model$coef[1])*exp(model$coef[2])^x par(mar=c(5,6,3,1), xpd=TRUE) plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000), ylab=&quot;&quot;, xaxt=&quot;n&quot;, xlim=c(0,60), xlab=&quot;Poverty rate&quot;, pch=21, bg=color_choices[nyc$borough], col=&quot;black&quot;, cex=log(nyc$popn/3000,5), bty=&quot;n&quot;) axis(2,at=c(0.1,1,10,100,1000,10000,100000), las=2, labels=paste(&quot;$&quot;,formatC(c(0.1,1,10,100,1000,10000,100000), format=&quot;d&quot;, big.mark=&quot;,&quot;), sep=&quot;&quot;)) title(ylab=&quot;amount per capita&quot;, line=5) axis(1,at=seq(from=0,to=60,by=10),labels=paste(seq(from=0,to=60,by=10),&quot;%&quot;,sep=&quot;&quot;)) lines(x,y, lwd=3, col=&quot;grey20&quot;) lg &lt;- legend(0, 100000, legend=c(&quot;Manhattan&quot;,&quot;Bronx&quot;,&quot;Brooklyn&quot;,&quot;Queens&quot;,&quot;Staten Island&quot;), pch=21, col=&quot;black&quot;, pt.bg=color_choices, ncol=5, cex=0.45, pt.cex=1.2, yjust=5) The first two arguments to legend give the x and y placement. The legend argument to legend (I know, its weird) gives the labels for the legend components. The pch argument tells the legend that I am using points and what their shape is. The col argument gives the border color for these points and the pt.bg argument gives their fill color. The ncol tells the legend to use five separate columns rather than a vertical alignment all in one column. The cex and pt.cex arguments indicate the size of the overall legend and the size of the dots, respectively. The yjust argument allows me to fudge the placement to get it just right. Notice, that I saved the output of legend to an object that I called lg. This is a very useful feature of plots. I want to draw my next legend for health area size next to this first legend, but I have no idea exactly how big the first legend will be, so its hard to know at what value of x to start it. I could guess a number here, but that might also change if I rescale the figure manually. However, if I look at the lg object, the information I am looking for is returned there: lg ## $rect ## $rect$w ## [1] 33.53141 ## ## $rect$h ## [1] 0.486 ## ## $rect$left ## [1] 0 ## ## $rect$top ## [1] 3.056 ## ## ## $text ## $text$x ## [1] 1.562143 8.190318 14.818492 21.446667 28.074842 ## ## $text$y ## [1] 2.813 2.813 2.813 2.813 2.813 In this specific case, I am looking for lg$rect$w which gives the width of the first legend. I can use that to set up the placement of my second legend: model &lt;- lm(I(log(nyc$amtcapita))~nyc$poverty) x &lt;- 0:60 y &lt;- exp(model$coef[1])*exp(model$coef[2])^x par(mar=c(5,6,3,1), xpd=TRUE) plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000), ylab=&quot;&quot;, xaxt=&quot;n&quot;, xlim=c(0,60), xlab=&quot;Poverty rate&quot;, pch=21, bg=color_choices[nyc$borough], col=&quot;black&quot;, cex=log(nyc$popn/3000,5), bty=&quot;n&quot;) axis(2,at=c(0.1,1,10,100,1000,10000,100000), las=2, labels=paste(&quot;$&quot;,formatC(c(0.1,1,10,100,1000,10000,100000), format=&quot;d&quot;, big.mark=&quot;,&quot;), sep=&quot;&quot;)) title(ylab=&quot;amount per capita&quot;, line=5) axis(1,at=seq(from=0,to=60,by=10),labels=paste(seq(from=0,to=60,by=10),&quot;%&quot;,sep=&quot;&quot;)) lines(x,y, lwd=3, col=&quot;grey20&quot;) lg &lt;- legend(0, 100000, legend=c(&quot;Manhattan&quot;,&quot;Bronx&quot;,&quot;Brooklyn&quot;,&quot;Queens&quot;,&quot;Staten Island&quot;), pch=21, col=&quot;black&quot;, pt.bg=color_choices, ncol=5, cex=0.45, pt.cex=1.2, yjust=-0.7) legend(lg$rect$w*1.05, 100000, legend=c(&quot;5000 people&quot;,&quot;50,000 people&quot;,&quot;100,000 people&quot;), pch=21, col=&quot;black&quot;,pt.bg=&quot;grey&quot;,ncol=3, pt.cex=log(c(5000,50000,100000)/3000, 5), yjust=-0.7, cex=0.45) And there is the final product. Keep in mind that some of the final touches here are fairly complex. I am not expecting you to be able to produce graphs of this complexity tomorrow. The goal was to show you the richness and depth of graphing in R and to give you some reference points for beginning to build your own beautiful graphs. The plot function is one of the most basic functions for creating plots and once you get the basics down you can create a wide variety of two-dimensional plots. However, there are a variety of other functions that will draw more specific plots. We have already seen examples of pie, barplot, hist, and boxplot. Most of the options for customization that are available for plot are also available for these other functions. For example, in the code here I use the text function to plot the actual percentage values at the top of my bars for a barplot of movie maturity rating and to create lines for the y-axis at 10% intervals. percent &lt;- round(100*table(movies$Rating)/sum(table(movies$Rating)),1) b &lt;- barplot(percent, las=1, ylab=&quot;Percent&quot;, ylim=c(0,50), col=&quot;salmon&quot;) text(b[,1], percent+2, paste(percent, &quot;%&quot;, sep=&quot;&quot;)) abline(h=seq(from=10,to=50,by=10), lwd=0.5, col=&quot;grey80&quot;, lty=2) Another useful command that we haven’t learned yet is matplot which is short for matrix plotting. The matplot function will plot the values of one dimension of the matrix across the indices of the other dimension. This allows you to plot, for example, trend lines separately by different categories. Lets try it out by plotting the time trend in movie runtime separately by maturity rating. The first step in doing this is to calculate the mean of movie runtime by year and maturity rating using the tapply command. tab &lt;- tapply(movies$Runtime, movies[,c(&quot;Year&quot;,&quot;Rating&quot;)], mean) tab ## Rating ## Year G PG PG-13 R ## 2001 89.20000 101.43750 105.1667 107.1744 ## 2002 92.66667 97.42308 107.0139 104.3974 ## 2003 75.40000 96.08333 109.7143 108.1692 ## 2004 93.00000 99.65714 106.2027 105.8769 ## 2005 84.83333 100.76471 109.6024 107.8659 ## 2006 93.85714 99.30233 107.2118 109.6282 ## 2007 99.66667 99.37931 104.7324 107.8900 ## 2008 91.44444 101.50000 105.8750 106.4935 ## 2009 99.33333 98.24242 107.8481 104.1071 ## 2010 103.00000 101.79412 109.4559 102.1495 ## 2011 89.33333 100.84000 109.3297 101.5098 ## 2012 88.00000 99.87500 109.4400 103.1043 ## 2013 104.00000 100.81250 112.7971 103.4904 Now, I can feed this matrix into matplot to see the trend across time. In this case, I am going to leave off NC-17 and Unrated because the small number of movies here makes these measures very noisy. matplot(2001:2013, tab[,1:4], type=&quot;b&quot;, xlab=&quot;year&quot;, ylab=&quot;mean movie runtime&quot;, las=1) One interesting trend is that PG-13 movies have become longer than R movies, mostly because the runtime of R movies has gotten progressively smaller since 2006. G movies are also getting slight longer over time, but its highly variable from year to year. I could have also created this plot with the basic plot command and some lines commands, like so: plot(-1,-1, xlab=&quot;year&quot;, ylab=&quot;mean movie runtime&quot;, las=1, xlim=c(2001,2013), ylim=c(70,120), bty=&quot;n&quot;) lines(2001:2013, tab[,1], lwd=2, col=&quot;green&quot;) lines(2001:2013, tab[,2], lwd=2, col=&quot;blue&quot;) lines(2001:2013, tab[,3], lwd=2, col=&quot;purple&quot;) lines(2001:2013, tab[,4], lwd=2, col=&quot;red&quot;) legend(2001, 120, legend=c(&quot;G&quot;,&quot;PG&quot;,&quot;PG-13&quot;,&quot;R&quot;), lty=1, lwd=2, col=c(&quot;green&quot;,&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;), ncol=4, cex=0.7) The plot command here basically creates an empty canvas because I give a single point coordinate (-1,-1) that is outside the range of my xlim and ylim values. I can then use the lines command to write specific lines onto this blank canvas. ggplot Now, I want to show you how to plot that same scatterplot of non-profit funding in NYC using ggplot. Ggplot builds up a graph from layers. The first and most essential component is the function ggplot where I indicate the data I am using and the aesthetics that I want to be carried through to all of the other layers: ggplot(nyc, aes(x=poverty, y=amtcapita)) This command does not actually plot anything yet. It just sets up the basic structure of my plot by identifying the dataset and that I will use poverty as my x variable and amtcapita as my y variable. I then can add layers to this basic command using the “+” sign. For example, if I wanted to create a scatterplot by plotting points: ggplot(nyc, aes(x=poverty, y=amtcapita))+ geom_point() Now, I have a basic (and very ugly) scatterplot that is similar to what I started with in base plot. I can now add a variety of layers to that start to make a better graph. There are three types of layers I can add: geoms - these are a variety of geometric patterns such as points, bars, lines, etc. The coords that define the coordinate system used to plot the values. We typically won’t futz around with this much because everything is drawn on a Cartesian coordinate system, but it can be useful for maps and some other things. scales - these indicate how I want the scales of my various aesthetics to work. This can include the scaling of my x and y variables, but also things like color gradiations. labels - I can identify labels and themes to use. For example, let me use the scale_y_log10() function to re-scale my y-axis to a logarithmic basis: ggplot(nyc, aes(x=poverty, y=amtcapita))+ geom_point()+ scale_y_log10(labels=dollar) That looks better. Notice that I also gave the scale_y_log10 command an argument of labels. This argument identifies the specific labels I want to use for the tick mark. In this case, I am supplying a function from the scales library that turns raw numbers into formatted dollar amounts. From here I can add a variety of layers and aesthetics to enrich my graph. Let me first add aesthetics for color and size. I will also tranform borough into a proper factor variable so it displays more nicelyin the legend. ggplot(nyc, aes(x=poverty, y=amtcapita, size=popn, color=borough))+ geom_point(alpha=0.7)+ scale_y_log10(labels=dollar)+ scale_color_brewer(palette=&quot;Dark2&quot;)+ theme_bw() By just adding in the borough and pop size as aesthetics, the graph was quickly adjusted. I didn’t have to fiddle around with exact sizing of the dots. Ggplot handled those details. Note that I also added a scale for the color using one of ggplot’s pre-defined palettes. I also used the argument of alpha=0.7 to add some transparency to points, which helps me deal with issues of overplotting. Finally, I used theme_bw() at the bottom to change to a black and white theme. I also want to add a line for the best-fitting OLS regressin line. The geom_smooth function will allow me to do this, although I will have to specify the method: ggplot(nyc, aes(x=poverty, y=amtcapita))+ geom_point(alpha=0.7, aes(color=borough, size=popn))+ geom_smooth(method=&quot;lm&quot;, color=&quot;black&quot;, se=FALSE)+ scale_y_log10(labels=dollar)+ scale_color_brewer(palette=&quot;Dark2&quot;)+ theme_bw() I made a couple of important changes here that are quite subtle but important. First, I moves the aesthetics for color and size out of the ggplot command and put them into the geom_point. This is because I don’t want those aesthetics to apply to all geoms. I only want them to apply to geom_point. If I had left them in they would have affected the geom_smooth and we would have had five separate lines for each borough. I also added color=\"black\" to the geom_smooth command. Note that this is not part of an aesthetic call (e.g. aes()). It is not considered an aesthetic because we are just asking for the line to be a single color. Try surrounding that command in an aes and see what happens. I also used the se=FALSE. If I don’t do this then the line above will be surrounded by a confidence band, which may be good or bad. For our purposes, I did not want to clutter the graph. We are now pretty close to being complete, but I still need to label all of my axes and provide a title. I also want better labeling for the two legends. This can all be done with the labs command which we append to the entire plot: rm(percent) ggplot(nyc, aes(x=poverty/100, y=amtcapita))+ geom_point(alpha=0.7, aes(color=borough, size=popn))+ scale_x_continuous(label=percent)+ geom_smooth(method=&quot;lm&quot;, color=&quot;black&quot;, se=FALSE)+ scale_y_log10(labels=dollar)+ scale_color_brewer(palette=&quot;Dark2&quot;)+ theme_bw()+ theme(legend.position=&quot;right&quot;)+ labs(x=&quot;poverty rate&quot;, y=&quot;amount per capita&quot;, title=&quot;Non-profit funding to NYC health area by poverty rate&quot;, caption=&quot;Data from NYC, 2009-2010&quot;, color=&quot;Borough&quot;, size=&quot;Population&quot;) I did a couple of other things here as well. I added a scale_x_continuous so I could label the x tick mark labels as percents. I also added another theme command that would allow me to change the placement of the legends. I want to keep it on the right, but could have chosen “left”, “right”, “top”, or “bottom.” We now have a very nice looking graph. Ggplot can be a little overwhelming at first, but it has quite a few advantages over base plot. It is designed so that we have to fidget around less with things like the size of our labels, the exact placement of our legends, and the margins of our table. All of that just works internally, and we can focus on the “grammar of graphics”, i.e. the logic structure of what we are trying to say with our graph. Lets do one more example to show how flexible ggplot is. Lets look at the distribution of popularity by race in the Add Health data. Because we have one categorical and one quantitative variable, we want comparative boxplots. Here is our basic set up: ggplot(addhealth, aes(x=race, y=indegree))+ geom_boxplot()+ theme_bw() That works pretty well, However, its often better to display these boxplots horizontally so that we don’t have to worry about category labels overlapping. We can do that with ggplot with the coord_flip command (an example of a coordinate layer): ggplot(addhealth, aes(x=race, y=indegree))+ geom_boxplot()+ coord_flip()+ theme_bw() This is already pretty good. Notice that I don’t have to worry about specifying margins to make sure my category labels fit. Ggplot does that for me. I just need to apply labels and maybe a bit of tint to my boxplots. ggplot(addhealth, aes(x=race, y=indegree))+ geom_boxplot(fill=&quot;grey70&quot;)+ coord_flip()+ theme_bw()+ labs(x=NULL, y=&quot;Number of friend nominations received&quot;, title=&quot;Comparative boxplots of friend nominations by race&quot;, caption=&quot;Add Health data, Wave 1&quot;) Note that the labels for x and y refer to the logic decision of which value is x and y as defined in the aesthetics not the actual placement, which was reversed due to the coord_flip. Note also that I used NULL for the x label because the category labels and title are self-explanatory. There are also some more advanced geoms that do something similar to a boxplot. A popular one is the geom_violin which plots a mirror image of the density distribution. With ggplot, its as simple as swapping out my boxplot with the violin: ggplot(addhealth, aes(x=race, y=indegree))+ geom_violin(fill=&quot;grey70&quot;)+ coord_flip()+ theme_bw()+ labs(x=NULL, y=&quot;Number of friend nominations received&quot;, title=&quot;Comparative boxplots of friend nominations by race&quot;, caption=&quot;Add Health data, Wave 1&quot;) I encourage you to explore the online ggplot documentation. I would also highly recommend Kieran Healy’s new book, Data Visualization which uses ggplot extensively if you want to learn more about using ggplot most effectively. "]
]
