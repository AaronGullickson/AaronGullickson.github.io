[
["index.html", "Statistical Analysis in Sociology Preface", " Statistical Analysis in Sociology Aaron Gullickson 2019-07-10 Preface This online textbook combines all of the information taught in my undergraduate statistics course as well as both terms of my introductory graduate statistics course. The undergraduate course consists of the first five chapters/modules up to and including Building Models. I have put a lot of work into this book, but it is still very much a work in progress, so you may notice typos and other errors on occasion. This textbook is designed to be used with the R statistical software program. If you are taking this course from me, then you will be running R through the RStudio Cloud. Various snippets of R code are interspersed throughout the book in order to show you how to do things. You will also find useful information in the appendices of this book. © Aaron Gullickson, 2017 This material is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License. Users are free to remix tweak, and build upon this work for non-commercial purposes, although new work must acknowledge the original author and use the same license. Full license is available here. "],
["understanding-data.html", "Understanding Data", " Understanding Data In this first module, we will cover what it actually means to have “data” and give a broad overview of what kinds of things we can do with data. Data are the foundation of any statistical analysis and most data that we use in the social sciences consist of variables measured on some observations. In the next two sections, we will learn more about these concepts. Slides for this module can be found here. "],
["what-does-data-look-like.html", "What Does Data Look Like?", " What Does Data Look Like? The data that we look at typically take the format of a “spreadsheet” with rows and columns. The table below shows some characteristics of four randomly drawn passengers from the Titanic, in this type of spreadsheet format. Table 1: Data on four passengers from the Titanic survival sex age agegroup pclass fare family Survived Female 24.0000 Adult First 69.3000 0 Died Male 24.0000 Adult Third 7.7958 0 Survived Male 0.9167 Child First 151.5500 3 Died Male 60.0000 Adult First 26.5500 0 Clearly, we can see variation in who survived and died, the passenger classes they were housed in, gender, and age. We also have a measure of the fare they paid for the trip (in English pounds) and the number of family members traveling with them. To understand how to think about data, we need to understand the concepts of an observation and a variable and the distinction between them. The observations The observations are what you have on the rows of your dataset. In the Titanic example, the observations are individual passengers on board the Titanic, but observations can take many different forms. We use the term unit of analysis to distinguish what kind of observation you have in your dataset. If you are interviewing individual people and recording their responses, then the unit of analysis is individual people. If you are collecting cross-national data by country, then the unit of analysis would be a country. If you are analyzing data on the “best colleges in the US” then the unit of analysis is a university/college. The most common unit of analysis that we will see in this course is an individual person, but several of our datasets involve other units of analysis and it is important to keep in mind that an observation can be many different kinds of things. The variables The variables are what you have on the columns of your dataset. Variables measure specific attributes of your observations. If you conduct a survey of individual people and ask them for their age, gender, and education, then these three attributes would be recorded as variables in your dataset. We refer to them as “variables” because they can take different values across the observations. If you were to conduct a survey of individual people and ask your respondents if they are human, then you probably wouldn’t have a proper variable because everyone would likely respond “yes” and there would be no variation (although we can’t necessarily rule out jedis. There are two major types of variables. Some variables measure quantities of something and thus can be represented by a number. We refer to these as quantitative variables. Other variables indicate a category to which the observation belongs. We refer to these as categorical variables. Quantitative variables Quantitative variables measure quantities of something. A person’s height, a worker’s hourly wage, the number of children that a woman has given birth to, a country’s gross domestic product, a US State’s poverty rate, and the percent of a university’s student body that are women are all examples of quantitative variables. They can all be represented by a number which indicates how much of the thing the observation has. There are two important sub-types of quantitative variables. Discrete variables can logically only take certain values within a range, while continuous variables can logically take any value within a range. The most common example of a discrete variable is a count variable. The number of children that a woman has given birth to is an example of a count variable. This number can only take the value of whole numbers (integers) such as 0, 1, 2, 3, and so on. It makes no sense if a respondent says they have given birth to 2.5 children. Count variables are discrete variables because only whole numbers are logical responses. A person’s height is an example of a continuous variable. It is true that we typically measure height only down to a certain level of precision, typically inches in the United States. We might think that if we were to measure a person’s height in inches, it would only take whole number values and therefore it is discrete. But limitations in measurement don’t define whether a variable is continuous or discrete. Rather the distinction is whether the value could be logically measured to any degree of accuracy. We often measure height out to half inches and we could imagine that if we have a precise enough measurement instrument, we could measure a person’s height out to any decimal level that we desired. So, it is perfectly sensible for someone to say they were 69.825467 inches tall, even though we might think they are being a bit tedious. Note that in both the height and the number of children examples, there are logical limits to the values. You can’t have negative children or height. There are no exact upper limits to the values that either can take, but we would likely think we have a data coding error if we saw a report of a 20 foot person or a woman who gave birth to 50 children. This is what I mean by the statement “within a range” above. Both discrete and continuous variables can be limited in the range of values that they can take. What distinguishes them from each other is what values they can logically take within that limited range. Categorical variables Categorical variables are not represented by numerical quantities but rather by a set of mutually exclusive categories to which observations can belong. The gender, race, political party affiliation, and highest educational degree of a person, the public/private status of a university, and the passenger class of a passenger on the Titanic are all examples of categorical variables. There are also two sub-types of categorical variables. Ordinal variables are categorical variables where the categories have an explicit ordered structure, while nominal variables are categorical variables where the categories are unordered. Highest educational degree is an example of an ordinal variables because it is ordered such that Graduate Degree &gt; BA Degree &gt; AA Degree &gt; High School Diploma &gt; No degree. Passenger class is also an ordinal variables that starts in Third class (or steerage - Think Leonardo DiCaprio) and ends in First class (think Kate Winslet), with a Second class in between. Race, gender, and political party affiliation are all examples of nominal variables. The categories here have no ordering to them. While some people might have their own political party preferences, these sort of normative evaluations of categories are irrelevant. For the same reason, even the variable of survival on the Titanic is a nominal variable. We don’t judge the value of life and death. "],
["what-can-we-do-with-data.html", "What Can We Do With Data?", " What Can We Do With Data? We now know what data looks like, but what do social scientists do with data? in the first part of this course, we will learn three fundamental data analysis tasks: analysis of the distribution of a single variable, measuring association, and statistical inference. In the final part of the course, we will build on these fundamentals to learn how to build more complex statistical models. How is a variable distributed? Sometimes, we just want to understand what a single variable “looks like.” We may simply be interested in its “average” or we may want to know something else, like how spread out the values of the variable are. In these cases, we calculate univariate (\"one variable) statistics on the distribution of a variable. Typically, univariate statistics aren’t as interesting to social scientists as the measures of association discussed below, but even then its often a good idea to look at univariate statistics to understand all of the variables in your research. In some cases, the calculation of a univariate statistics is the important question at hand. For example, when poll researchers try to figure out who is going to win an election, they are very much interested in the univariate distribution of support for each candidate, which gives the proportions of likely voters who intend to vote for each candidate. Here are some other questions we could ask in our data: How much variability is there in the amount of money that movies make? What percent of passengers survived the Titanic disaster? What is the average age of voters in the United States? Measuring association Social scientists are often most interested in the relationships, or association, between two or more variables. These associations allow us to test hypotheses about causal relationships between underlying social constructs. For example, we might be interested in whether divorce affected children’s well-being. In this case, we would want to look at the relationship between the categorical variable indicating whether a child’s parents were divorced and some measure of their well-being, such as feelings of stress, academic performance, etc. There are different ways of measuring association depending on the types of variables involved.. Here are some questions about association we could ask in our data: Did the probability of surviving the Titanic depend on passenger class? (categorical and categorical) Do the earnings of movies vary by genre? (quantitative and categorical) Is income inequality in a state related to its crime rate? (quantitative and quantitative) In the first part of the course, we will learn how basic measures of association between two variables, depending on what kind of variables we are using. In the final part of the course, we will return to this topic when we learn how to build more complex statistical models discussed below. Making statistical inferences If I told you that in my sample of twenty people, brown-eyed individuals make $5 more than all other eye colors combined, would you believe me? You probably shouldn’t, because in a sample of twenty people, even when drawn without systematic bias, weird results like this are not unlikely just by random chance. If I told you I observed this phenomenon on a well-drawn sample of 20,000 individuals, you would probably be more likely to believe me. The underlying concept here is called statistical inference. We often want to draw conclusions about the larger populations from which our samples are drawn. Statistical inference is the technique of quantifying how uncertain we are about whether our data are similar to the population or not. When you hear press reports on political polls with the term “margin of error,” they are referring to statistical inference. Many introductory statistics course focus most of their attention on statistical inference, partly because it is more abstract and complex. However, statistical inference is always secondary to the basic descriptive measures of univariate, bivariate, and multivariate statistics. Therefore, I spend considerably less time on this topic than in most statistics courses, so that we can focus on the more important stuff. Building Models Although our basic measures of association are useful, the most common tool in social science analysis is a statistical model in which the user can specify the relationships between variables by some kind of mathematical functions. In the final module of the course, we will learn how to build basic versions of these models that allow us to look at the relationships between multiple quantitative and categorical variables. This section will build on our prior work in all of the previous modules. We will specifically focus on two uses of statistical models. First, statistical models will allow us to “control” for other variables when we look at the association between any two variables. Controlling for other variables is important because they may be confounded with the relationship we want to measure. For example, we may be interested in the relationship between marital status (e.g. never married, married, widowed, divorced) and sexual frequency in our GSS data. However, these different groups vary significantly in their age. Never married individuals are much younger than all of the other groups and widowed individuals much older. Given the fact that sexual frequency tends to decline with age (something we will show later in this term), it seems problematic to just compare the average sexual frequency across these groups. This is because age confounds the relationship between marital status and sexual frequency. Statistical models will gives us tools to account for this problem and to get a better estimate of the relationship between marital status and sexual frequency, net of this confounder. Second, statistical models will allow us to account for how the relationship between two variables might differ depending on the context of a third variable. This is what we call an interaction. For example, lets say we were interested in the relationship between the number of sports played and a student’s popularity (measured by friend nominations) in our Add Health data. Because of gender norms, we might expect that this relationship is different for boys and girls. We can use statistical models to empirically examine whether this is true. This kind of contextualization is an important component of sociological practice. Observational Data, Experimental Thinking Much of the data that we use in sociology is observational rather than experimental. In an experimental design, the researcher randomly selects subjects to receive some sort of treatment and then observes how this treatment affects some outcome. Thus, the research engages in systematic manipulation to observe a response. In observational data, the researcher does not directly manipulate the environment but rather just observes and records the social setting as it is. Experimental data can be more powerful than observational data because the random assignment of a treatment through researcher manipulation strengthens claims of causation. If there is a relationship between treatment and response it can only come through a causal relationship or random chance. In observational data, the relationship between any two variables can be a result of a causal relationship, random chance,spuriousness. Spuriousness occurs when other variables produce the relationship between two other variables rather than them directly causing each other. The example above about marital status and sexual frequency is a simple example. If we note that widows have less sex than other people, we may be tempted to think that something about being widowed reduces someone’s sexual drive or their interactions with others. However, the more obvious explanation is that widows tend to be quite a bit older than other marital status groups and older people have less sex. Age is generating a spurious relationship between widowhood and sexual frequency. This kind of spuriousness is the reason for the frequent claim that “correlation is not causation.” There are two different philosophical approaches to the statistical analysis of observational data where spuriousness can be a problem. The first approach approaches it as pseudo-experimental data. The goal of this approach is to try to find ways to mimic the experimental design approach with observational data. At a basic level this can include “controlling” for other variables (which we will learn) and can extend to a variety of techniques of causal modeling that are intending to use some feature of the data to recover causation (which we will not learn). The second approach treats statistical analysis as a way to describe observed data in a formal, systematic, and replicable way. The goal is to establish to what extent the data are consistent with competing theories that seek to understand the outcome in question, rather than to mimic the experimental approach. Although quantitative and qualitative approaches are often seen as philosophically different approaches, this approach to observational data shares many features with more purely qualitative approaches to data analysis. This is the approach that I take in this course. "],
["the-distribution-of-a-variable.html", "The Distribution of a Variable", " The Distribution of a Variable The distribution of a variable refers to how the different values of that variable are spread out across observations. This distribution gives us a sense of what the most common values are for a given variable and how much these values vary. It can also alert us to unusual patterns in the data. Intuitively, we think about distributions in our personal life any time we think about how we “measure” up relative to everyone else on something (income, number of Facebook friends, GRE scores, etc.). We want to know where we “fall” in the distribution of one of these variables. In this chapter we will first learn graphical techniques that allow us to visualize what the distribution of a variable looks like. For quantitative variables we will then move on to calculate important summary statistics that measure the center and spread of a distribution. Slides for this module can be found here. "],
["looking-at-distributions.html", "Looking at Distributions", " Looking at Distributions One of the best ways to understand the distribution of a variable is to visualize that distribution with a graph. However, the technique we use to graph the distribution will depend on whether we have a categorical or a quantitative variable. For categorical variables, we will use a barplot, while for quantitative variables, we will use a histogram. Looking at the distribution of a categorical variable Calculating the frequency In order to graph the distribution of a categorical variable, we first need to calculate its frequency. The frequency is just the number of observations that fall into a given category of a categorical variable. We could, for example, count up the number of passengers who were in the various passenger classes on the Titanic. Doing so would give us the following: Table 2: Passengers on the Titanic by passenger class Passenger class Frequency First 323 Second 277 Third 709 Total 1309 There were 323 first class passengers, 277 second class passengers, and 709 third class passengers. Adding those numbers up gives us 1,309 total passengers. R will calculate these numbers for us easily using the table command: table(titanic$pclass) ## ## First Second Third ## 323 277 709 Frequency, proportion, and percent The frequency we just calculated is sometimes called the absolute frequency because it just counts the raw number of observations. However such raw numbers are not usually very helpful because they will vary by the overall number of observations. Instead, we typically want to calculate the proportion of observations that fall within each category. These proportions are sometimes also called the relative frequency. We can calculate the proportion by simply dividing our absolute frequency by the total number of observations: Table 3: Passengers on the Titanic by passenger class Passenger class Frequency Proportion First 323 323/1309=0.247 Second 277 277/1309=0.212 Third 709 709/1309=0.542 Total 1309 1.0 R provides a nice shorthand function titled prop.table to conduct this operation. The prop.table command should be run on the output from a table command. prop.table(table(titanic$pclass)) ## ## First Second Third ## 0.2467532 0.2116119 0.5416348 Note that I have “wrapped” the prop.table command around the table command here to do this calculation in a single line. We often convert proportions to percents which are more familiar to most people. To convert a proportion to a percent, just multiply by 100: Table 4: Passengers on the Titanic by passenger class Passenger class Frequency Proportion Percent First 323 323/1309=0.247 0.247*100=24.7% Second 277 277/1309=0.212 0.212*100=21.2% Third 709 709/1309=0.542 0.542*100=54.2% Total 1309 1.0 100% 24.7% of passengers were first class, 21.2% of passengers were second class, and 54.2% of passengers were third class. Just over half of passengers were third class and the remaining passengers were fairly evenly split between first and second class. Don’t make a piechart Now that we have proportions/percents, we can use these values to construct a graphical display of the distribution. One of the most common techniques for doing this is a piechart. Figure 1 shows a piechart of the distribution of passengers on the Titanic. Figure 1: Piechart of passenger class distribution on Titanic You will notice that I do not show you the code for constructing this piechart. I hid this code because I don’t ever want you to construct a piechart. Despite their popularity, piecharts are a poor tool for visualizing distributions. In order to judge the relative size of the slices on a piechart, your eye has to make judgments in two dimensions and with an unusual pie-shaped slice (\\(\\pi*r^2\\) anyone?). As a result, it can often be difficult to decide which slice is bigger and to properly evaluate the relative sizes of each of the slices. In this case, for example, the relative size of first and second class are quite close and it is not immediately obvious which category is larger. Make a barplot A better way to display the distribution is by using a barplot in which vertical or horizontal bars give the proportion or percent. Figure 2 shows a barplot for the distribution of passengers on the Titanic. ggplot(titanic, aes(x=pclass, y=..prop.., group=1))+ geom_bar()+ labs(x=&quot;passenger class&quot;, y=&quot;percent&quot;)+ scale_y_continuous(labels=scales::percent)+ coord_flip()+ theme_bw() Figure 2: Barplot of passenger class distribution on the Titanic Figure 2 is our first code example of the ggplot library that we will use to make figures in R. I will discuss how this code works below, but first I want to focus on the figure itself. Unlike the piechart, our eye only has to work in one dimension (in this case, horizontal). We can clearly see that third class is the largest passenger class with slightly more than 50% of all passengers. We also can see easily determine visually that third class passengers are at more than twice as common as either of the other two categories, simply by comparing the height of the bars. We can also see that slightly more passengers were in first class than second class simply by comparing the heights of those two bars. So how did I create this graph? The ggplot syntax is a little different than most of the other R syntax we will look at this term. With ggplot, we add several commands together with the + sign to create our overall plot. Each command adds a “layer” to our plot. These layers are: ggplot(titanic, aes(x=pclass, y=..prop.., group=1)): The first command is always the ggplot command itself which defines the dataset we will use (in this case, titanic) and the “aesthetics” that are listed in the aes argument. In this case, we defined an x (horizontal) variable as the pclass variable itself and the y (vertical) variable as a proportion (which in ggplot-speak is ..prop..). The group=1 argument is a trick for barplots that makes sure our proportions add up to 1 across all categories. geom_bar(): This command defines what geometric shape (in this case a bar) to actually plot. These two layers would be enough for a figure, but I also add four more layers that make for a nicer looking figure. You can try the command above with only the first two elements to see how it is different. labs(x=\"passenger class\", y=\"percent\"): The labs command allows me to add a variety of nice labels to my graph. In this case I labeled my x and y axes. scale_y_continuous(labels=scales::percent): This is not a necessary command but it is useful to get better labels on my y-axis tickmarks. Without this command, the tickmarks would just show proportions (e.g. 0.2, 0.4). This command allows me define custom labels for those tickmarks that show them as percents rather than proportions. coord_flip: This does exactly what it sounds like. It flips the x and y axes of the graph. This causes the bars to display horizontally rather than vertically. This is not necessary, but is often a useful feature to avoid problems with long category names overlapping on the x-axis. Note that even though passenger class looks like it is on the y-axis, ggplot still treats it as the “x” variable for things like defining aesthetics and labeling with the labs layer. theme_bw(): This last command is for the “theme” layer that just defines a variety of characteristics for the overall look of the figure. I like the theme_bw (bw for black and white) theme over the default theme that comes with ggplot. Ggplot is a very flexible system that repeats these same basic layer elements in all the graphs it creates. In an appendix to this book, you can see cookbook examples of all the ways we will we use it to create figures throughout the term. Looking at the distribution of a quantitative variable Barplots won’t work for quantitative variables because quantitative variables don’t have categories. However, we can do something quite similar with the histogram. One way to think about the histogram is that we are imposing a set of categories on a quantitative variable by breaking our quantitative variable into a set of equally wide intervals that we call bins. The most important decision with a quantitative variable is how wide to make these bins. As an example, lets take the age variable from our politics dataset. I could break this variable into five-year intervals that go from 0-5, 5-10, 10-15, 15-20, and so on. Alternatively, I could use 10-year intervals from 0-9, 10-19, 20-29, and so on. I could also use any other interval I like such as 1-year, 3-year, and so on. Lets use 10-year intervals for this example. I then just have to count up the number of observations that fall into each 10 year interval. Table 5: Age distribution in politics datatset in 10 year groups Age Group Frequency 0-9 0 10-19 127 20-29 616 30-39 756 40-49 650 50-59 822 60-69 745 70-79 369 80-89 153 Because the survey was only administered to adults, I have zero individuals from 0-9 and only a few in the 10-19 age range. Now that we have the frequencies for each bin, we can plot the histogram. The histogram looks much like a barplot except for two important differences. First, on the x-axis we have a numeric scale for the quantitative variable rather than categories. Second, we don’t put any space between our bars. Below I show some R code for producing the histogram shown in Figure 3 using ggplot. This code is simpler than the barplot code. We only need to define an x aesthetic in our ggplot command. Instead of a geom_bar we use a geom_histogram. I can specify different bin widths in this command with the binwidth argument. Here I have specified a binwidth of ten years. I have also specified two different colors. The col argument is the color of the border for each bar and the fill argument is the color of the bars themselves. ggplot(politics, aes(x=age))+ geom_histogram(binwidth=10, col=&quot;black&quot;, fill=&quot;grey&quot;)+ labs(x=&quot;age&quot;)+ theme_bw() Figure 3: Histogram of age in the politics data Looking at Figure 3, I can see the peak in the distribution in the 50’s (the baby boomers) with a long fat tail to the right and a steeper drop off at older ages, due to smaller cohorts and old age mortality.I can also sort of see a smaller peak in the 25-34 range which is largely the children of the baby boomers. What would this histogram look like if I had used 5-year bin widths instead of 10-year bins? Lets try it: ggplot(politics, aes(x=age))+ geom_histogram(binwidth=5, col=&quot;black&quot;, fill=&quot;grey&quot;)+ labs(x=&quot;age&quot;)+ theme_bw() Figure 4: Histogram of age in the politics data with five year bins As Figure 4 shows, I get more or less the same overall impression but a more fine-grained view. I can more easily pick out the two distinct “humps” in the distribution that correspond to the baby boomers and their children. Sometimes adjusting bin width can reveal or hide important trends and sometimes it can just make it more difficult to visualize the distribution. As an exercise, you can play around with the interactive example below. What kinds of things should you be looking for in your histogram? There are four general things to be on the lookout for when you examine a histogram. Center Where is the center of the distribution? Loosely we can think of the center as the peak in the data, although we will develop some more technical terms for center in the next section. Some distributions might have more than one distinct peak. When a distribution has one peak, we call it a unimodal distribution. Figure 5 shows a clear unimodal distribution for the runtime of movies. We can clearly see that the peak is somewhere between 90 and 100 minutes. When a distribution has two distinct peaks, we call it a bimodal distribution. Figure 6 shows that the distribution of violent crime rates across states is bimodal. The first peak is around 2200-2700 crimes per 100,000 and the second is around 3500 crimes per 100,000. This suggests that there are two distinctive clusters of states: in the first cluster are states with a moderate crime rate and in the second cluster, states with high crime rate states. You can probably guess what we call a distribution with three peaks (and so on), but its fairly rare to see more than two distinct peaks in a distribution. Figure 5: The distribution of movie runtimes is unimodal with one clear peak around 90-100 minutes Figure 6: The distribution of property crimes by states is bimodal with a two separate peaks. Shape Is the shape symmetric or is one of the tails longer than the other? When the long tail is on the right, we refer to this distribution as right skewed. When the long tail is on the left, we refer to the distribution as left skewed. Figures 7 and 8 show examples of roughly symmetric and heavily right-skewed distributions, respectively, from the movies dataset. The Tomato Rating that movies receive (a score from 1-10) is roughly symmetric with about equal numbers of movies above and below the peak. Box office returns on the other hand are heavily right-skewed. Most movies make less than $100 million at the box office but there are few “blockbusters” that rake in far more. Right-skewness to some degree or another is common in social science data, partially because many variables can’t logically have values below zero and thus the left tail of the distribution is truncated. Left-skewed distributions are rare. In fact, they are so rare that I don’t really have a very good example to show you from our datasets. Figure 7: The distribution of movie tomato ratings is roughly symmetric Figure 8: The distribution of movie box office returns is heavily right skewed Spread How spread out are the values around the center? Do they cluster tightly around the center or are they spread out more widely? Typically this is a question that can only be asked in relative terms. We can only say that the spread of a distribution is larger or smaller than some comparable distribution. We might be interested for example in the spread of the income distribution in the US compared to Sweden, because this spread is one measure of income inequality. Figure 9 compares the distribution of movie runtime for comedy movies and sci-fi/fantasy movies. The figure clearly shows that the spread of movie runtime is much greater for sci-fi/fantasy movies. Comedy movies are all tightly clustered between 90 and 120 minutes while longer movies are more common for sci-fi/fantasy movies leading to a longer right tail and a correspondingly higher spread. Figure 9: The distribution of movie runtime is much more spread out for sci-fi/fantasy films than it is for comedies. Outliers Are there extreme values which fall outside the range of the rest of the data? We want to pay attention to these values because they may have a strong influence on the statistics that we will learn to calculate to summarize a distribution. They might also influence some of the measures of association that we will learn later. Finally, extreme outliers may help identify data coding errors. In Figure 6 above, we can see a clear outlier in the violent crime rate for Washington DC. Washington DC’s crime rate is such an outlier relative to the other states that we will pay attention to it throughout this term as we conduct our analysis. Figure 9 above shows that Peter Jackson’s Return of the King was an outlier for the runtime of sci-fi/fantasy movies, although it doesn’t seem quite as extreme as for the case of Washington DC. In large datasets, it can sometimes be very difficult to detect a single outlier on a histogram because the height of its bar will be so small. Later in this chapter, we will learn another graphical technique called the boxplot that can be more useful for detecting outliers in a quantitative variable. "],
["measuring-the-center-of-a-distribution.html", "Measuring the Center of a Distribution", " Measuring the Center of a Distribution When we look at a distribution, we often can get an intuitive sense of where its “center” is. But what do we really mean by the term “center?” The notion of center often allows us to think about the value we expect a typical or “average” observation to have, but there are multiple ways of defining this center. In statistics, three different measures of center are used: the mean, median, and mode. The mean The mean is the measure most frequently referred to as the “average” although that term could apply to the median and mode as well. The mean is the balancing point of a distribution. Imagine trying to balance a distribution on your finger like a basketball. Where along the distribution would you place your finger to achieve this balance? This point is the mean. It is equivalent to the concept of “center of mass” from physics. The calculation of the mean is straightforward: Sum up all the values of your variable across all observations. Divide this sum by the number of observations. As an example, lets take a sub-sample of our movie data. I am going to select all the romances (not including rom-coms which are coded as comedies) produced in 2013. There were nine “pure” romances in 2013. I want to know their mean Tomato Meter rating. Table 6: Tomato meter of pure romance movies released in 2013 Title Tomato Meter Kill Your Darlings 77 I’m in Love with a Church Girl 6 Safe Haven 12 The Face of Love 43 Love and Honor 13 Before Midnight 98 Drinking Buddies 83 Ain’t Them Bodies Saints 79 The Great Gatsby 49 Sum 460 In the very last row, I show the sum of the Tomato Meter rating which simply sums up the Tomato Meter rating of each movie. To calculate the mean, I simply divide this sum by the number of movies which is 9. \\[\\bar{x}=\\frac{460}{9}=51.11\\] The mean Tomato Meter rating for romantic movies in 2013 was 51.11. Notice the funny \\(x\\) with a bar over it (“x bar”). Mathematically, we often represent variables with lower-case roman letters like \\(x\\) or \\(y\\). Putting the bar above the letter is the way to mathematically signify the mean of that variable. Since we are discussing math symbols, lets talk about creating a mathematical formula for what we just did. In order to do that, I need to introduce a variety of mathematical symbols, but don’t get frightened. We are just formalizing the method of calculating the mean that I just demonstrated above. First, as I said we represent a given variable by a lower-case roman letter, such as \\(x\\). If we want to specify a particular observation of \\(x\\), we use a subscript number. So \\(x_1\\) is the value of the first observation of \\(x\\) in our data and \\(x_{25}\\) is the value of the 25th observation of \\(x\\) in our data. We use the letter n to signify the number of observations so \\(x_n\\) is always the last observation of \\(x\\) in our data. Now we need some way to indicate “sum up all the values of \\(x\\).” This is given by the summation sign which looks as follows: \\[\\sum_{i=1}^n x_i\\] In English, this just means “sum up all the values of \\(x\\), starting at \\(x_1\\) and ending at \\(x_n\\).” That gives us our sum, which we just need to divide by the number of observations, \\(n\\). \\[\\bar{x}=\\frac{\\sum_{i=1}^n x_i}{n}\\] In R, the mean is very straightforward to calculate. Lets calculate the mean of the Tomato Meter rating for all movies in our dataset: sum(movies$TomatoMeter)/nrow(movies) ## [1] 47.77595 The sum command calculates the sum and the nrow command calculates the number of rows of our dataset, which is equivalent to the number of observations. Or we could go even simpler and just use the mean command: mean(movies$TomatoMeter) ## [1] 47.77595 We can see that the mean Tomato Meter of romantic movies in 2013 (51.1) were slightly above the mean for all the movies in our dataset (47.8), but not by much. One last thing to note is that it only makes sense to calculate the mean of quantitative variables. You cannot add up the values for categorical variables because categorical values don’t have numeric values. Rather, they have categories. Notice that you will get an “NA” value and a warning if you try to do this using the mean command: mean(movies$Rating) ## Warning in mean.default(movies$Rating): argument is not numeric or logical: ## returning NA ## [1] NA The median The median is almost as widely used as the mean as a measure of center, for reasons I will discuss below. The median is the midpoint of the distribution. It is the point at which 50% of observations have lower values and 50% of the observations have higher values. In order to calculate the median, we need to first order our observations from lowest to highest. Lets do that with the romantic movie data above. Table 7: Tomato meter of pure romance movies released in 2013, sorted from worst to best Title Tomato Meter Order I’m in Love with a Church Girl 6 1 Safe Haven 12 2 Love and Honor 13 3 The Face of Love 43 4 The Great Gatsby 49 5 Kill Your Darlings 77 6 Ain’t Them Bodies Saints 79 7 Drinking Buddies 83 8 Before Midnight 98 9 To find the median, we have to find the observation right in the middle. When we have an odd number of observations, finding the exact middle is easy. In this case, it is given by the 5th observation because it has four observations lower than it and four observations higher than it. So the median Tomato Meter rating for romantic movies in 2013 was 49. When you have an even number of observations, then finding the median is slightly more tricky because you have no single observation that is exactly in the middle. In this case, you find the two observations that are closest to the middle and calculate their mean (sum them up and divide by two). For example, if we had ten observations, we would take the mean of the 5th and 6th observations to calculate the median. The median command in R will calculate the median for you. median(movies$TomatoMeter) ## [1] 47 In this particular case, the mean (47.8) and median (47) of the Tomato Meter produced very similar measures of center, but this isn’t always the case as I will demonstrate below. As for the mean, it makes no sense to calculate the median of a categorical variable. The mode The mode is the least used of the three measures of center. In fact, it is so infrequently used that R does not even have a built-in function to calculate it. The mode is the high point or peak of the distribution. When you look at a distribution graphically, the mode is what your eye is drawn to as a measure of center. Calculating the mode however is much trickier. Simply speaking, the mode is the most common value in the data. However, when data are recorded down to very precise decimal levels, this can be a misleading number. Furthermore, there may be multiple “peaks” in the data and so speaking of a single mode can be misleading. In the sample of 2013 romance movies, no value was repeated and so there is no legitimate value for the mode. In the case of the tomato meter distribution for the movies dataset, we have a tie for the most common value. 41 movies had Tomato Meter ratings of 29, 33, and 51, respectively. Figure 10 shows that the distribution of tomato meter ratings lacks any real peak, with a relatively even distribution across most values. ggplot(movies, aes(x=TomatoMeter))+ geom_histogram(binwidth=5, col=&quot;black&quot;, fill=&quot;grey&quot;)+ labs(x=&quot;age&quot;)+ geom_vline(xintercept = c(29,33,51), size=2, color=&quot;red&quot;)+ theme_bw() Figure 10: Histogram of Tomato Meter ratings from politics dataset with three candidates for the mode drawn in red Interestingly, while it does not make sense to think about a mean or a median for quantitative variables, we can use the idea of the mode for categorical variables. The most common category is often referred to as the modal category. If you want to quickly identify the modal category for a categorical variable, you can wrap a table command inside a sort command with the decreasing=TRUE argument to see the top category at the right. sort(table(movies$Rating), decreasing = TRUE) ## ## R PG-13 PG G ## 1143 991 363 56 R-Rated movies are the modal category for maturity rating in our movies dataset. Comparing the mean and median How can the mean and median give different results? Remember that the mean defines the balancing point and the median defines the midpoint. If you have a perfectly symmetric distribution, then these two points are the same because you would balance the distribution at the midpoint. However, when the distribution is skewed in one direction or another, the mean and the median will be different. In order to maintain balance, the mean will be pulled in the direction of the skew. When you have heavily skewed distributions, this can lead to dramatically different values for the mean and median. Lets look at this phenomenon for a couple of variables in the movie dataset. As the histogram above showed, the Tomato Meter variable is fairly symmetric and as a result we end up with a mean (47.8) and median (47) that are pretty close. Figure 11 shows the distribution of movie runtime which is somewhat more right-skewed. Figure 11: Distribution of movie runtime with mean (105.2) and median (102) shown as vertical lines The skew here is not too dramatic, but it pulls the mean about 3 minutes higher than the median. Figure 12 shows the distribution of box office returns to movies which is heavily right skewed. Most movies make moderate amounts of money, and then there are a few star performers that make bucket loads of cash. Figure 12: Distribution of movie box office returns with mean (45.2) and median (21.6) shown as vertical lines As a result of this skew, the mean box office returns are about $45.2 million, while the median box office returns are about $21.6 million. The mean here is more than double the median! Note that neither estimate is in some fundamental way incorrect. They are both correctly estimating what they were intended to estimate. It is up to us to understand and interpret these numbers correctly and to understand their limitations. In many cases, we are actually more interested in the median as a measure of “average” experience than the mean, even though we think of the mean as the “average.” This is, for example, why you see home prices in an area always reported in terms of medians rather than means. Mean home prices tend to be much higher than median home prices because of the relatively few very expensive homes in a given area. "],
["percentiles-and-the-five-number-summary.html", "Percentiles and the Five Number Summary", " Percentiles and the Five Number Summary In this section, we will learn about the concept of percentiles. Percentiles will allow us to calculate a five number summary of a distribution and introduce a new kind of graph for describing a distribution called the boxplot. Percentiles We have already seen one example of a percentile. The median is the 50th percentile of the distribution. It is the point at which 50% of the observations are lower and 50% are higher. We can actually use this same logic to calculate other percentiles. We could calculate the 25th percentile of the distribution by finding the point where 25% of the observations are below and 75% are above. We could even calculate something like the 43rd percentile if we were so inclined. We calculate percentiles in a fashion similar to the median. First, sort the data from lowest to highest. Then, find the exact observation where X% of the observations fall below to find the Xth percentile. In some cases, there might not be an exact observation that fits this description and so you may have to take the mean across the two closest numbers. The quantile command in R will calculate percentiles for us in this fashion (quantile is a synonym for percentile). In addition to telling the quantile command which variable we want the percentiles of, we need to tell it which percentiles we want. In the command below, I ask for the 27th and 57th percentile of age in our sexual frequency data. quantile(sex$age, p=c(.27,.57)) ## 27% 57% ## 32 46 27% of the sample were younger than 32 years of age and 57% of the sample were younger than 46 years of age. The five number summary We can split our distribution into quarters by calculating the minimum(0th percentile), the 25th percentile, the 50th percentile (the median), the 75th percentile, and the maximum (100th percentile). Collectively, these percentiles are known at the quartiles of the distribution (not to be confused with quantile) and are also described as the five number summary of the distribution. We can calculate these quartiles with the quantile command. If I don’t enter in specific percentiles, the quantile command will give me the quartiles by default: quantile(sex$age) ## 0% 25% 50% 75% 100% ## 18 31 43 56 89 The bottom 25% of respondents are between the ages of 18-31. The next 25% are between the ages of 31-43. The next 25% are between the ages of 43-56. The top 25% are between the ages of 56-89. We can also use this five number summary to calculate the interquartile range (IQR) which is just the difference between the 25th and 75th percentile. This gives us a sense of how spread out observations are. In this data: \\[IQR=56-31=25\\] So, the 25th and 75th percentile of age are separated by 25 years. Boxplots We can also use this five number summary to create another graphical representation of the distribution called the boxplot. Figure 13 below shows a boxplot for the age variable from the sexual frequency data. Figure 13: Boxplot of respondent’s age in sexual frequency data The “box” in the boxplot is drawn from the 25th to the 75th percentile. The height of this box is equal to the interquartile range. The median is drawn as a thick bar within the box. Finally, “whiskers” are then drawn to the minimum and maximum of the data. Sometimes, the whiskers are drawn to less than the minimum and maximum if these values are very extreme and instead the whiskers are drawn out to 1.5xIQR in length and then individual points are plotted. In this case there were no extreme values, so the whiskers were drawn all the way out to the actual maximum and minimum. The boxplot provides many pieces of information. It shows the center of the distribution as measured by the median. It also gives a sense of the spread of the distribution and extreme values by the height of the box and whiskers. It can also show skewness in the distribution depending on where the median is drawn within the box and the size of the whiskers. If the median is in the center of the box, then that indicates a symmetric distribution. If the median is towards the bottom of the box, then the distribution is right-skewed. If the median is towards the top of the box, then the distribution is left-skewed. I have not yet shown you how to make a boxplot using ggplot. The code for Figure 13 is shown below. ggplot(sex, aes(x=&quot;&quot;, y=age))+ geom_boxplot(fill=&quot;skyblue&quot;)+ labs(x=NULL)+ theme_bw() Most of this code is straightforward. We use the y aesthetic to indicate the variable we want the boxplot for and we use the geom_boxplot command to graph the boxplot (in this case the fill argument can be used to specify a color choice for the box of the boxplot). The only unusual thing here is the use of x=\"\" in the top-level aesthetics and the use of x=NULL in the labs command. These additions are not strictly necessary but they do cause the horizontal x-scale on the graph to be suppressed. Otherwise we would see some non-intuitive numbers here. The exercise below allows you to adjust a slider to see different percentiles on both a histogram and a boxplot. In general, boxplots for a single variable do not contain as much information as a histogram and so are generally inferior for understanding the full shape of the distribution. The real advantage of boxplots will come in the next module when we learn to use comparative boxplots to make comparisons of the distribution of a quantitative variable across different categories of a categorical variable. "],
["measuring-the-spread-of-a-distribution.html", "Measuring the Spread of a Distribution", " Measuring the Spread of a Distribution The second most important measure of a distribution is its spread. Spread indicates how far individual values tend to fall from the center of the distribution. As Figure 14 below shows, two distributions can have the same center and general shape (in this case, a bell curve) but have very different spreads. Figure 14: Two different distributions with the same mean but very different spreads, based on simulated data. Range and interquartile range One of the simplest measures of spread is to calculate the range. The range is the distance between the highest and lowest value. Lets take a look at the range in the fare paid (in British pounds) for tickets on the Titanic. The summary command, will give us the information we need: summary(titanic$fare) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 7.896 14.454 33.276 31.275 512.329 Note that at least one person made it on the Titanic for free. The highest fare paid was 512.3 pounds. So the range is easy to calculate 512.3 - 0 = 512.3. The difference between the highest and lowest paying passenger was about 512 pounds. This example also reveals the shortcoming of the range as a measure of spread. If there are any outliers in the data, they are going to show up in the range and so the range may give you a misleading idea of how spread out the values are. Note that the 75th percentile here is only 31.28 pounds, which would suggest that the 512.3 maximum is a pretty high outlier. We can check this by graphing a boxplot, as I have done in Figure 15. ggplot(titanic, aes(x=&quot;&quot;, y=fare))+ geom_boxplot(fill=&quot;seagreen&quot;, outlier.color = &quot;red&quot;)+ labs(x=NULL, y=&quot;fare paid in British pounds&quot;)+ theme_bw() Figure 15: Boxplot of fare paid on the Titanic The maximum value is such an outlier that the rest of the boxplot has to be “scrunched” in order to fit it all into the graph. Clearly this is not a good indicator of spread. However, we have already seen a better measure of spread using a similar idea: the interquartile range or IQR. The IQR is just the range between the 25th and 75th percentile. We already have these numbers from the output above, so the IQR = 31.28-7.90=23.38. So, the difference in fare between the 25th and 75th percentile (the middle 50% of the data) was 23.4 pounds. That result suggests a much smaller spread of fares. You can also use the IQR command in R to directly calculate the IQR. IQR(titanic$fare) ## [1] 23.3792 Variance and standard deviation The most common measure of spread is the variance and its derivative measurement, the standard deviation. It is so common in fact, that most people simply refer to the concept of “spread” as “variance.” The variance can be defined as the “average squared distance to the mean.” Of course, “squared distance” is a bit hard to think about, so we more commonly take the square root of the variance to get the standard deviation which gives us the “average distance to the mean.” Imagine if you were to randomly pick one observation from your dataset and guess how far it would be from the mean. Your best guess would be the standard deviation. The calculation for the variance and standard deviation is a bit intimidating but we will break it down into steps to show it is not that hard. At the same time, I will show you to calculate the parts in R using the fare variable from the Titanic data. The overall formula for the variance (which is represented as \\(s^2\\)) is: \\[s^2=\\frac{\\sum_{i=1}^n (x_i-\\bar{x})^2}{n-1}\\] That looks tough, but lets break it down. The first step is this: \\[(x_i-\\bar{x})\\] You take each value of your variable \\(x\\) and subtract the mean from it. This can be done in R easily: diffx &lt;- titanic$fare-mean(titanic$fare) This measure gives us a description of how far each observation is from the mean which is already kind of a measure of spread, but we can’t do much with it yet because some differences are positive (higher than the mean) and some are negative (lower than the mean). In fact, if we take the mean of these differences, it will be zero by definition because this is what it means for the mean to be the balancing point of the distribution. round(mean(diffx),5) ## [1] 0 The next step is: \\[(x_i-\\bar{x})^2\\] We just need to square the differences. This will get rid of our negative/positive problem, because the squared values will all be positive. diffx.squared &lt;- diffx^2 The next step is: \\[\\sum_{i=1}^n (x_i-\\bar{x})^2\\] We need to sum up all of our values. This value is sometimes called the sum of squared X or SSX for short. It is already pretty close to a measure of variance already. ssx &lt;- sum(diffx.squared) The more distance there is from the mean on average, the larger this value will be. However, it also gets larger when we have more values because we are just taking a sum. To get a number that is comparable across different number of observations, we need to do the final step: \\[s^2=\\frac{\\sum_{i=1}^n (x_i-\\bar{x})^2}{n-1}\\] We are going to divide our SSX value by the number of observations minus one. The “minus one” thing is a bit tricky and I don’t want to get into the details of why we do it here. When n is large, this will have little effect and basically you are taking an average of the squared distance from the mean. variance &lt;- ssx/(length(diffx)-1) variance ## [1] 2677.398 So the average squared distance from the mean fare is 2677.39 pounds squared. Of course, this isn’t a very interpretable number, so its probably better to square root it and get the standard deviation: sqrt(variance) ## [1] 51.74358 So, the average distance from the mean fare is 51.74 pounds. Note that I could have used the power of R to do this entire calculation in one line: sqrt(sum((titanic$fare-mean(titanic$fare))^2)/(length(titanic$fare)-1)) ## [1] 51.74358 Alternatively, I could have just used the sd command to have R do all the heavy lifting: sd(titanic$fare) ## [1] 51.74358 "],
["measuring-association-1.html", "Measuring Association", " Measuring Association Measuring association between variables, in some form or another, is really what social scientists use statistics for the most. Establishing whether two variables are related to one another can help to affirm or cast doubt on theories about the social world. Does substance use improve an adolescent’s popularity in school? Does increasing wealth in a country lead to more or less environmental degradation? Does income inequality inhibit voter participation? These are just a few of the questions one could ask that require a measurement of the association between two or more variables. In this module, we will learn about how to visualize and measure association between two variables. How we do this depends on the kinds of variables that we have. There are three possibilities: two categorical variables one categorical and one quantitative variable two quantitative variables Each of these cases requires that we learn and master different techniques. In the three sections that follow, we will learn how to measure association for all three cases. Slides for this module can be found here. "],
["the-two-way-table.html", "The Two-Way Table", " The Two-Way Table The two-way table (also known as a cross-tabulation or crosstab) gives the joint distribution of two categorical variables. Lets use our politics dataset to construct a two-way table of belief in anthropogenic climate change by political party: tab &lt;- table(politics$party, politics$globalwarm) tab ## ## No Yes ## Democrat 230 1235 ## Republican 577 664 ## Independent 326 1055 ## Other 47 104 The two-way table gives us the joint distribution of the two variables, which is the number of respondents who fell into both categories. For example, we can see that 234 democrats did not believe in anthropogenic climate change while 1230 did. From this table, we can also calculate the marginal distribution of each of the variables, which are just the distributions of each of the variables separately. We can do that by adding up across the rows and down the columns: Table 8: Two-way table of party affiliation by climate change belief, ANES data 2016 Deniers Believers Total Democrat 230 1235 230+1235=1465 Republican 577 664 577+664=1241 Independent 326 1055 326+1055=1381 Other 47 104 47+104=151 Total 230+577+326+47=1180 1235+664+1055+104=3058 4238 The marginal distribution of party affiliation is given by the Total column on the right and the marginal distribution of climate change belief is given by the Total row at the bottom. Looking at the column marginal, we can see that there were a total of 1465 Democrats, 1241 Republicans, and so on. Looking at the row marginal, we can see that there were 1180 anthropogenic climate change deniers and 3058 anthropogenic climate change believers. The final number (4238) in the far right corner is the total number of respondents altogether. You can get this number by summing up the column marginals (1180+3058) or row marginals (1465+1241+1381+151). The margin.table command in R will also calculate marginals for us. I can use the margin.table command on the table I created and saved above as tab to calculate the same marginals as above. Note that you need to indicate which marginal you want by a number, where 1=row and 2=column, as the second option to margin.table: margin.table(tab,1) ## ## Democrat Republican Independent Other ## 1465 1241 1381 151 margin.table(tab,2) ## ## No Yes ## 1180 3058 The two-way table provides us with evidence about the association between two categorical variables. To understand what the association looks like, we will learn how to calculate conditional distributions. Conditional distributions To this point, we have learned about the joint and marginal distributions in a two-way table. In order to look at the relationship between two categorical variables, we need to understand a third kind of distribution: the conditional distribution. The conditional distribution is the distribution of one variable conditional on being in a certain category of the other variable. In a two-way table, there are always two ways to calculate a conditional distribution. In our case, we could look at the distribution of climate change belief conditional on party affiliation, or we could look at the distribution of party affiliation conditional on climate change belief. Both of these distributions really give us the same information about the association, but sometimes one way is more intuitive to understand. In this case, I am going to start with the former case and calculate the distribution of climate change belief conditional on party affiliation. This conditional distribution is basically given by the rows of our two-way table, which give the number of individuals of a given party who fall into each belief category. For example, the distribution of denial/belief among Democrats is 429 and 1932, while among Republicans, this distribution is 708 and 681. However, these two rows are not directly comparable as they are because Republicans are a much smaller group than Democrats. Thus, even if the shares were very different between the two groups, the absolute numbers for Republicans would probably be smaller for both categories. In order to make these rows comparable, we need the proportion of each party that falls into each belief category. In order to do that, we need to divide our rows through by the marginal distribution of party affiliation, like so: Table 9: Calculation of distribution of belief conditional on party affiliation Deniers Believers Total Democrat 230/1465 1235/1465 1465 Republican 577/1241 664/1241 1241 Independent 326/1381 1055/1381 1381 Other 47/151 104/151 151 Note that each row gets divided by its row marginal. If we do the math here, we will come out with the following proportions: ## Warning in base::cbind(...): number of rows of result is not a multiple of ## vector length (arg 2) Table 10: Distribution of belief conditional on party affiliation Deniers Believers Total Democrat 0.157 0.843 1 Republican 0.465 0.535 1 Independent 0.236 0.764 1 Other 0.311 0.689 1 Note that the proportions should add up to 1 within each row because we are basically calculating the share of each row that belongs to each column category. To understand these conditional distributions, you need to look at the numbers within each row. For example, the first row tells us that 15.7% of Democrats are deniers and 84.3% of Democrats are believers. The second row tells us that 46.5% of Republicans are deniers and 53.5% of Republicans are believers. We can tell if there is an association between the row and column variable if these conditional distributions are different across rows. In this case, they are clearly very different. About 84.3% of Democrats are believers while only about half (53.5%) of Republicans are believers. About 76.4% of Independents are believers, while about 68.9% of members of other parties are believers. We can use the prop.table command we saw in the last module to estimate these conditional distributions. In order to do that we feed in the crosstab we calculated with tab and one additional argument that indicates which dimension (row or column ) we want to condition across. Like margin.table a value of 1 will condition on rows (rows sum to 1) and a value of 2 will condition on columns (columns sum to 1). If we condition on rows here, we will get the same table as above. prop.table(tab,1) ## ## No Yes ## Democrat 0.1569966 0.8430034 ## Republican 0.4649476 0.5350524 ## Independent 0.2360608 0.7639392 ## Other 0.3112583 0.6887417 Its important to remember which way you did the conditional distribution and get the interpretation correct. If you are not sure, just note which way the proportions add up to one - this is the direction you should be looking (i.e. within row or column). In this case, I am looking at the distribution of variables within rows, so the proportions refer to the proportion of respondents from a given political party who hold a given belief. But, I could have done my conditional distribution the other way: prop.table(tab,2) ## ## No Yes ## Democrat 0.19491525 0.40385873 ## Republican 0.48898305 0.21713538 ## Independent 0.27627119 0.34499673 ## Other 0.03983051 0.03400916 Note that this table looks deceptively similar to the table above. But look again. The numbers now don’t add up to one within each row. They do however add up to one within each column. In order to read this table properly, we have to understand that it is giving us the distribution within each column: the proportion of respondents who have a given belief who belong to a given political party. So we can see in the first number that 19.5% of deniers are Democrats, 48.2% are Republicans, 27.9% are Independents, and 4.1% belong to other parties. This distribution is very different from the party affiliation distribution of believers in the second column which tells us that there is an association. However, the large party cleavages on the issue are not as immediately obvious here as they were with the previous conditional distribution. Always think carefully about which conditional distribution is more sensible to interpret and always make sure that you are interpreting them in the correct way. It is also possible to graph the conditional distribution as a set of barplots. To do that, we will learn a new feature of ggplot called faceting. Faceting allows us to make the same plot based on subsets of the data in a series of panels. In this case, we can use the code for a univariate barplot but faceted by First, lets save the output of our prop.table into a new object. ggplot(politics, aes(x=globalwarm, y=..prop.., group=1))+ geom_bar()+ facet_wrap(~party)+ scale_y_continuous(labels = scales::percent)+ labs(x=&quot;belief in anthropogenic climate change&quot;, y=NULL)+ theme_bw() Figure 16: Distribution of belief in anthropogenic climate change by party affiliation, ANES 2016 Figure 16 is a comparative barplot. This is our first example of a graph that looks at a relationship. Each panel shows the distribution of climate change beliefs for respondents with that particular party affiliation. What we are interested in is whether the distribution looks different across each panel. In this case, because there were only two categories of the response variable, we only really need to look at the heights of the bar for one category to see the variation across party affiliation, which is substantial. Lets try an example with more than two categories. For this example, I want to know whether there was a difference in passenger class by gender on the Titanic. I start with a comparative barplot: ggplot(titanic, aes(x=pclass, y=..prop.., group=1))+ geom_bar()+ facet_wrap(~sex)+ scale_y_continuous(labels = scales::percent)+ labs(x=&quot;passenger class&quot;, y=NULL)+ theme_bw() Figure 17: Distribution of passenger class by gender I can also calculate the conditional distributions by hand using prop.table: round(prop.table(table(titanic$sex, titanic$pclass),1),3)*100 ## ## First Second Third ## Female 30.9 22.7 46.4 ## Male 21.2 20.3 58.5 As these numbers and Figure 17 both show, the distribution of men and women by passenger class is very different. Women were less likely to be in third class and more likely to be in first class than men, while about the same percent of men and women were in second class. Odds ratio (advanced) We can also use the odds ratio to measure the association between two categorical variables. The odds ratio is not a term that is common in everyday speech but it is a critical concept in all kinds of scientific research. Lets take the different distributions of climate change belief for Democrats and Republicans. About 84% of Democrats were believers, but only 54% of Republicans were believers. How can we talk about how different these two numbers are from one another? We could subtract one from the other or we could take the ratio by dividing one by the other. However, both of these approaches have a major problem. Because the percents (and proportions) have minimum and maximum values of 0 and 100, as you approach those boundaries the differences necessarily have to shrink because one group is hitting its upper or lower limit. This makes it difficult to compare percentage or proportional differences across different groups because the overall average proportion across groups will affect the differences in proportion. Odds ratios are a way to get around this problem. To understand odds ratios, you first have to understand what odds are. All probabilities (or proportions) can be converted to a corresponding odds. If you have a \\(p\\) probability of success on a task, then your odds \\(O\\) of success are given by: \\[O=\\frac{p}{1-p}\\] The odds are basically the ratio of the probability of success to the probability of failure. This tells you how many successes you expect to get for every one failure. Lets say a baseball player gets a hit 25% of the time that the player comes up to bat (an average of 250). The odds are then given by: \\[O=\\frac{0.25}{1-0.25}=\\frac{0.25}{0.75}=0.33333\\] The hitter will get on average 0.33 hits for every one out. Alternatively, you could say that the hitter will get one hit for every three outs. Re-calculating probabilities in this way is useful because unlike the probability, the odds has no upper limit. As the probability of success approaches one, the odds will just get larger and larger. We can use this same logic to construct the odds that a Democratic and Republican respondent, respectively, will be climate change believers. For the Democrat, the probability is 0.843, so the odds are: \\[O=\\frac{0.843}{1-0.843}=\\frac{0.843}{0.157}=5.369\\] Among Democrats, there are 5.369 believers for every one denier. Among Republicans, the probability is 0.541, so the odds are: \\[O=\\frac{0.535}{1-0.535}=\\frac{0.535}{0.465}=1.151\\] Among Republicans, there are 1.151 believers for every one denier. This number is close to “even” odds of 1, which happen when the probability is 50%. The final step here is to compare those two odds. We do this by taking their ratio, which means we divide one number by the other: \\[\\frac{5.369}{1.151}=4.67\\] This is our odds ratio. How do we interpret it? This odds ratio tells us how much more or less likely climate change belief is among Democrats relative to Republicans. In this case, I would say that “the odds of belief in anthropogenic climate change are 4.665 times higher among Democrats than Republicans.” Note the “times” here. This 4.665 is a multiplicative factor because we are taking a ratio of the two numbers. You can calculate odds ratios from conditional distributions just as I have done above, but there is also a short cut technique called the cross-product. Lets look at the two-way table of party affiliation but this time just for Democrats and Republicans. For reasons I will explain below, I am going to reverse the ordering of the columns so that believers come first. Believer Denier Democrat 1235 230 Republican 664 577 The two numbers in blue are called the diagonal and the two numbers in red are the reverse diagonal. The cross-product is calculated by multiplying the two numbers in the diagonal by each other and multiplying the two numbers in the reverse diagonal together and then dividing the former product by the latter: \\[\\frac{1235*577}{664*230}=4.67\\] I get the exact same odds ratio as above without having to calculate the proportions and odds themselves. This is a useful shortcut for calculating odds ratios. The odds ratio that you calculate is always the odds of the first row being in the first column relative to those odds for the second row. Its easy to show how this would be different if I had kept the original ordering of believers and deniers: Denier Believer Democrat 230 1235 Republican 577 664 \\[\\frac{230*664}{577*1235}=0.21\\] I get a very different odds ratio, but that is because I am calculating something different. I am now calculating the odds ratio of being a denier rather than a believer. So I would say that the “the odds of denial of anthropogenic climate change among Democrats are only 22% of the odds for Republicans.” In other words, the odds of being a denier are much lower among Democrats. However, the information here is the same because the 0.21 here is exactly equal to 1/4.67. In other words, the odds ratio of denial is just the inverted mirror image of the odds ratio of belief. Its just important that you remember that when you calculate the cross-product, you are always calculating the odds ratio of being in the category of the first column, whatever category that may be. "],
["mean-differences.html", "Mean Differences", " Mean Differences Measuring association between a quantitative and categorical variable is fairly straightforward. We want to look for differences in the distribution of the quantitative variable at different categories of the categorical variables. For example, if we were interested in the gender wage gap, we would want to compare the distribution of wages for women to the distribution of wages for men. There are two ways we can do this. First, we can graphically examine the distributions using the techniques we have already developed, particularly the boxplot. Second, we can compare summary measures like the mean across categories. Graphically examining differences in distributions We could compare entire histograms of the quantitative variable across different categories of the categorical variable, but this is often too much information. A cleaner method is to use comparative boxplots. Comparative boxplots construct boxplots of the quantitative variable across all categories of the categorical variable and plot them next to each other for easier comparison. We can easily construct a comparative boxplot in ggplot by adding an x aesthetic to our existing boxplot code. Lets try an example looking at differences in the distribution of movie runtime across different movie genres. ggplot(movies, aes(x=Genre, y=Runtime))+ geom_boxplot(fill=&quot;seagreen&quot;, outlier.color = &quot;red&quot;)+ labs(x=NULL, y=&quot;movie runtime in minutes&quot;)+ theme_bw() Figure 18: Boxplots of movie runtime by genre This plot is a good start, but I am running into a problem where the genre labels are running into each other on the x-axis because they are too long. We can solve this problem very easily by using the coord_flip command to flip the axis: ggplot(movies, aes(x=Genre, y=Runtime))+ geom_boxplot(fill=&quot;seagreen&quot;, outlier.color = &quot;red&quot;)+ labs(x=NULL, y=&quot;movie runtime in minutes&quot;)+ coord_flip()+ theme_bw() Figure 19: Boxplots of movie runtime by genre, with coordinates flipped Now my genre labels are much easier to read. However, there is one more addition I can make to this graph in order to improve its readability. I want to order my genre categories so that they are ordered from largest to smallest median runtime on the graph. I can do this by applying the reorder command directly within ggplot. The reorder command takes three arguments. The first argument is the categorical variable to be reordered (genre in my case). The second variable is the variable that reordering should be based upon (runtime in my case). The third argument is the mathematical function that will be used for sorting (median in my case). The full command looks like: ggplot(movies, aes(x=reorder(Genre, Runtime, median), y=Runtime))+ geom_boxplot(fill=&quot;seagreen&quot;, outlier.color = &quot;red&quot;)+ labs(x=NULL, y=&quot;movie runtime in minutes&quot;)+ coord_flip()+ theme_bw() Figure 20: Boxplots of movie runtime by genre, with coordinates flipped Figure 20 now contains a lot of information. At a glance, I can see which genres had the longest median runtime and which had the shortest. But I can also see how much variation in runtime there is within movies by comparing the width of the boxes. For example, I can see that there is relatively little variation in runtime for animation movies, and the most variation in runtime among sci-fi/fantasy and action movies. I can also see some of the extreme outliers by genre. Most of these outliers are for extremely long movies, relative to the genre norm, but in a couple of cases, I can see movies that were remarkably short for their genre. Comparing differences in the mean We can also establish a relationship by looking at differences in summary measures. Implicitly, we are already doing this in the comparative boxplot when we look at the median bars across categories. However, in practice it is more common to compare the mean of the quantitative variable across different categories of the categorical variable. In R, you can get the mean of one variable at different levels of a categorical variable using the tapply command like so: tapply(movies$Runtime, movies$Genre, mean) ## Action Animation Comedy Drama Family ## 111.91304 90.06475 100.45711 112.65060 102.60000 ## Horror Musical/Music Mystery Romance SciFi/Fantasy ## 97.39545 108.46602 114.25641 109.89855 112.56202 ## Thriller ## 110.13260 The tapply command takes three arguments. The first argument is the quantitative variable for which we want means. The second argument is the categorical variable. The third argument is the method we want to run on the quantitative variable, in this case the mean. The output is the mean movie runtime by genre. If we want a quantitative measure of how genre and runtime are related we can calculate a mean difference. This number is simply the difference in means between any two categories. For example, action movies are 111.9 minutes long, on average, while horror movies are 97.4 minutes long, on average. The difference is \\(111.9-97.4=14.5\\). Thus, we can say that, on average, action movies are 14.5 minutes longer than horror movies. We could have reversed that subtraction to get \\(97.4-111.9=-14.5\\). In this case, we would say that horror movies are 14.5 minutes shorter than action movies. Either way, we get the same information. However, its important to keep track of which number applies to which category when you take the difference, so that you get the interpretation correct. We can also display these results graphically using a barplot, although this will take a little more processing for ggplot because ggplot expects data to be in a “data.frame” object and the output of tapply is a single vector of numbers. To make this work, we have to use the as.data.frame.table command to convert our object and then rename the variables. Also, to make this prettier, I am first going to sort the output from largest to smallest mean runtime: mruntime &lt;- tapply(movies$Runtime, movies$Genre, mean) #sort highest to lowest mruntime &lt;- sort(mruntime, decreasing=FALSE) #convert to data.frame mruntime &lt;- as.data.frame.table(mruntime) #rename variables colnames(mruntime) &lt;- c(&quot;genre&quot;,&quot;runtime&quot;) ggplot(mruntime, aes(x=genre, y=runtime))+ geom_col()+ coord_flip()+ labs(x=NULL, y=&quot;runtime in minutes&quot;)+ theme_bw() Figure 21: Mean runtime by movie genre This is is one of the few examples this term where we have to process a dataset into something else prior to feeding it into ggplot. The result is shown in Figure 21. While this information is useful, it doesn’t really tell us anything that we haven’t already seen in the comparative boxplot from Figure 20. The only real difference is that the boxplots showed us medians while this figure shows us means. However, the comparative boxplots also showed us additional information about spread and outliers and so are generally preferable. Figure 21 also breaks a common stylistic rule in data visualization. The big thick bars take up a lot of ink but carry relatively little information. This is called the “ink to information ratio” made famous by Edward Tufte. An alternative way to display this information would be to use “lollipops” rather than bars: ggplot(mruntime, aes(x=genre, y=runtime))+ geom_lollipop()+ coord_flip()+ labs(x=NULL, y=&quot;runtime in minutes&quot;)+ theme_bw() Figure 22: Using a lollipop graph to display ean runtime by movie genre is a lot easier on the eye and the ink cartridge "],
["scatterplot-and-correlation-coefficient.html", "Scatterplot and Correlation Coefficient", " Scatterplot and Correlation Coefficient The techniques for looking at the association between two quantitative variables are more developed than the other two cases, so we will spend more time on this topic. Additionally, the major approach here of ordinary least squares regression turns out to be a very flexible, extendable method that we will build on later in the term. When examining the association between two quantitative variables, we usually distinguish the two variables by referring to one variable as the dependent variable and the other variable as the independent variable. The dependent variable is the variable whose outcome we are interested in predicting. The independent variable is the variable that we treat as the predictor of the dependent variable. For example, lets say we were interested in the relationship between income inequality and life expectancy. We are interested in predicting life expectancy by income inequality, so the dependent variable is life expectancy and the independent variable is income inequality. The language of dependent vs. independent variable is causal, but its important to remember that we are only measuring the association. That association is the same regardless of which variable we set as the dependent and which we set as the independent. Thus, the selection of the dependent and independent variable is more about which way it more intuitively makes sense to interpret our results. The scatterplot We can examine the relationship between two quantitative variables by constructing a scatterplot. A scatterplot is a two-dimensional graph. We put the independent variable on the x-axis and the dependent variable on the y-axis. For this reason, we often refer generically to the independent variable as x and the dependent variable generically as y. To construct the scatterplot, we plot each observation as a point, based on the value of its independent and dependent variable. For example, lets say we are interested in the relationship between the median age of the state population and violent crime in our crime data. Our first observation, Alabama, has a median age of 37.8 and a violent crime rate of 378 crimes per 100,000. this is plotted in 23 below. Figure 23: Starting a scatterplot by plotting the firest observation If I repeat that process for all of my observations, I will get a scatterplot that looks like: Figure 24: Scatterplot of median age by the violent crime rate for all US states What are we looking for when we look at a scatterplot? There are four important questions we can ask of the scatterplot. First, what is the direction of the relationship. We refer to a relationship as positive if both variables move in the same direction. if y tends to be higher when x is higher and y tends to be lower when x is lower, then we have a positive relationship. On the other hand, if the variables move in opposite directions, then we have a negative relationship. If y tends to be lower when x is higher and y tends to be higher when x is lower, then we have a negative relationship. In the case above, it seems like we have a generally negative relationship. States with higher median age tend to have lower violent crime rates. Second, is the relationship linear? I don’t mean here that the points fall exactly on a straight line (which is part of the next question) but rather does the general shape of the points appear to have any “curve” to it. If it has a curve to it, then the relationship would be non-linear. This issue will become important later, because our two primary measures of association are based on the assumption of a linear relationship. In this case, there is no evidence that the relationship is non-linear. Third, what is the strength of the relationship. If all the points fall exactly on a straight line, then we have a very strong relationship. On the other hand, if the points form a broad elliptical cloud, then we have a weak relationship. In practice, in the social sciences, we never expect our data to conform very closely to a straight line. Judging the strength of a relationship often takes practice. I would say the relationship above is of moderate strength. Fourth, are there outliers? We are particularly concerned about outliers that go against the general trend of the data, because these may exert a strong influence on our later measurements of association. In this case, there are two clear outliers, Washington DC and Utah. Washington DC is an outlier because it has an extremely high level of violent crime relative to the rest of the data. Its median age tends to be on the younger side, so its placement is not inconsistent with the general trend. Utah is an outlier that goes directly against the general trend because it has one of the lowest violent crime rates and the youngest populations. This is, of course, driven by Utah’s heavily Mormon population, who both have high rates of fertility (leading to a young population) and whose church communities are able to exert a remarkable degree of social control over these young populations. Constructing scatterplots in R You can construct scatterplots in ggplot by using the geom_point geometry. You just need to define the aesthetics for x (on the x-axis) and y (on the y-axis). ggplot(crimes, aes(x=Poverty, y=Property))+ geom_point()+ labs(x=&quot;poverty rate&quot;, y=&quot;property crimes per 100,000 population&quot;)+ theme_bw() Figure 25: Scatterplot of a state’s poverty rate by property crime rate, for all US states Sometimes with large datasets, scatterplots can be difficult to read because of the problem of overplotting. This happens when many data points overlap, so that its difficult to see how many points are showing. For example: ggplot(movies, aes(x=Runtime, y=TomatoMeter))+ geom_point()+ labs(x=&quot;movie runtime in minutes&quot;, y=&quot;Percent of reviews that are positive&quot;)+ theme_bw() Figure 26: An example of the problem of overplotting where points are being plotted on top of each other Because so many movies are in the 90-120 minute range, it is difficult to see the density of points in this range and thus difficult to understand the relationship. One way to address this is to allow the points to be semi-transparent, so that as more and more points are plotted in the same place, the shading will become darker. We can do this in geom_point by setting the alpha argument to something less than one but greater than zero: ggplot(movies, aes(x=Runtime, y=TomatoMeter))+ geom_point(alpha=0.3)+ labs(x=&quot;movie runtime in minutes&quot;, y=&quot;Percent of reviews that are positive&quot;)+ theme_bw() Figure 27: Using a semi-transparent point will help us see areas where there is a dense tangle of points Overplotting can also be a problem with discrete variables because these variables can only take on certain values which will then exactly overlap with one another. This can be seen in Figure 28 which shows the relationship between age and wages in the earnings data. ggplot(earnings, aes(x=age, y=wages))+ geom_point()+ labs(x=&quot;age&quot;, y=&quot;hourly wage&quot;)+ theme_bw() Figure 28: Age is discrete so the scatterplot looks like a bunch of vertical lines of dots and is very hard to understand Because age is discrete all points are stacked up in vertical bars making it difficult to understand what is going on. One solution for this problem is to “jitter” each point a little bit by adding a small amount of randomness to the x and y values. The randomness added won’t affect our sense of the relationship but will reduce the issue of overplotting. We can do this simply in ggplot by replacing the geom_point command with geom_jitter. ggplot(earnings, aes(x=age, y=wages))+ geom_jitter()+ labs(x=&quot;age&quot;, y=&quot;hourly wage&quot;)+ theme_bw() Figure 29: Jittering helps with overplotting but its still difficult to see how dense points are for most of the plot Jittering helped get rid of those vertical lines, but there are so many observations that we still have problems of understanding the density of points for most of the plot. If we add an alpha parameter to the geom_jitter command, we should be better able to understand the scatterplot. ggplot(earnings, aes(x=age, y=wages))+ geom_jitter(alpha=0.01)+ labs(x=&quot;age&quot;, y=&quot;hourly wage&quot;)+ theme_bw() Figure 30: Jittering and transparency help us to make sense of the relationship between age and wages The correlation coefficient We can measure the association between two quantitative variables with the correlation coefficient, r. The formula for the correlation coefficient is: \\[r=\\frac{1}{n-1}\\sum_{i=1}^n (\\frac{x_i-\\bar{x}}{s_x}*\\frac{y_i-\\bar{y}}{s_y})\\] That looks complicated, but lets break it down step by step. We will use the association between median age and violent crimes as our example. The first step is to subtract the means from each of our x and y variables. This will give us the distance above or below the mean for each variable. diffx &lt;- crimes$MedianAge-mean(crimes$MedianAge) diffy &lt;- crimes$Violent-mean(crimes$Violent) The second step is to divide these differences from the mean of x and y by the standard deviation of x and y, respectively. diffx.sd &lt;- diffx/sd(crimes$MedianAge) diffy.sd &lt;- diffy/sd(crimes$Violent) Now each of your x and y values have been converted from their original form into the number of standard deviations above or below the mean. This is often called standarization. By doing this, we have put both variables on the same scale and have removed whatever original units they were measured in (in our case, years of age and crimes per 100,000). The third step is to to multiply each converted value of x by each converted value of y. product &lt;- diffx.sd*diffy.sd Why do we do this? First consider this scatterplot of our standardized x and y: Figure 31: Where a point falls in the four quadrants of this scatterplot indicate whether it provides evidence for a positive or negative relationship Points shown in blue have either both positive or both negative x and y values. When you take the product of these two numbers, you will get a positive product. This is evidence of a positive relationship. Points shown in red have one positive and one negative x and y value. When you take the product of these two numbers, you will get a negative product. This is evidence of a negative relationship. The final step is to add up all this evidence of a positive and negative relationship and divide by the number of observations (minus one). sum(product)/(length(product)-1) ## [1] -0.3015232 This final value is our correlation coefficient. We could have also calculated it by using the cor command: cor(crimes$MedianAge, crimes$Violent) ## [1] -0.3015232 How do we interpret this correlation coefficient? It turns out the correlation coefficient r has some really nice properties. First, the sign of r indicates the direction of the relationship. If r is positive, the association is positive. If r is negative, the association is negative. if r is zero, there is no association. Second, r has a maximum value of 1 and a minimum value of -1. These cases will only happen if the points line up exactly on a straight line, which never happens with social science data. However, it gives us some benchmark to measure the strength of our relationship. Figure 32 shows simulated scatterplots with different r in order to help you get a sense of the strength of association for different values of r. Figure 32: Strength of association for various values of the correlation coefficient, based on simulated data Third, r is a unitless measure of association. It can be compared across different variables and different datasets in order to make a comparison of the strength of association. For example, the correlation coefficient between unemployment and violent crimes is 0.45. Thus, violent crimes are more strongly correlated with unemployment than with median age (0.44&gt;0.30). The association between median age and property crimes is -0.36, so median age is more strongly related to property crimes than violent crimes (0.36&gt;0.30). There are some important cautions when using the correlation coefficient. First, the correlation coefficient will only give a proper measure of association when the underlying relationship is linear. if there is a non-linear (curved) relationship, then r will not correctly estimate the association. Second, the correlation coefficient can be affected by outliers. We will explore this issue of outliers and influential points more in later sections. "],
["statistical-inference.html", "Statistical Inference", " Statistical Inference Now that we have the basics of examining data down, we turn to another issue that we can address with statistical analysis. Howe confident are we that the the results from the data represent the larger population from which the data are drawn? This issue only applies to cases where the data we use constitutes a sample from a larger population. However, many of the datasets that we work with in the social sciences are of this type, so this is typically an important issue. We don’t want to to reach an incorrect conclusion that \\(x\\) is associated with \\(y\\) in cases when that association in our sample is basically a result of random chance. In many introductory statistics courses, statistical inference would take up the majority of the course and you would learn a variety of cookbook formulas for conducting “tests.” We won’t do much of that here. Instead I will focus on the logic of the two most common procedures in statistical inference: the confidence interval and the hypothesis test. Once you understand the logic behind these procedures, it turns out that all of the various “tests” are just iterations on the same basic theme. Nonetheless, we will have to use some formulas in this module with associated number crunching. This is the most math heavy module of the course, so be prepared. "],
["the-problem-of-statistical-inference.html", "The Problem of Statistical Inference", " The Problem of Statistical Inference So far, we have only been looking at measurements from our actual datasets. We examined both univariate statistics like the mean, median, and standard deviation, as well as measures of association like the mean difference, correlation coefficient and OLS regression line slope. We can use this measures to draw conclusions about our data. In many cases, the dataset that we are working is only a sample from some larger population. Importantly, we don’t just want to know something about the sample, but rather we want to know something about the population from which the sample was drawn. For example, when polling organizations like Gallup conduct political polls of 500 people, they are not drawing conclusions about just those 500 people, but rather about the whole population from which those 500 people were sampled. To take another example from our General Social Survey (GSS) data on sexual frequency. We can calculate the mean sexual frequency by marital status: tapply(sex$sexf, sex$marital, mean) ## Married Widowed Divorced Separated Never married ## 56.094106 9.222628 41.388720 55.652778 53.617704 Married respondents had sex 2.5 (56.1-53.6) times more per year than never married individuals. We don’t want to draw this conclusion just for our sample. Rather, we want to know what the relationship is between marital status and sexual frequency in the US population as a whole. In other words, we want to infer from our sample to the population. Figure 33 shows this process graphically. Figure 33: The process of making statistical inferences The large blue rectangle is the population that we want to know about. Within this population, there is some value that we want to know. In this case, that value is the mean difference in sexual frequency between married and never married individuals. We refer to this unknown value in the population as a parameter. You will also notice that there are some funny-looking Greek letters in that box. We always use Greek symbols to represent values in the population. In this case, \\(\\mu_1\\) is the population mean of sexual frequency for married individuals and \\(\\mu_2\\) is the population mean of sexual frequency for never married individuals. Thus, \\(\\mu_1-\\mu_2\\) is the population mean difference in sexual frequency between married and never married individuals. We typically don’t have data on the entire population, which is why we need to draw a sample in the first place. Therefore, these population parameters are unknown. To estimate what they are, we draw a sample as shown by the smaller yellow square. In this sample, we can calculate the sample mean difference in sexual frequency between married and never married individuals, \\(\\bar{x}_1-\\bar{x}_2\\). We refer to a measurement in the sample as a statistic. We represent these statistics with roman letters to distinguish them from the corresponding value in the population. The statistic is always an estimate of the parameter. In this case, \\(\\bar{x}_1-\\bar{x}_2\\) is an estimate of \\(\\mu_1-\\mu_2\\). We can infer from the sample to the population and conclude that our best guess as to the true mean difference in the population is the value we got in the sample. The sample mean difference may be our best guess as to the true value in the population, but how confident are we in that guess? Intuitively, if I only had a sample of 10 people I would be much less confident than if I had a sample of 10,000 people. Statistical inference is the technique of quantifying our uncertainty in the estimate. If you have ever read the results of a political poll, you will be familiar with the term “margin of error.” This is a measure of statistical inference. Why might our sample produce inaccurate results? There are two sources of bias that could result in our sample statistic being different from the true population parameter. The first form of bias is systematic bias. Systematic bias occurs when something about the procedure for generating the sample produces a systematic difference between the sample and the population. Sometimes, systematic bias results from the way the sample is drawn. For example, if I sample my friends and colleagues on their voting behavior, I will likely introduce very large systematic bias in my estimate of who will win an election because my friends and colleagues are more likely than the general population to hold similar views to my own. Systematic bias can also result from the way questions are worded, the characteristics of interviewers, the time of day interviews are conducted, etc. Systematic bias can often be minimized in well-designed and executed scientific surveys. Statistical inference cannot do anything to account for systematic bias. The second form of bias is random bias. Random bias occurs when the sample statistic is different from the population parameter, just by random chance due to the actual sample that was drawn. In other words, even if there is no systematic bias in my survey design, I can get a bad estimate simply due to the bad luck of drawing a really unusual sample. Imagine that I am interested in estimating mean wealth in the United States and I happen to draw Bill Gates in my sample. I will dramatically overestimate mean wealth in the US. Random bias affects every sample, regardless of how well-designed and executed. In practice, the sample statistic is extremely unlikely to be exactly equal to the population parameter, so some degree of random bias is always present in every sample. However, this random bias will become less important as the sample size increases. In the previous example, Bill Gates is going to bias my results much more if I draw a sample of 10 people, than if I draw a sample of 100,000 people. Our goal with statistical inference is to more precisely quantify how bad that random bias could be in our sample. Notice the word “could” in the previous sentence. The tricky part about statistical inference is that while we know that random bias could be causing our sample statistic to be very different from the population parameter, we never know for sure whether random bias had a big effect or a small effect in our particular sample, because we don’t have the population parameter with which we could compare it. Keep this issue in mind in the next sections, as it plays a key role in how we understand our procedures of statistical inference. It is also important to keep in mind that statistical inference only works when you are actually drawing a sample from a larger population that you want to draw conclusions about. In some cases, our data either constitute a unique event, as in the Titanic case, that cannot be properly considered a sample of something larger or the data actually constitute the entire population of interest, as is the case in our dataset on movies. Although you will occasionally still see people use inferential measures on such data, it is technically inappropriate because there is no larger population to make inferences about. "],
["the-concept-of-the-sampling-distribution.html", "The Concept of the Sampling Distribution", " The Concept of the Sampling Distribution Lets say that you want to know the mean years of education of US adults. You implement a well-designed representative survey that samples 100 respondents from the USA. You ask people the simple question “how many years of education do you have?” You then calculate the mean years of education in your sample. This simple example involves three different kinds of distributions. Understanding the difference between these three different distributions is the key to unlocking how statistical inference works. The Population Distribution. This is the distribution of years of education in the entire US population. The mean of this distribution is given by \\(\\mu\\) and is a population parameter. If we had data on the entire population we could show the distribution and calculate \\(\\mu\\). However, because we don’t have data on the full population, the population distribution and \\(\\mu\\) are unknown. This distribution is also static - it doesn’t fluctuate. The Sample Distribution. This is the distribution of years of education in the sample of 100 respondents that I have. The mean of this distribution is \\(\\bar{x}\\) and is a sample statistic. Since we collected this data, this distribution and \\(\\bar{x}\\) are known. We can calculate \\(\\bar{x}\\) and we can show the distribution of years of education in the sample (with a histogram or boxplot, for example). The sample distribution is an approximation of the population distribution, but because of random bias, it may be somewhat different. Also, because of this random bias, the distribution is not static - if we were to draw another sample the two sample distributions would almost certainly not be identical. The Sampling Distribution. Imagine all the possible samples of size 100 that I could have drawn from the US population. Its a tremendously large number. If I had all those samples, I could calculate the sample mean of years of education for each sample. Then, I would have the mean years of education in every possible sample of size 100 that I could have drawn from the population. The sampling distribution is the distribution of all of these possible sample means. More generally, the sampling distribution is the distribution of the desired sample statistic in all possible samples of size \\(n\\). The sampling distribution is much more abstract than the other two distributions, but is key to understanding statistical inference. When we draw a sample and calculate a sample statistic from this sample, we are in effect reaching into the sampling distribution and pulling out a value. Therefore, the sampling distribution give us information about how variable our sample statistic might be as a result of randomness in the sampling. Example: class height As an example, lets use some data on a recent class that took this course. I will treat this class of 42 students as a population that I would like to know something about. In this case, I would like to know the mean height of the class. In most cases, the population distribution is unknown but in this case, I know the height of all 42 students because of surveys they all took at the beginning of the term. Figure 34 shows the population distribution of height in the class: Figure 34: Population distribution of height in a class of students The population distribution of height is bimodal which is typical, because we are mixing the heights of men and women. The population mean of height (\\(\\mu\\)) is 66.517 inches. Lets say I lost the results of the student survey and I wanted to know the mean height of the class. I could take a random sample of two students in order to calculate the mean height. Lets say I drew a sample of two people who were 68 and 74 inches respectively in height. I would estimate a sample mean of 71 inches which in this case would be too high. Lets say I took another sample of two students and ended up with a mean height of 66 which would be too low. Lets say I repeat this procedure until I had sampled all possible combinations of two students out of the twenty in the class. How many samples would this be? On the first draw from the population of 42 students there are 42 possible results. On the second draw, there are 41 possible results, because I won’t re-sample the student I selected the first time. This gives me 42*41=1722 possible combinations of 42 students in two draws. However, half of these draws are just mirror images of the other draws where I swap the first and second draw. Since I don’t care about the order, I actually have 1722/2=861 possible samples. I have used a computer routine to actually calculate the sample means in all 861 of those possible samples. The distribution of these sample means then gives us the sampling distribution of mean height for samples of size 2. Figure 35 shows a histogram of that distribution. Figure 35: The sampling distribution of class height for samples of size 2 When I randomly draw one sample of size 2 and calculate its mean, I am effectively reaching into this distribution and pulling out one of these values. Note that many of the means are clustered around the true population mean of 66.5 inches, but in a few cases I can get extreme overestimates or extreme underestimates. What if I were to increase the sample size? I have used the same computer routine to calculate the sampling distribution for samples of size 3, 4, and 5. Figure 36 shows the results. Figure 36: The sampling distribution of class height for samples of various sizes Clearly, the shape of these distributions is changing. Another way to visualize this would be to draw density graphs which basically fit a curve to the histograms. We can then then overlap these density curves on the same graph. Figure 37 shows this graph. Figure 37: The sampling distribution of class height for samples of various sizes. The vertical red line shows the true population mean. There are three thing to note here. First, each sampling distribution seems to have most of the sample means clustered (where the peak is) around the true population mean of 66.5. In fact, the mean of each of these sampling distributions is exactly the true population parameter, as I will show below. Second, the spread of the distributions is shrinking as the sample size increases. You can see that the when the sample size increases, the tails of the distribution are “pulled in” and more of the sample means are closer to the center. This indicates that we are less likely to draw a sample mean that is extremely different from the population mean in larger samples. Third, the shape of the distribution at larger sample sizes is becoming more symmetric and “bell-shaped.” Lets take a look at the mean and standard deviation of these sampling distributions: Table 11: Mean and standard deviation of height for the population of students in a class as well as from several sampling distributions of that population Distribution Mean Standard Deviation Population Distribution 66.517 4.87 Sampling Distribution (n=2) 66.517 3.363 Sampling Distribution (n=3) 66.517 2.71 Sampling Distribution (n=4) 66.517 2.316 Sampling Distribution (n=5) 66.517 2.044 Note that the mean of each sampling distribution is identical to the true population mean. This is not a coincidence. The mean of the sampling distribution of sample means is always itself equal to the population mean. In statistical terminology, this is the definition of an unbiased statistic. Given that we are trying to estimate the true population mean, it is reassuring that the “average” sample mean we should get is the true population mean. Also note that the standard deviation of the sampling distributions gets smaller with increasing sample size. This is the mathematically way of seeing the shrinking of the spread that we observed graphically. In larger sample sizes, we are less likely to draw a sample mean far away from the true population mean. Central limit theorem and the normal distribution The patterns we are seeing here are well known to statisticians. In fact, they are patterns that are predicted by the most important theorem in statistics, the central limit theorem. We won’t delve into the technical details of this theorem. We can generally interpret the central limit theorem to say: As the sample size increases, the sampling distribution of a sample mean becomes a normal distribution. This normal distribution will be centered on the true population mean \\(\\mu\\) and with a standard deviation equal to \\(\\sigma/\\sqrt{n}\\), where \\(\\sigma\\) is the population standard deviation. What is this “normal” distribution? The name is somewhat misleading because there is nothing particularly normal about the normal distribution. Most real-world distributions don’t look normal, but the normal distribution is central to statistical inference because of the central limit theorem. The normal distribution is a bell-shaped, unimodal, symmetric distribution. It has two characteristics that define its exact shape. The mean of the normal distribution define where its center is and the standard deviation of the normal distribution defines its spread. Lets look at the normal sampling distribution of the mean to become familiar with it. (#fig:normal_dist)The Normal Distribution The distribution is symmetric and bell shaped. The center of the distribution is shown by the red dotted line. This center will always be at the true population mean, \\(\\mu\\). The area of the normal distribution also has a regularity that is sometimes referred to as the “68%,95%,99.7%” rule. Regardless of the actual variable, 68% of all the sample means will be within 68% of the true population mean, 95% of all the sample means will be within 95% of the true population mean, and 99.7% of all the sample means will be within three standard deviations of the mean. This regularity will become very helpful later on for making statistical inferences. The standard error It is easy to get confused by the number of standard deviations being thrown around in this section. There are three standard deviations we need to keep track of to properly understand what is going on here. Each of these standard deviations is associated with one of the types of distributions I introduced at the beginning of this section. \\(\\sigma\\): the population standard deviation. In our example, this would be the standard deviation of height for all 42 students which is 4.8702139. Typically, like other values in the population, this number is unknown. \\(s\\): the sample standard deviation. In our example, this would be the standard deviation of height from a particular sample of a given size from the class. This number can be calculated for the sample that you have, using the techniques we learned earlier in the class. \\(\\sigma/\\sqrt{n}\\): The standard deviation of the sampling distribution of the sample mean. We divide the population standard deviation \\(\\sigma\\) by the square root of the sample size. In general, we refer to the standard deviation of the sampling distribution as the standard error, for short. So remember that when I refer to the “standard error” I am using shorthand for the “standard deviation of the sampling distribution.” Other sample statistics In the example here, I have focused on the sample mean as the sample statistic of interest. However, the logic of the central limit theorem applies to several other important sample statistics of interest to us. In particular, the sampling distributions of: means proportions mean differences proportion differences correlation coefficients all become normal as the sample size increases. Thus, this normal distribution becomes critically important in making statistical inferences. Note that the standard error formula \\(\\sigma/\\sqrt{n}\\) only applies to the sampling distribution of sample means. Other sample statistics have different formulas for their standard errors, which I will introduce in the next section. What can we do with the sampling distribution? Now that we know the sampling distribution of the sample mean should be normally distributed, what can we do with that information? The sampling distribution gives us information about how we would expect the sample means to be distributed. This seems like it should be helpful in figuring out whether we got a value close to the center or not. However, there is a catch. We don’t know either \\(\\mu\\), the center of this distribution or \\(\\sigma\\) which we need to calculate its standard deviation. Thus, we know theoretically what it should look like but we have no concrete numbers to determine its actual shape. We can fix the problem with not knowing \\(\\sigma\\) fairly easily. We don’t know \\(\\sigma\\) but we do have an estimate of it in \\(s\\), the sample standard deviation. In practice, we us this value to calculate an estimated standard error of \\(s/\\sqrt{n}\\). However, this substitution has consequences. Because we are using a sample statistic subject to random bias to estimate the standard error, this creates greater uncertainty in our estimation. I will come back to this issue in the next section. We still have the more fundamental problem that we don’t know where the center of the sampling distribution should be. In order to make statistical inferences, we are going to employ two different methods that make use of what we do know about the sampling distribution: Confidence intervals: Provide a range of values within which you feel confident that the true population mean resides. Hypothesis tests: Play a game of make believe. If the true population mean was a given value, what is the probability that I would get the sample mean value that I actually did? I will discuss these two different methods in the next two sections. "],
["confidence-intervals.html", "Confidence Intervals", " Confidence Intervals As Figure 38 shows, 95% of all the possible sample means will be within 1.96 standard errors of the true population mean \\(\\mu\\). Figure 38: 95% of all sample means will be within 1.96 standard errors of the true population mean. Lets say I were to construct the following interval for every possible sample: \\[\\bar{x}\\pm1.96(\\sigma/\\sqrt{n})\\] It follows from the statement above that for 95% of all samples, this interval would contain the true population mean, \\(\\mu\\). To see how this works graphically, imagine constructing this interval for twenty different samples from the same population. Figure 39 shows the mean and interval for each of these twenty samples, relative to the true population mean. Figure 39: Intervals of sample mean plus and minus 1.96 standard errors for 20 different samples, with the true population mean shown by the blue line. Sample means shown by dots. You can see that these sample means fluctuate around the true population mean due to random sampling error. The lines give the interval outlined above. In 19 out of the 20 cases, this interval contains the true population mean (as you can see by the fact that the interval crosses the blue line). The one sample where this is not true is shown in red. On average, 95% of samples will contain the true population mean in the interval, so 5% or 1 in 20 will not. We refer to this interval as the 95% confidence interval. Of course, in practice, we only construct one interval on the sample that we have. We use this interval to give a range of values that we feel “confident” will contain the true population mean. What do we mean by “confident?” The term “confident” is a little ambiguous. Given my statements above, it might be tempting to interpret the 95% confidence interval to mean that there is a 95% probability that the true population mean is within the interval. This interpretation seems intuitive and straightforward, but that interpretation is incorrect according to the classic approach to inference. The problem here is subtle, but from the classical viewpoint, probability is an objective phenomenon that relates to the outcomes of future processes over the long run. From this viewpoint, we cannot express our subjective uncertainties about numbers in terms of probabilities.The population mean is a single static number. This leads us to a sort of yoda-like statement: The population mean is either in your interval or it is not. There is no probability. This is why we use a more neutral term like “confidence.” If we want to be long-winded about it, we might say that we are 95% confident because “in 95% of all the possible samples I could have drawn, the true population mean would be in the interval. I don’t know if I have one of those 95% or the unlucky 5%, but nonetheless, there it is.” If this all seems a bit confusing, you are perfectly normal. As I said, this is the classic view of probability. Intuitively, people often think of uncertainty in probabilistic terms (e.g. what are the odds your team will win the game?). Many contemporary statisticians would in fact agree that it is perfectly okay to express subjective uncertainty as a probability. But, I still need to let you know that from the classic approach, interpreting your confidence interval as a probability statement is a no-no. Calculating the confidence interval for the sample mean Okay, lets try calculating a confidence interval. Lets try this out for age in the politics dataset. The formula is: \\[\\bar{x}\\pm1.96(\\sigma/\\sqrt{n})\\] Oh wait, we can’t do it! We don’t know the value of the population standard deviation \\(\\sigma\\). As I explained in the last section, we are going to have to do a little “fudging” here. Instead of \\(\\sigma\\), we can use our sample standard deviation \\(s\\). However, doing so will have consequences. Here is our new formula: \\[\\bar{x} \\pm t*(s\\sqrt{n})\\] As you can see, I have replaced the 1.96 with some number \\(t\\), referred to as the t-statistic. Basically to adjust for the greater uncertainty in using a sample statistic in my calculation of the standard error, I need to increase the number here slightly from 1.96. How much I increase it will depend on the degrees of freedom which are given by the sample size minus one (\\(n-1\\)). To figure out the correct t-statistic, I can use the qt command in R. qt(0.975, nrow(politics)-1) ## [1] 1.960524 The first command to qt is the confidence you want. This is a little bit tricky because for a 95% confidence interval, we actually want to input 0.975. This is because we are basically asking for only the upper tail of that normal distribution shown at the beginning of this section. This area contains only 2.5% of the area outside, with the other 2.5% being in the lower tail. The second number is the degrees of freedom which equals \\(n-1\\). In this case, we have such a large sample, that the t-statistic we need is very close to 1.96. In smaller samples, using the t-statistic rather than 1.96 can make a bigger difference. Its not a proper sample, but lets take the case of the crime data. Here there are only 51 observations, so the t-statistic is: qt(0.975, 51-1) ## [1] 2.008559 The difference from 1.96 is a little more noticeable. Lets return to the politics data. We now have all the information we need to calculate the 95% confidence interval: xbar &lt;- mean(politics$age) sdx &lt;- sd(politics$age) n &lt;- nrow(politics) se &lt;- sdx/sqrt(n) t &lt;- qt(0.975,n-1) xbar+t*se ## [1] 50.03165 xbar-t*se ## [1] 48.97307 We are 95% confident that the mean age among all US adults is between 48.97 and 50.03 years of age. As you can see, the large sample of nearly 6,000 respondents produces a very tight confidence interval. Calculating the confidence interval for other sample statistics As noted in the previous section, the sampling distribution of other sample statistics such as proportions, mean differences, and regression slopes is also normally distributed in large enough samples. This means that we can use the same approach to construct confidence intervals for other sample statistics. The general form of the confidence interval is: \\[\\texttt{(sample statistic)} \\pm t*\\texttt{(standard error)}\\] In order to do this for any of the above sample statistics, we only need to know how to calculate that sample statistic’s standard error and the degrees of freedom used to look up the t-statistic for that sample statistic. Table 12 provides a useful cheat sheet of those formulas: Table 12: Cheat sheet of equations for calculating standard errors and degrees of freedom Type SE df for \\(t\\) Mean \\(s/\\sqrt{n}\\) \\(n-1\\) Proportion \\(\\sqrt\\frac{\\hat{p}*(1-\\hat{p})}{n}\\) \\(n-1\\) Mean Difference \\(\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}\\) min(\\(n_1-1\\),\\(n_2-1\\)) Proportion Difference \\(\\sqrt{\\frac{\\hat{p}_1*(1-\\hat{p}_1)}{n_1}+\\frac{\\hat{p}_2*(1-\\hat{p}_2)}{n_2}}\\) min(\\(n_1-1\\),\\(n_2-1\\)) Correlation Coefficient \\(\\sqrt{\\frac{1-r^2}{n-2}}\\) \\(n-2\\) I know some of that math might look intimidating but I will go through an example of each case below to show you how it works for each case. Example with proportions As an example, lets use the proportion of respondents who do not believe in anthropogenic climate change. In our politics sample, we get: table(politics$globalwarm) ## ## No Yes ## 1180 3058 n &lt;- 1180+3058 p_hat &lt;- 1180/n n ## [1] 4238 p_hat ## [1] 0.2784332 About 27.8% of the respondents in our sample are climate change deniers. What can we conclude about the proportion in the US population? First, lets figure out the t-statistic. We use the same \\(n-1\\) for degrees of freedom: t_stat &lt;- qt(0.975, n-1) t_stat ## [1] 1.960524 Our sample is large enough that we are basically using 1.96. Now we need to calculate the standard error. The formula from above is: \\[\\sqrt{\\frac{\\hat{p}*(1-\\hat{p})}{n}}\\] The term \\(hat{p}\\) is the standard way to represent the sample proportion, which in this case is 0.279. So, our formula is: \\[\\sqrt{\\frac{0.278*(1-0.278)}{4238}}\\] We can calculate this in R: se &lt;- sqrt(p_hat*(1-p_hat)/n) se ## [1] 0.006885228 We now have all the pieces to construct the confidence interval: p_hat+t_stat*se ## [1] 0.2919319 p_hat-t_stat*se ## [1] 0.2649346 We are 95% confident that the true percentage of climate change deniers in the the US population is between 26.5% and 29.2%. Example with mean differences using our Add health data, what is the mean difference in popularity (number of friend nominations) between frequent smokers and those who do not smoke frequently? tab &lt;- tapply(popularity$indegree, popularity$smoker, mean) tab ## Non-smoker Smoker ## 4.506699 4.796992 mean_diff &lt;- 4.796992 - 4.506699 mean_diff ## [1] 0.290293 In our sample data, frequent smokers had 0.290 more friends on average than those who did not smoke frequently. What is the confidence interval for that value in the population? We start by calculating the t-statistic for this confidence interval. We use the size of the smaller group minus one for the degrees of freedom. table(popularity$smoker) ## ## Non-smoker Smoker ## 3732 665 n1 &lt;- 3732 n2 &lt;- 665 t_stat &lt;- qt(.975, n2-1) t_stat ## [1] 1.963543 The value is pretty close to 1.96 but a little bigger. Now we need to calculate the standard error. The formula is: \\[\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}\\] We already have \\(n_1\\) and \\(n_2\\), so we just need to get the standard deviation of friend nominations for the two groups to get \\(s_1\\) and \\(s_2\\). We can do this with another tapply command, but changing from mean to sd in the third argument. tapply(popularity$indegree, popularity$smoker, sd) ## Non-smoker Smoker ## 3.652224 3.901188 s1 &lt;- 3.652224 s2 &lt;- 3.901188 se &lt;- sqrt(s1^2/n1+s2^2/n2) se ## [1] 0.1626661 Now we have all the pieces to put together the confidence interval: mean_diff - t_stat*se ## [1] -0.02910896 mean_diff + t_stat*se ## [1] 0.609695 We are 95% confident that in the population of US adolescents, those who smoke frequently have between 0.03 fewer to 0.61 more friend nominations, on average, than those who do not smoke frequently. Note that because our confidence interval contains both negative and positive value, we cannot be confident about whether smoking is truly associated with having more or less friends. The direction of the relationship between the two variables is uncertain. Example with proportion differences Lets continue to use the Add Health data. Do we observe a gender difference in smoking behavior in our sample? prop.table(table(popularity$smoker, popularity$sex), 2) ## ## Female Male ## Non-smoker 0.8534894 0.8435407 ## Smoker 0.1465106 0.1564593 p_hat_f &lt;- 0.1465106 p_hat_m &lt;- 0.1564593 p_hat_diff &lt;- p_hat_m-p_hat_f p_hat_diff ## [1] 0.0099487 In our sample, about 14.7% of girls were frequent smokers and about 15.6% of boys were frequent smokers. The percentage of boys who smoke is about 1% higher than the percentage of girls. Do we think this moderate difference in the sample is true in the population? Lets start again by calculating the appropriate t-statistic for our confidence interval. We use the same procedure as for mean differences above, choosing the size of the smaller group for the degrees of freedom. However, its important to note that our groups are now boys and girls, not smokers and non-smokers. table(popularity$sex) ## ## Female Male ## 2307 2090 n_f &lt;- 2307 n_m &lt;- 2090 t_stat &lt;- qt(.975, n_m-1) t_stat ## [1] 1.9611 Now we can calculate the standard error. The formula is: \\[\\sqrt{\\frac{\\hat{p}_1*(1-\\hat{p}_1)}{n_1}+\\frac{\\hat{p}_2*(1-\\hat{p}_2)}{n_2}}\\] That looks like a lot, but we just have to focus on plugging in the right values in R: se &lt;- sqrt((p_hat_f*(1-p_hat_f)/n_f)+(p_hat_m*(1-p_hat_m)/n_m)) se ## [1] 0.01083286 Now we have all the parts to calculate the confidence interval: p_hat_diff - t_stat*se ## [1] -0.01129562 p_hat_diff + t_stat*se ## [1] 0.03119302 We are 95% confident that in the population of US adolescents, between 1.1% fewer to 3.1% more boys smoke frequently than girls. As above, because our confidence interval includes both negative and positive values, we are not very confident at all about whether boys or girls smoke more frequently. Example with correlation coefficient Lets stick with the Add Health data. What is the correlation between GPA and the number of friend nominations that a student receives? r &lt;- cor(popularity$pseudoGPA, popularity$indegree) r ## [1] 0.1680881 In our sample, there is a moderately positive correlation between a student’s GPA and the number of friend nominations that a student receives. What is our confidence interval for the population? For the t-statistic, we use \\(n-2\\) for the degrees of freedom: n &lt;- nrow(popularity) t_stat &lt;- qt(.975, n-2) For the standard error, the formula is: \\[\\sqrt{\\frac{1-r^2}{n-2}}\\] This is straightforward to calculate in R: se &lt;- sqrt((1-r^2)/(n-2)) se ## [1] 0.01486952 Now we have all the parts we need to calculate the confidence interval: r - t_stat*se ## [1] 0.1389363 r + t_stat*se ## [1] 0.1972398 We are 95% confident that the true correlation coefficient between GPA and friend nominations in the population of US adolescents is between 0.139 and 0.197. While there is some difference in the strength of that relationship, we are pretty confident that the correlation is moderately positive. "],
["hypothesis-tests.html", "Hypothesis Tests", " Hypothesis Tests In social scientific practice, hypothesis testing is far more common than confidence intervals as a technique of statistical inference. Both techniques are fundamentally derived from the sampling distribution and produce similar results, but the methodology and interpretation of results is very different. In hypothesis testing, we play a game of make believe. Remember that the fundamental issue we are trying to work around is that we don’t know the value of the true population parameter and thus we don’t know where the center is for the sampling distribution of the sample statistic. In hypothesis testing, we work around this issue by boldly asserting what we think the true population parameter. We then test whether the data that we got are reasonably consistent with that assertion. Example: Coke winners Lets take a fairly straightforward example. Coca-Cola used to run promotions where a certain percentage of bottle caps were claimed to win you another free coke. In one such promotion, when I was in graduate school, Coca-Cola ran a promotion where they claimed that 1 in 12 bottles were winners. If this is true, then 8.3% (1/12=0.083) of all the coke bottles in every grocery store and mini mart should be winners. Being a grad student who needed to stay up late writing a dissertation fueled by caffeine and “sugar,” I use to drink quite a few Cokes. After only receiving a few winners after numerous attempts, I began to get suspicious of the claim. I started collecting bottle caps to see if I could statistically find evidence of fraudulent behavior. For the sake of this exercise, lets say I collected 100 coke bottle caps (I never got this high in practice, but its a nice round number) and that I only got five winners. My winning percentage is 5% which is lower than Coke’s claim of 8.3%. The critical question is whether it is likely or unlikely that I would get a winning percentage this different from the claim in a sample of 100 bottle caps. That is what a hypothesis test is all about. We are asking whether the data that we got are likely under some assumption about the true parameter. If they are unlikely, then we reject that assumption. if they are not unlikely, then we do not reject the assumption. We call that assumption the null hypothesis, \\(H_0\\). The null hypothesis is a statement about what we think the true value of the parameter is. The null hypothesis is our “working assumption” until we can be proven to be wrong. In this case, the parameter of interest is the true proportion of winners among the population of all Coke bottles in the US. Coke claims that this proportion is 0.083, so this is my null hypothesis. In mathematical terms, we write: \\[H_0: \\rho=0.083\\] I use the Greek letter \\(\\rho\\) as a symbol for the population proportion. I will use \\(\\hat{p}\\) to represent the sample proportion in my sample, which is 0.05. Some standard statistical textbooks will also claim that there is an “alternative hypothesis.” That alternative hypothesis is specified as “anything but the null hypothesis.” In my opinion, this is incorrect because vague statements about “anything else” do not constitute an actual hypothesis about the data. We are testing only whether the data are consistent with the null hypothesis. No other hypothesis is relevant. We got a sample proportion of 0.05 on a sample of 100. Assuming the null hypothesis is true, what would the sampling distribution look like from which I pulled my 0.05? Note the part in bold above. We are now playing our game of make believe. We know that on a sample of 100, the sample proportion should be normally distributed. It should also be centered on the true population proportion. Because we are assuming the null hypothesis is true, it should be centered on the value of 0.083. The standard error of this sampling distribution is given by: \\[\\sqrt{\\frac{0.083*(1-0.083)}{100}}=0.028\\] Therefore, we should have a sampling distribution that looks like: Figure 40: A game of make believe, or the sampling distribution for sample proportion of winning Coca-Cola bottle caps assuming the null hypothesis is true The blue line shows the true population proportion assumed by the null hypothesis. The red line shows my actual sample proportion. The key question of hypothesis testing is whether the observed data (or more extreme data) are reasonably likely under the assumption of the null hypothesis. Practically speaking, I want to know how far my sample proportion is from the true proportion and whether this distance is far enough to consider it unlikely. To calculate how far away I am on some standard scale, I divide the distance by the standard error of the sampling distribution to calculate how many standard errors my sample proportion is below the population parameter (assuming the null hypothesis is true). \\[\\frac{0.05-0.083}{0.028}=\\frac{-0.033}{0.028}=-1.18\\] My sample proportion is 1.18 standard errors below the center of the sampling distribution. Is this an unlikely distance? To figure this out, we need to calculate the area in the lower tail of the sampling distribution past my red line. This number will tell us the proportion of all sample proportions that would be 0.05 or lower, assuming the null hypothesis is true. This standardized measure of how far is sometimes called the test statistic for a given hypothesis test. Calculating this area is not a trivial exercise, but R provides a straightforward command called pt which is somewhat similar to the qt command above. We just need to feed in how many standard errors our estimate is away from the center (-1.18) and the degrees of freedom. These degrees of freedom are identical to the ones used in confidence intervals (in this case, \\(n-1\\), so 99). pt(-1.18, 99) ## [1] 0.1204139 There is one catch with this command. It always gives you the area in the lower tail, so if your sample statistic is above the center, you should still put in a negative value in the first command. We will see an example of this below. Our output indicates that 12% of all samples would produce a sample proportion of 0.05 or less when the true population proportion is 0.083. Graphically it looks like this: Figure 41: The proportion of all possible sample proportions that are lower than our sample proportion, assuming the null hypothesis is true The grey area is the area in the lower tail. It would seem that we are almost ready to conclude our hypothesis test. However, there is a catch and its a tricky one. Remember that I was interested in the probability of getting a sample proportion this far or farther from the true population proportion. This is not the same as getting a sample proportion this low or lower. I need to consider the possibility that I would have been equally suspicious if I had got a sample proportion much higher than 8.3%. In mathematically terms, that means I need to take the area in the upper tail as well, where I am .033 above the true population proportion. This is called a two-tailed test. Luckily, because the normal distribution is symmetric, this area will be identical to the area in the lower tail and so I can just double this percent. Figure 42: We have to also consider the possibility of getting a sample proportion as far from the population proportion but in the other direction Assuming the null hypothesis is true, there is a 24% chance of getting a sample proportion as far from the true population mean or farther, just by random chance. We call this probability the p-value. The p-value is the ultimate goal of the hypothesis test. All hypothesis tests produce a p-value and it is the p-value that we will use to make a decision about our test. What should that decision be? We have only two choices. If the p-value is low enough, then it is unlikely that we would have gotten this data or data more extreme, assuming the null hypothesis is true. Therefore, we reject the null hypothesis. If the p-value is not low enough, then it is reasonable that we would have gotten this data or data more extreme, assuming the null hypothesis is true. Therefore, we fail to reject the null hypothesis. Note that we NEVER accept or prove the null hypothesis. It is already our working assumption, so the best we can do for it is to fail to reject it and thus continue to use it as our working assumption. How low does the p-value need to be in order to reject it? There is no right answer here, because this is a subjective question. However, there is a generally agreed upon practice in the social sciences that we reject the null hypothesis when the p-value is at or below 0.05 (5%). Note that while there is general consensus around this number, it is an arbitrary cutpoint. The practical difference between a p-value of 0.049 and 0.051 is negligible, but under this arbitrary standard, we would make different decisions in each case. I would rather that you just learn to think about what the p-value represents and reach your own decision. No reasonable scientist, however, would reject the null hypothesis with a p-value of 24% as we have in our Coke case. Nearly 1 in 4 samples of size 100 would produce a sample proportion this different from the assumed true proportion of 8.3% just by random chance. I therefore do not have sufficient evidence to reject the null hypothesis that Coke is telling the truth. Note that I have not proved that Coke is telling the truth. I have only failed to produce evidence that they are lying. The general procedure of hypothesis testing The general procedure of hypothesis testing is as follows: State a null hypothesis. This null hypothesis is a claim about the true value of an unknown parameter. Calculate a test statistic that tells you how far your sample statistic is from the center of the sampling distribution, assuming the null hypothesis is true. For our purposes, this test statistic will always be the number of standard errors above or below the true population parameter, assuming the null hypothesis is true. Calculate the p-value for the test statistic. The p-value is the probability of getting a sample statistic this far or farther (in absolute value) from the true population parameter, assuming the null hypothesis is true. If the p-value is below some threshold (typically 0.05), reject the null hypothesis. Otherwise, fail to reject the null hypothesis. Interpreting p-values correctly P-values are widely misunderstood in practice. Studies have been done of practicing researchers across different disciplines where these researchers were asked to interpret a p-value from a multiple choice question and the majority get it wrong. Therefore, don’t feel bad if you are having trouble understanding a p-value. You are in good company! Nonetheless, proper interpretation of a p-value is critically important for our understanding of what a hypothesis test does. The reason many people get the interpretation of p-values wrong is that they want the p-value to express the probability of a hypothesis being correct or incorrect. People routinely misinterpret the p-value as a statement about the probability of the null hypothesis being correct. The p-value is NOT a statement about the probability of a hypothesis being correct or incorrect. For the same reason that we cannot call a confidence interval a probability statement, the classical approach dictates that we cannot characterize our subjective uncertainty about whether hypotheses are true or not by a probability statement. The hypothesis is either correct or it is not. There is no probability. Correctly interpreted, the p-value is a probability statement about the data, not about the hypothesis. Specifically, we are asking what the probability is of observing data this extreme or more extreme, assuming the null hypothesis is true. We are not making a probability statement about hypotheses. Rather we are assuming a hypothesis and then asking about the probability of the data. This difference may seem subtle, but it is in fact quite substantial in interpretation. The reason why everyone (including you and me) struggles with this is that our brains want it to be the other way around. Ultimately by rejecting or failing to reject we are making statements about whether we believe the hypothesis or not, but we are not doing that directly by a probability statement about the hypothesis but rather a probability statement about the likelihood of the data given the hypothesis. Hypothesis tests of relationships The hypothesis tests that we care the most about in the sciences are hypothesis tests about relationships between variables. We want to know whether the association we are observing in the sample is true in the population. In all of these cases, our null hypothesis is that there is no association, and we want to know whether the association we observe in the sample is strong enough to reject this null hypothesis of no association. We can do hypothesis tests of this nature for both mean differences and regression slopes. Example: mean differences Lets look at differences in mean income (measured in $1000) by religion in the politics dataset. tapply(politics$income, politics$relig, mean) ## Mainline Protestant Evangelical Protestant Catholic ## 81.83439 58.32606 77.53498 ## Jewish Non-religious Other ## 120.92958 88.62963 60.75311 I want to look at the difference between Evangelical Protestants and “Other Religions.” The mean difference here is: \\[58.32606-60.75311=-2.42705\\] Evangelical Protestants make $2,427 less than members of other religions, in my sample. Let me set up a hypothesis test where the null hypothesis is that Roman Catholics and members of other religions have the same income, or in other words, the mean difference in income is zero: \\[H_0: \\mu_c-\\mu_o=0\\] Where \\(\\mu_c\\) is the population mean income of evangelical Protestants and \\(\\mu_o\\) is the population mean income of members of other religions. In order to figure out how far my sample mean difference of -2.427 is from 0, I need to find the standard error of the mean difference. The formula for this number is: \\[\\sqrt{\\frac{s_c^2}{n_c}+\\frac{s_o^2}{n_o}}\\] I can calculate this in R: tapply(politics$income, politics$relig, sd) ## Mainline Protestant Evangelical Protestant Catholic ## 62.90763 51.23396 66.59281 ## Jewish Non-religious Other ## 89.84166 71.46392 56.37886 table(politics$relig) ## ## Mainline Protestant Evangelical Protestant Catholic ## 785 917 1015 ## Jewish Non-religious Other ## 71 567 883 se &lt;- sqrt(51.23396^2/917+56.37886^2/883) -2.42705/se ## [1] -0.9547436 The t-statistic of -0.95 here is not very large. I am only 0.44 standard errors below 0 on the sampling distribution, assuming the null hypothesis is true. Lets go ahead and calculate the p-value for this t-statistic. Remember that I need to put in the negative version of this number to the pt command. I also need to use the smaller of the two sample sizes for my degrees of freedom: 2*pt(-0.95, 883-1) ## [1] 0.3423725 In a sample of this size, there is an 34% chance of observing a mean income difference of $2,427 or more between evangelical Protestants and members of other religions, just by sampling error, assuming that there is no difference in income in the population. Therefore, I fail to reject the null hypothesis that evangelical Protestants and members of other religions make the same income. Example of proportion differences Lets look at the difference in smoking behavior between white and black students in the Add Health data. Our null hypothesis is: \\[H_0: \\rho_w-\\rho_c=0\\] In simple terms, our null hypothesis is that the same proportion of white and black adolescents smoke frequently. Lets look at the actual numbers from our sample: prop.table(table(popularity$smoker, popularity$race),2) ## ## White Black/African American Latino ## Non-smoker 0.79560106 0.94847162 0.89000000 ## Smoker 0.20439894 0.05152838 0.11000000 ## ## Asian/Pacific Islander Other ## Non-smoker 0.90123457 0.92592593 ## Smoker 0.09876543 0.07407407 ## ## American Indian/Native American ## Non-smoker 0.80769231 ## Smoker 0.19230769 p_w &lt;- 0.20439894 p_b &lt;- 0.05152838 p_diff &lt;- p_w-p_b p_diff ## [1] 0.1528706 About 20.4% of white students smoked frequently, compared to only 5.2% of black students. The difference in proportion is a large 15.3% in the sample. This would seem to contradict our null hypothesis. However, we need to confirm that a difference this large in a sample of our size is unlikely to happen by random chance. To do that we need to calculate the standard error, just as we learned to do it for proportion differences in the confidence interval section: table(popularity$race) ## ## White Black/African American ## 2637 1145 ## Latino Asian/Pacific Islander ## 400 162 ## Other American Indian/Native American ## 27 26 n_w &lt;- 2637 n_b &lt;- 1145 se &lt;- sqrt((p_w*(1-p_w)/n_w)+(p_b*(1-p_b)/n_b)) se ## [1] 0.01021531 Now many standard errors is our observed difference in proportion from zero? t_stat &lt;- p_diff/se t_stat ## [1] 14.96485 Wow, thats a lot. We can be pretty confident already without the final step of the p-value, but lets calculate it anyway. Remember ot always take the negative version of the t-statistic you calculated: 2*pt(-14.96485, n_b-1) ## [1] 2.247969e-46 The p-value is astronomically small. In a sample of this size, the probability of observing a difference in the proportion frequent smokers between whites and blacks of 15.3% or larger if there is no difference in the population is less than 0.0000001%. Therefore, I reject the null hypothesis and conclude that white students are more likely to be frequent smokers than are black students. Example of correlation coefficient Lets look at the correlation between the parental income of a student and the number of friend nominations they receive. Our null hypothesis will be that there is no relationship between parental income and student popularity in the population of US adolescents. Lets look at the data in our sample: r &lt;- cor(popularity$parentinc, popularity$indegree) r ## [1] 0.1247392 In the sample we observe a moderately positive correlation between a student’s parental income and the number of friend nominations they receive. How confident are we that we wouldn’t observe such a large correlation coefficient in our sample by random chance if the null hypothesis is true? First, we need to calculate the standard error: n &lt;- nrow(popularity) se &lt;- sqrt((1-r^2)/(n-2)) se ## [1] 0.01496633 How many standard errors are we away from the assumption of zero correlation? r/se ## [1] 8.334658 What is the probability of being that far away from zero for a sample of this size? 2*pt(-8.334658, n-2) ## [1] 1.027779e-16 The probability is very small. In a sample of this size, the probability is less than 0.00000001% of observing a correlation coefficient between parental income and friend nominations received of an absolute magnitude of 0.125 or higher when the true correlation is zero in the population. Therefore, we reject the null hypothesis and conclude that there is a positive correlation between parental income and popularity among US adolescents. Statistical Significance When a researcher is able to reject the null hypothesis of “no association,” the result is said to be statistically significant. This is a somewhat unfortunate phrase that is sometimes loosely interpreted to indicate that the result is “important” in some vague scientific sense. In practice, it is important to distinguish between substantive and statistical significance. In very large datasets, standard errors will be very small, and thus it is possible to observe associations that are very small in substantive size that are nonetheless statistically significant. On the flip side, in small datasets, standard errors will often be large, and thus it is possible to observe associations that are very large in substantive size but not statistically significant. It is important to remember that “statistical significance” is a reference to statistical inference and not a direct measure of the actual magnitude of an association. I prefer the term “statistically distinguishable” to “statistically significant” because it more clearly indicates what is going on. In the previous example, we found that the income differences in our sample between Catholics and members of other religions are not statistically distinguishable from zero. We also found that the negative association in our sample between age and sexual frequency was statistically distinguishable from zero. Establishing whether an association is worthwhile in its substantive effect is a totally different exercise from establishing whether it is statistically distinguishable from zero. It is also important to remember that a statistically insignificant finding is not evidence of no relationship because we never accept the null hypothesis. We have just failed to find sufficient evidence of a relationship. No evidence of an association is not evidence of no association. "],
["building-models-1.html", "Building Models", " Building Models Up to this point, we have learned the elementary components of a good statistical analysis. However, the typical social scientist doesn’t spend that much time with these elementary components. Instead, most social scientific analysis depends on building statistical model. A statistical model is a formal mathematical representation of how we think variables might be related to one another. By building models, we can better understand the relationships between variables and how these relationships are affected by other variables. We will focus on model building in some form or another for all the remaining modules of this course. We will begin with the simplest kind of model: we just try to fit a straight line through a set of points on a scatterplot. Although this approach may not seem very sophisticated, it forms the basis for more advanced modeling techniques we will learn later. After understanding this basic model, often called the *OLS regression model** we will move on to a variety of techniques we can use to build more complicated models that are both more realistic and more informative. Throughout this module, we will focus primarily on issues of interpretation. We are now starting to learn the techniques that you will see presented in real world social science research. Being able to interpret and understand this work is a key objective of this course. "],
["the-ols-regression-line.html", "The OLS Regression Line", " The OLS Regression Line Figure 43 shows a scatterplot of the relationship between median age and violent crime rates: Figure 43: Scatterplot of median age and violent crime rates across US states, with a best-fitting straight line drawn through points I have also plotted a line through those points. When you were trying to determine the direction of the relationship many of you were probably imagining a line going through the points already. Of course, if we just tried to “eyeball” the best line, we would get many different results. The line I have graphed above, however, is the best fitting line, according to standard statistical criteria. It is the best-fitting line because it minimizes the total distance from all of the points collectively to the line. This line is called the ordinary least squares regression line ( or OLS regression line, for short). This fairly simply concept of fitting the best line to a set of points on a scatterplot is the workhorse of social science statistics and is the basis for most of the models that we will explore in this module. The Formula for a Line Remember the basic formula for a line in two-dimensional space? In algebra, you probably learned something like this: \\[y=a+bx\\] The two numbers that relate \\(x\\) to \\(y\\) are \\(a\\) and \\(b\\). The number \\(a\\) gives the y-intercept. This is the value of \\(y\\) when \\(x\\) is zero. The number \\(b\\) gives the slope of the line, sometimes referred to as the “rise over the run.” The slope indicates the change in \\(y\\) for a one-unit increase in \\(x\\). The OLS regression line above also has a slope and a y-intercept. But we use a slightly different syntax to describe this line than the equation above. The equation for an OLS regression line is: \\[\\hat{y}_i=b_0+b_1x_i\\] On the right-hand side, we have a linear equation (or function) into which we feed a particular value of \\(x\\) (\\(x_i\\)). On the left-hand side, we get not the actual value of \\(y\\) for the \\(i\\)th observation, but rather a predicted value of \\(y\\). The little symbol above the \\(y\\) is called a “hat” and it indicates the “predicted value of \\(y\\).” We use this terminology to distinguish the actual value of \\(y\\) (\\(y_i\\)) from the value predicted by the OLS regression line (\\(\\hat{y}_i\\)). The y-intercept is given by the symbol \\(b_0\\). The y-intercept tells us the predicted value of \\(y\\) when \\(x\\) is zero. The slope is given by the symbol \\(b_1\\). The slope tells us the predicted change in \\(y\\) for a one-unit increase in \\(x\\). In practice, the slope is the more important number because it tells us about the association between \\(x\\) and \\(y\\). Unlike the correlation coefficient, this measure of association is not unitless. We get an estimate of how much we expect \\(y\\) to change in terms of its units for a one-unit increase in \\(x\\). For the scatterplot in Figure 43 above, the slope is -25.6 and the y-intercept is 1343.9. We could therefore write the equation like so: \\[\\hat{\\texttt{crime rate}_i}=1343.9-25.6(\\texttt{median age}_i)\\] We would interpret our numbers as follows: The model predicts that a one-year increase in age within a state is associated with 25.6 fewer violent crimes per 100,000 population, on average. (the slope) The model predicts that in a state where the median age is zero, the violent crime rate will be 1343.9 crimes per 100,000 population, on average. (the intercept) There is a lot to digest in these interpretations and I want to return to them in detail, but first I want to address a more basic question. How did I know that these are the right numbers for the best-fitting line? Calculating the Best-Fitting Line The slope and intercept of the OLS regression line are determined based on addressing one simple criteria: minimize the distance between the actual points and the line. More formally, we choose the slope and intercept that produce the minimum sum of squared residuals (SSR). A residual is the vertical distance between an actual value of \\(y\\) for an observation and its predicted value: \\[residual_i=y_i-\\hat{y}_i\\] These residuals are also sometimes called error terms, because the larger they are in absolute value, the worse is our prediction. Take a look at the Figure 44 below which shows the residuals graphically as vertical distances between the actual point and the line. Figure 44: Scatterplot with best-fitting line shown in blue and residuals shown in red Unless the points all fall along an exact straight line, there is no way for me to eliminate these residuals altogether, but some lines will produce higher residuals than others. What I am aiming to do is minimize the sum of squared residuals which is given by: \\[SSR = \\sum_{i=1}^n(y_i-\\hat{y}_i)^2\\] I square each residual and then sum them up. By squaring, I eliminate the problem of some residuals being negative and some positive. To see how this all works, you can play around with the interactive example below which allows you to guess slope and intercept for a scatterplot and then see how well you did in minimizing the sum of squared residuals. Fortunately, we don’t have to figure out the best slope and intercept by trial and error, as in the exercise above. There are relatively straightforward formulas for calculating the slope and intercept. They are: \\[b_1=r\\frac{s_y}{s_x}\\] \\[b_0=\\bar{y}-b_1*\\bar{x}\\] The r here is the correlation coefficient. The slope is really just a re-scaled version of the correlation coefficient. We can calculate this with the example above like so: slope &lt;- cor(crimes$MedianAge, crimes$Violent)*sd(crimes$Violent)/sd(crimes$MedianAge) slope ## [1] -25.5795 I can then use that slope value to get the y-intercept: mean(crimes$Violent)-slope*mean(crimes$MedianAge) ## [1] 1343.936 Using the lm command to calculate OLS regression lines in R We could just use the given formulas to calculate the slope and intercept in R, as I showed above. However, the lm command will become particularly useful later in the term when we extend this basic OLS regression line to more advanced techniques. In order to run the lm command, you need to input a formula. The structure of this formula looks like “dependent~independent” where “dependent” and “independent” should be replaced by your specific variables. The tilde (~) sign indicates the relationship. So, if we wanted to use lm to calculate the OLS regression line we just looked at above, I would do the following: model1 &lt;- lm(crimes$Violent~crimes$MedianAge) Please keep in mind that the dependent variable always goes on the left-hand side of this equation. You will get very different answers if you reverse the ordering. In this case, I have entered in the variable names using the data$variable syntax, but lm also offers you a more streamlined way of specifying variables, by including a data option separately so that you only have to put the variable names in the formula, like so: model1 &lt;- lm(Violent~MedianAge, data=crimes) Because I have specified data=crimes, R knows that the variables “Violent” and “MedianAge” refer to variables within this dataset. The result will be the same as the previous command, but this approach makes it easier to read the formula itself. I have saved the output of the lm command into a new object that I have called “model1”. You can call this object whatever you like. This is out first real example of the “object-oriented” nature of R. I can apply a variety of functions to this object in order to extract information about the relationship. If I want to get the most information, I can run a summary on this model. summary(model1) ## ## Call: ## lm(formula = Violent ~ MedianAge, data = crimes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -381.76 -118.02 -36.25 99.28 853.41 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1343.94 434.21 3.095 0.00325 ** ## MedianAge -25.58 11.56 -2.214 0.03154 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 188.1 on 49 degrees of freedom ## Multiple R-squared: 0.09092, Adjusted R-squared: 0.07236 ## F-statistic: 4.9 on 1 and 49 DF, p-value: 0.03154 There is a lot information here and we actually don’t know what most of it means yet. All we want is the intercept and slope. These numbers are given by the two numbers in the “Estimate” column of the “Coefficients” section. The intercept is 1343.94 and the slope is -25.58. We could also run the coef command which will give us just the slope and intercept of the model. coef(model1) ## (Intercept) MedianAge ## 1343.9360 -25.5795 This result is much more compact and will do for our purposes at the moment. Adding an OLS regression line to a plot You can easily add an OLS regression line to a scatterplot in ggplot. We can do this using the geom_smooth function. However we also need to specify that our method of smoothing is “lm” (for linear model) with the method=\"lm\" argument. Here is the code for the example earlier: Figure 45: Use geom_smooth to plot an OLS regression line with or without a confidence interval band You will notice that Figure 45 also adds a band of grey. This is the confidence interval band for my line and is drawn by default. We will discuss issues of inference for the OLS regression line below. If you want to remove this you can change the se argument in geom_smooth to FALSE. The OLS regression line as a model You will note that I saved the output of my lm command above as model. The lm command itself stands for “linear model.” What do I mean by this term “model?” When we talk about “models” in statistics, we are talking about modeling the relationship between two or more variables in a formal mathematical way. In the case of the OLS regression line, we are predicting the dependent variable as a linear function of the independent variable. Just as the general term model is used to describe something that is not realistic but rather an idealized representation, the same is true of our statistical models. We certainly don’t believe that our linear function provides a correct interpretation of the exact relationship between our two variables. Instead we are trying to abstract from the details and fuzziness of the relationship to get a “big picture” of what the relationship looks like. However, we always have to consider that our model is not a very good representation of the relationship. The most obvious potential problem is if the relationship is non-linear and yet we fit the relationship by a linear model, but there can be other problems as well. I will discuss these more below and the next few sections of this module will give us techniques for building better models. However, we first need to focus on how to interpret the results we just got. Interpeting Slopes and Intercepts Learning to properly interpret slopes and intercepts (especially slopes) is the number one most important thing you will learn all term, because of how common the use of OLS regression is in social science statistics. You simply cannot pass the class unless you can interpret these numbers. So take the time to be careful in interpretation here. Interpreting Slopes In abstract terms, the slope is always the predicted change in \\(y\\) for a one unit increase in \\(x\\). However, this abstract definition will simply not do when you are dealing with specific cases. You need to think about the units of \\(x\\) and \\(y\\) and interpret the slope in concrete terms. There are also a few other caveats to consider. Take the interpretation I used above for the -25.6 slope of median age as a predictor of violent crime rates. My interpretation was: The model predicts that a one year increase in age within a state is associated with 25.6 fewer violent crimes per 100,000 population, on average. There are multiple things going on in this sentence that need to be addressed. First, lets address the phrase “model predicts.” The idea of a model is something we will explore more later, but for now I will say that when we fit a line to a set of points to predict \\(x\\) by \\(y\\), we are applying a model to the data. In this case, we are applying a model that relates \\(y\\) to \\(x\\) by a simple linear function. All of our conclusions are dependent on this being a good model. Prefacing your interpretation with “the model predicts…” highlights this point. Second, a “one year increase in age” indicates the meaning of a one unit increase in \\(x\\). Never literally say a “one unit increase in \\(x\\).” Think about the units of \\(x\\) and describe the change in \\(x\\) in these terms. Third, I use “is associated with” to indicate the relationship. This phrase is intentionally passive. We want to avoid causal language when we describe the relationship. Saying something like “when \\(x\\) increases by one \\(y\\) goes up by \\(b_1\\)” may sound more intuitive, but it also implies causation. The use of “is associated with” here indicates that the two variables are related without implicitly implying that one causes the other. Using causal language is the most common mistake in describing the slope. Fourth, “25.6 fewer violent crimes per 100,000 population” is the expected change in \\(y\\). Again, you always have to consider the unit scale of your variables. In this case, \\(y\\) is measured as the number of crimes per 100,000 population, so a decrease of 25.6 means 25.6 fewer violent crimes per 100,000 population. Fifth, I append the term “on average” to the end of my interpretation. This is because we know that our points don’t fall on a straight line and so we don’t expect a deterministic relationship between median age and violent crime. Rather, we think that if we were to take a group of states that had one year higher median age than another group of states, the average difference between the groups would be -25.6. Lets try a couple of other examples to see how this works. I will use the lm command in R to calculate the slopes and intercepts, which I explain in the section below. First, lets look at the association between age and sexual frequency (I will explain the code I use here later in this section). coef(lm(sexf~educ, data=sex)) ## (Intercept) educ ## 49.7295901 0.0266939 The slope here is 0.03. Education is measured in years and sexual frequency is measured as the number of sexual encounters per year. So, the interpretation of the slope should be: The model predicts that a one year increase in education is associated with 0.03 more sexual encounters per year, on average. There is a tiny positive effect here, but in real terms the relationship is basically zero. It would take you about 100 years more education to get laid 3 more times. Just think of the student loan debt. Now, lets take the relationship between movie runtimes and tomato meter ratings: coef(lm(TomatoMeter~Runtime, data=movies)) ## (Intercept) Runtime ## 5.1074601 0.4054953 The slope is 0.41. Runtime is measured in minutes. The tomato meter is the percent of reviews that were judged to be positive. The model predicts that a one minute increase in movie runtime length is associated with a 0.38 percentage point increase in the movie’s Tomato Meter rating, on average. Longer movies tend to have higher ratings. We may rightfully question the assumption of linearity for this relationship however. It seems likely that if a movie can become too long, so its possible the relationship here may be non-linear. We will explore ways of modeling that potential non-linearity later in the term. Interpreting Intercepts Intercepts give the predicted value of \\(y\\) when \\(x\\) is zero. Again you should never interpret an intercept in these abstract terms but rather in concrete terms based on the unit scale of the variables involved. What does it mean to be zero on the \\(x\\) variable? In our example of the relationship of median age to violent crime rates, the intercept was 1343.9. Our independent variable is median age and the dependent variable is violent crime rates, so: The model predicts that in a state where the median age is zero, the violent crime rate would be 1343.9 crimes per 100,000 population, on average. Note that I use the same “model predicts” and “on average” prefix and suffix for the intercept as I used for the slope. Beyond that I am just stating the predicted value of \\(y\\) (crime rates) when \\(x\\) is zero in the concrete terms of those variables. Is it realistic to have a state with a median age of zero? No, its not. You will never observe a US state with a median age of zero. This is a common situation that often confuses students. In cases when zero falls outside the range of the independent variable, the intercept is not a particular useful number because it does not tell us about a realistic situation. The intercept’s only “job” is to give a number that allows the line to go through the points on the scatterplot at the right level. You can see this in the interactive exercise above if you select the right slope of 148 and then vary the intercept. In general making predictions for values of \\(x\\) that fall outside the range of \\(x\\) in the observed data is problematic. This is often leads to intercepts which don’t make a lot of sense. This problem with zero being outside the range of data is also evident in the other two examples of slopes from the previous section. When looking at the relationship between education and sexual frequency, no respondents are actually at zero years of education and no movies are at zero minutes of runtime. In truth, to fit the line correctly, we only need the slope and one point along the line. It is convenient to choose the point where \\(x=0\\) but there is no reason why we could not choose a different point. It is actually quite easy to calculate a different predicted value along the line by re-centering the independent variable. To re-center the independent variable \\(x\\), we just need to to subtract some constant value \\(a\\) from all the values of \\(x\\), like so: \\[x^*=x-a\\] The zero value on our new variable \\(x^*\\) will indicates that we are at the value of \\(a\\) on the original variable \\(x\\). If we then use \\(x^*\\) in the OLS regression line rather than \\(x\\), the intercept will give us the predicted value of \\(y\\) when \\(x\\) is equal to \\(a\\). Lets try this out on the model predicting violent crimes by median age. We will create a new variable where we subtract 35 from the median age variable and use that in the regression model. crimes$MedianAge.ctr &lt;- crimes$MedianAge-35 coef(lm(Violent~MedianAge.ctr, data=crimes)) ## (Intercept) MedianAge.ctr ## 448.6533 -25.5795 The intercept now gives me the predicted violent crime rate in a state with a median age of 35. In effect, I have moved my y-intercept from zero to thirty-five as is shown in Figure 46 below. Figure 46: Re-centering the independent variable moves the intercept but does not change the slope Its also possible to re-center an independent variable in the lm command without creating a whole new variable. If you surround the re-centering in the I() function within the formula, R will interpret the result of whatever is inside the I() function as a new variable. Here is an example based on the previous example: coef(lm(Violent~I(MedianAge-35), data=crimes)) ## (Intercept) I(MedianAge - 35) ## 448.6533 -25.5795 How good is \\(x\\) as a predictor of \\(y\\)? If I selected a random observation from the dataset and asked you to predict the value of \\(y\\) for this observation, what value would you guess? Your best guess would be to guess the mean of y because this is the case where your average error would be smallest. This error is defined by the distance between the mean of y and the selected value, \\(y_i-\\bar{y}\\). Now, lets say instead of making you guess randomly I first told you the value of another variable \\(x\\) and gave you the slope and intercept predicting \\(y\\) from \\(x\\). What is your best guess now? You should guess the predicted value of \\(\\hat{y}_i\\) from the regression line because now you have some additional information. There is no way that having this information could make your guess worse than just guessing the mean. The question is how much better do you do than guessing the mean. Answering this question will give us some idea of how good \\(x\\) is as a predictor of \\(y\\). We can do this by separating, or partitioning the total possible error in our first case when we guessed the mean, into the part accounted for by \\(x\\) and the part that is unaccounted for by \\(x\\). I demonstrate this partitioning for one observation in our crime data (the state of Alaska) with the scatterplot in Figure 47 below. Figure 47: We can parition the total distance (in red) between an observation’s value of the dependent variable and the mean (the dotted horizontal line) into the part accounted for by the model (in gold) and the residual (in green) that is unaccounted for by the model The distance in red is the total distance between the observed violent crime rate in the state of Alaska and the mean violent crime rate across all states (given by the dotted line). If I were instead to use the OLS regression line predicting the violent crime rate by median age, I would predict a higher violent crime rate than average for Alaska because of its relatively low median age, but I would still predict a crime rate that is too low relative to the actual crime rate. The red line can be partitioned into th gold line which is the improvement in my estimate and the green line which is the error that remains in my prediction from the model. If I could then repeat this process for all of the states, I could calculate the percentage of the total red lines that the gold lines cover. This would give me an estimate of how much I reduce the error in my prediction by using the regression line rather than the mean to predict a state’s violent crime rate. In practice, we actually need to square those vertical distances because some are negative and some are positive and then we can sum them up over all the observations. So we get the following formulas: Total variation: \\(SSY=\\sum_{i=1}^n (y_i-\\bar{y})^2\\) Explained by model: \\(SSM=\\sum_{i=1}^n (\\hat{y}_i-\\bar{y})^2\\) Unexplained by model: \\(SSR=\\sum_{i=1}^n (y_i-\\hat{y}_i)^2\\) The proportion of the variation in \\(y\\) that is explainable or accountable by variation in \\(x\\) is given by \\(SSM/SSY\\). This looks like a kind of nasty calculation, but it turns out there is a much simpler way to calculate this proportion. If we just take our correlation coefficient \\(r\\) and square it. We will get this proportion. This measure is often called “r squared” and can be interpreted as the proportion of the variation in \\(y\\) that is explainable or accountable by variation in \\(x\\). In the example above, we can calculate R squared: cor(crimes$MedianAge, crimes$Violent)^2 ## [1] 0.09091622 About 9% of the variation in violent crime rates across states can be accounted for by variation in the median age across states. Inference for OLS Regression models When working with sample data, our usual issues of statistical inference apply to regression models. In this case, our primary concern is the estimate of the regression slope because the slope measures the relationship between \\(x\\) and \\(y\\). We can think of an underlying OLS regression model in the population: \\[\\hat{y}_i=\\beta_0+\\beta_1x_i\\] We use Greek “beta” values because we are describing unobserved parameters in the population. The null hypothesis in this case would be that the slope is zero indicating no relationship between \\(x\\) and \\(y\\): \\[H_0:\\beta_1=0\\] In our sample, we have a sample slope \\(b_1\\) that is an estimate of \\(\\beta_1\\). We can apply the same logic of hypothesis testing and ask whether our \\(b_1\\) is different enough from zero to reject the null hypothesis. We just need to find the standard error for this sample slope and the degrees of freedom to use for the test and we can do this manually. However, I have good news for you. You don’t have to do any of this by hand because the lm function does it for you automatically. Lets look at the full output of the model predicting violent crime rates from median age again using the summary command: model &lt;- lm(Violent~MedianAge, data=crimes) summary(model) ## ## Call: ## lm(formula = Violent ~ MedianAge, data = crimes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -381.76 -118.02 -36.25 99.28 853.41 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1343.94 434.21 3.095 0.00325 ** ## MedianAge -25.58 11.56 -2.214 0.03154 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 188.1 on 49 degrees of freedom ## Multiple R-squared: 0.09092, Adjusted R-squared: 0.07236 ## F-statistic: 4.9 on 1 and 49 DF, p-value: 0.03154 The “Coefficients” table in the middle gives us all the information we need. The first column gives us the sample slope of -25.58. The second column gives us the standard error for this slope of 11.56. The third column gives us the t-statistic derived by dividing the first column by the second column. The final column gives us the p-value for the hypothesis test. In this case, there is about a 3.2% chance of getting a sample slope this large on a sample of 51 cases if the true value in the population is zero. Of course, in this case its nonsensical because we don’t have a sample, but the numbers here will be valuable in cases with real sample data. Regression Line Cautions OLS regression models can be very useful for understanding relationships, but they do have some important limitations that you should be aware of when you are doing statistical analysis. There are three major limitations/cautions to be aware of when using OLS regression: OLS regression only works for linear relationships. Outliers can sometimes exert heavy influence on estimates of the relationship Don’t extrapolate beyond the scope of the data. Linearity By definition, an OLS regression line is a straight line. If the underlying relationship between x and y is non-linear, then the OLS regression line will do a poor job of measuring that relationship. One common case of non-linearity is the case of diminishing returns in which the slope gets weaker at higher values of x. Figure 48 demonstrates a class case of non-linearity in the relationship between a country’s life expectancy and GDP per capita. Figure 48: Scatterplot of GDP per capita and life expectancy across countries, 2007 The relationship is clearly a strongly positive one, but also one of diminishing returns where the positive relationship seems to plateau at higher levels of GDP per capita. This makes sense because the same absolute increase in country wealth at low levels of life expectancy can be used to reduce the incidence of well-understood infectious and parasitic diseases, whereas the same absolute increase in country wealth at high levels of life expectancy must try to reduce the risk of less understood and treatable diseases like cancer. You get more bang for your buck when life expectancy is low. Figure 49 shows what happens if we try to fit a line to this data. ## Warning: Removed 3 rows containing missing values (geom_smooth). Figure 49: Fitting a line to a non-linear relationship will cause systematic errors in your prediction Clearly a straight line is a poor fit. We systematically overestimate life expectancy at low and high GDP per capita and underestimate life expectancy in the middle. Its possible, in some circumstances, to correct for this problem of non-linearity but we will not explore those options in this module. For now, its just important to be aware of the problem and if you see clear non-linearity then you should question the use of an OLS regression line. Outliers and Influential Points An outlier is an influential point if removing this observation from the dataset substantially changes the slope of the OLS regression line. You can try the interactive exercise below to see how removing points changes the slope of your line (click on a point a second time to add it back). Can you identify any influential points? For the case of median age, Utah and DC both have fairly strong influences on the shape of the line. Removing DC makes the relationship weaker, while removing Utah makes the relationship stronger. Outliers will tend to have the strongest influence when their placement is inconsistent with the general pattern. In this case, Utah is very inconsistent with the overall negative effect because it has both low median age and low crime rates. Lets say that you have identified an influential point. What then? In truth there is only so much you can do. You cannot remove a valid data point just because it is an influential point. There are two cases where it would be legitimate to exclude the point. First, if you have reason to believe that the observation is an outlier because of a data error, then it would be acceptable to remove it. Second, if you have a strong argument that the observation does not belong with the rest of the cases, because it is logically different, then it might be OK to remove it. In our case, there is no legitimate reason to remove Utah, but there probably is a legitimate reason to remove DC. Washington DC is really a city and the rest of our observations are states that contain a mix of urban and rural population. Because crime rates are higher in urban areas, DC’s crime rates look very exaggerated compared to states. Because of this “apples and oranges” problem, it is probably better to remove DC. If our unit of analysis was cities, on the other hand, then DC should remain. In large datasets (1000+ observations), its unusual that a single point or even a small cluster of points will exert much influence on the shape of the line. The concern about influential points is mostly a concern in small datasets like the crime dataset. Thou Doth Extrapolate Too Much Its dangerous enough to assume that a linear relationship holds for your data (see the first point in this module). Its doubly dangerous to assume that this linear relationship holds beyond the scope of your data. Lets take the relationship between sexual frequency and age. We saw in the previous module that the slope here is -1.3 and the intercept is 108. The intercept itself is outside the scope of the data because we only have data on the population 18 years and older. It would be problematic to make predictions about the sexual frequency of 12 year olds, let alone zero-year olds. Another trivial example would be to look at the growth rate of children 5-15 years of age by correlating age with height. It would be acceptable to use this model to predict the height of a 14 year old, but not a 40 year old. We expect that this growth will eventually end sometime outside the range of our data when individuals reach their final adult height. If we extrapolated the data, we would predict that 40 year olds would be very tall. "],
["the-power-of-controlling-for-other-variables.html", "The Power of Controlling for Other Variables", " The Power of Controlling for Other Variables In the previous module, I showed that the OLS regression line predicting sexual frequency by years of education was 0.03. So in my dataset, there is a very small positive association between sexual frequency and years of education. Its possible that this is a causal effect. We could even spin stories about why we think such a positive association (a very small one) might exist. Maybe more educated people appear sexier to the opposite sex. Maybe more educated people take better care of themselves and thus are healthier and more able to have sex. Maybe more educated people are just more sexually liberated. Before we get carried away however its important to consider whether our results might be spurious. Its possible that the positive association between years of education and sexual frequency is driven by a third variable that we haven’t accounted for. This is a common problem in research using observational data. Association does not necessarily mean causation because of the potential for other variables to account for our observed association (and because of the possibility of reverse causation). We refer to such variables as lurking or confounding variables. In this case, the potentially confounding variable that we need to consider is age. Lets look at the association between age and each of our other variables (sexual frequency and education). cor(sex$sexf,sex$age) ## [1] -0.3974668 cor(sex$educ,sex$age) ## [1] -0.06018569 Age is negatively correlated with sexual frequency. We have observed this relationship before and it is not terribly surprising. Older people have less sex, on average. The negative correlation between age and years of education is perhaps a little more surprising. Older people have less education than younger people, on average. This may seem surprising to you because as you get older you have more opportunity to complete more education. However, you have to remember that the data we have are a snapshot in time. We are not tracking individuals over time as they age, but rather looking at differences between older and younger people at a single point in time. This kind of data is often called a cross-sectional dataset. Because we are looking at a single point in time, the age differences really reflect differences in birth cohorts or what people often loosely call “generations.” Remember that this dataset is from 2004. The difference between a 20 year old and a 60 year old is that the 20 year old was born in 1984 and the 60 year old was born in 1944. Because we are comparing birth cohorts, the differences in educational attainment reflect history more than life cycle. Older cohorts were less educated than younger birth cohorts. On average, you will be more educated than your parents and your parents were more educated than your grandparents. Thus, the correlation between age and education is negative. These two negative correlations suggest a spurious reason why we might observe a positive association between sexual frequency and education. Younger people have more education and younger people have more sex. Thus, when we look at the relationship between sexual frequency and education, we see a positive association but that positive association is indirectly driven by youth and the association of youth with both education and sex. How can we examine whether this potential spurious explanation is accurate? It turns out that we can add more than one independent variable to an OLS regression model at the same time. The mathematical structure of such a model would be: \\[\\hat{frequency}_i=b_0+b_1(education_i)+b_2(age_i)\\] We now have two different “slopes”, \\(b_1\\) and \\(b_2\\). These two slopes give the association of education and age, respectively, on sexual frequency, while controlling for the other independent variable. We now have what is called a multivariate OLS regression model. This “controlling” concept is a key point that I will return to below, but first I want to try to think graphically about what this model is doing. In the case of bivariate regression, we thought of fitting a line to a scatterplot in two-dimensional space. We are doing something similar here, but since we now have three variables, our scatterplot is in three dimensions. Figure 50 shows an interactive three-dimensional plot of the three variables. Figure 50: Interactive 3d scatterplot of years of education, age, and sexual frequency The dependent variable is shown on the vertical (z) axis and the two independent variables are shown on the “width” and “depth” axes (x and y). The flat plane shown is defined by the OLS regression model equation above. So, rather than fitting a straight line through the data, I am fitting a plane. Note however that if you rotate the 3d scatterplot to hide the age “depth” dimension, it will then look like a two-dimensional scatterplot between years of education and sexual frequency. In this case, the the edge of the plane would indicate the slope between years of education and sexual frequency. Similarly, I could rotate it the other way to look at the relationship between age and sexual frequency. How do I know what are the best values for \\(b_0\\), \\(b_1\\), and \\(b_2\\) that define my plane? The logic is the same as for bivariate OLS regression: I choose values that minimize the sum of squared residuals (SSR): \\[SSR=\\sum_{i=1}^n (\\hat{y}_i-y_i)^2\\] SSR is a measure of how far the predicted values of the dependent variable are from the actual values, so we want the intercept and slopes that minimizes this error. Unlike the bivariate case, however, there is no simple formula that I can give you for the slope and intercept, without some knowledge of matrix algebra. However, R can calculate the correct numbers for you easily. I am not concerned with your technical ability to calculate these numbers by hand, but I do want you to understand why those are the “best” numbers. They are the best numbers because they minimize the sum of the squared residuals for the model. We can calculate this model in R just by adding another variable to our model in the lm command: model &lt;- lm(sexf~educ+I(age-18), data=sex) coef(model) ## (Intercept) educ I(age - 18) ## 91.062080 -0.427747 -1.303385 Note that as I did in the previous module, I am re-centering age on 18 years so that I have reasonable value for the interpretation of the intercept. In equation form, our model will look like: \\[\\hat{frequency}_i=91.06-0.43(education_i)-1.30(age_i-18)\\] Interpreting results in a multivariate OLS regression models How do we interpret the results? Intercept: The model predicts that 18-year old individuals at with no education will have 91.06 sexual encounters per year, on average. Education Slope: The model predicts that, holding age constant, an additional year of education is associated with 0.43 fewer sexual encounters per year, on average. Age Slope: The model predicts that, holding education constant, an additional year of age is associated with 1.3 fewer sexual encounters per year, on average. The intercept is now the predicted value when all independent variables are zero. My interpretation of the slopes is almost identical to the bivariate case, except for one very important addition. I am now estimating the effect of each independent variable on the dependent variable while holding constant all other independent variables. You could also say “controlling for all other independent variables.” What does it mean to “hold other variables constant?” It means that when we look at the effect of one independent variable, we are looking at how the predicted value of the dependent variable changes while keeping all the other variables the same. For instance, the education effect above is the effect of a one year increase in education among individuals of the same age. Because we are looking at the effect of education among individuals of the same age, age should no longer have a confounding effect on our estimate of the effect of education. Thus holding constant/controlling for other variables helps to remove the potential spurious effect of those variables as confounders. Note how the effect of education on sexual frequency changed once I included age as a control variable. Before controls, I estimated a slightly positive slope (0.03) but now I am estimating a substantial negative slope (-0.43). So my understanding of the relationship between education and sexual frequency is completely reversed. When you compare individuals of the same age, more educated individuals have less sex, on average, than less educated individuals. Crime example Lets build a regression model where we predict the property crime rate in a state by the percent of adults in the state without a high school diploma and the median age of the state’s residents. summary(lm(Property~PctLessHS+MedianAge, data=crimes)) ## ## Call: ## lm(formula = Property ~ PctLessHS + MedianAge, data = crimes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1080.38 -376.78 12.19 346.82 1600.56 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5136.66 1411.21 3.640 0.000666 *** ## PctLessHS 69.47 24.79 2.803 0.007286 ** ## MedianAge -83.31 35.30 -2.360 0.022368 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 567.2 on 48 degrees of freedom ## Multiple R-squared: 0.2495, Adjusted R-squared: 0.2182 ## F-statistic: 7.977 on 2 and 48 DF, p-value: 0.001021 Note that I am giving you the full output of summary now, but we can find the slopes and intercept by looking at the Estimate column of the “Coefficients” table. “Coefficients” is another term for slopes and intercepts because that it the technical term for these values in the regression model equation. The model is: \\[\\hat{\\texttt{crime}}_i=5137+69(\\texttt{pct less hs}_i)-83(\\texttt{median age}_i)\\] The model predicts that, comparing two states with the same median age of residents, a one percent increase in the percent of the state with less than a high school diploma is associated with an increase of 69 property crimes per 100,000, on average. The model predicts that, comparing two states with the same percentage of adults without a high school diploma, a one year increase in the median age of a state’s residents is associated with a decrease of 83 property crimes per 100,000, on average. Note that we also get the \\(R^2\\) value from the summary command. In multivariate models, the \\(R^2\\) value always tells you what proportion of the variation in the dependent variable is accountable for by variation in all of the independent variables combined. In this case \\(R^2\\) is 0.2495. About 25% of the variation in property crime rates across states is accountable for by variation in the percent of adults without a high school diploma and the median age of residents across states. Including more than two independent variables If we can include two independent variables in a regression model, why stop there? Why not include three or four or more? The number of independent variables you can include is only limited by the sample size (you can never have more independent variables than the sample size minus one), although in practice we generally stop well short of this limit for pragmatic reasons. Lets take the model above predicting property crime rates by percent of adults with less than a high school diploma and the median age of residents. Lets add the poverty rate as another predictor: summary(lm(Property~PctLessHS+MedianAge+Poverty, data=crimes)) ## ## Call: ## lm(formula = Property ~ PctLessHS + MedianAge + Poverty, data = crimes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1174.43 -236.69 -30.96 286.41 1218.77 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4240.38487 1383.49276 3.065 0.0036 ** ## PctLessHS 0.04504 36.07642 0.001 0.9990 ## MedianAge -73.27098 33.68882 -2.175 0.0347 * ## Poverty 97.67933 38.52145 2.536 0.0146 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 537.6 on 47 degrees of freedom ## Multiple R-squared: 0.3398, Adjusted R-squared: 0.2976 ## F-statistic: 8.063 on 3 and 47 DF, p-value: 0.0001947 The model predicts: A one percent increase in the percent of adults in a state without a high school diploma is associated with 0.05 more property crimes per 100,000, on average, holding constant the median age of residents and the poverty rate in a state. This result is about as close to zero as you will find. A one year increase in the median age of a state’s residents is associated with 73 fewer property crimes per 100,000, on average, holding constant the percent of adults without a high school diploma and the poverty rate in a state. A one percent increase in a state’s poverty rate is associated with 98 more crimes per 100,000, on average, holding constant the percent of adults without a high school diploma and the median age of residents in a state. 34% of the variation in property crime rates across states can be accounted for by variation in the percent of adults without a high school diploma, residents’ median age, and the poverty rates across states. When I interpret the models now, I am holding constant the other two variables when I estimate the effect of each. Note that controlling for the poverty rate has a huge effect on the education variable whose effect goes from a substantial positive effect to basically zero effect. What does this tell us? Poverty rates and high school dropout rates are positively correlated and so when you don’t control for poverty rates, it looks like the high school dropout rate predicts crime because states with high high school dropout rates have high poverty rates and high poverty rates predict property crime rates. Once you control for the poverty rate, you see that it is economic deprivation not educational deprivation that is driving the crime rate. In general, the form of the multivariate regression model is: \\[\\hat{y}_i=b_0+b_1x_{i1}+b_2x_{i2}+b_3x_{i3}+\\ldots+b_px_{ip}\\] The intercept is given by \\(b_0\\). This is the predicted value of \\(y\\) when all of the independent variables are zero. The remaining \\(b\\)’s give the slopes for all of the variables up through the \\(p\\)th variable. Each of these gives the predicted change in \\(y\\) for a unit increase in that independent variable, holding all other independent variables constant. How to read a table of regression results In academic journal articles and books, the results of OLS regression models are represented in a fairly standard way. In order to understand how to read these articles, you need to understand this presentation style. Its not immediately intuitive for everyone. Table 13 below shows the typical style. In this table, I am reporting three regression models with the property crime rates as the dependent variable and three different independent variables. Table 13: OLS regression models predicting violent crime rates for US states Model 1 Model 2 Model 3 Intercept 1892.85*** 5136.66*** 4240.38** (335.33) (1411.21) (1383.49) Percent Less than HS 78.83** 69.47** 0.05 (25.58) (24.79) (36.08) Median Age -83.31* -73.27* (35.30) (33.69) Poverty Rate 97.68* (38.52) R-squared 0.16 0.25 0.34 N 51 51 51 ***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05. Standard errors in parenthesis. When reading this table and others like it, keep the following issues in mind: The first question you should ask is “what is the dependent variable?” This is the outcome that we are trying to predict. Typically, the dependent variable will be listed in the title of the table. In this case, the title tells you that the dependent variable is property crime rates and the unit of analysis is US states. The independent variables are listed on the rows of the table. In this case, I have independent variables of percent less than HS, median age, and the poverty rate. As I will explain below, just because an independent variable is listed here does not mean that it is actually included in all models. Models are listed in each column of the table. If numbers are listed for the row of a particular independent variable then that variable is included in that particular model. In this case, I have three different models. The first model only has numbers listed for Percent less than HS, so that is the only independent variable in the first model. The second model has numbers listed for Percent less than HS and Median Age, so both of these variables are included in the model. The third model includes all three variables in the model. Remember that in each case the dependent variable is the property crime rate. Within each cell with numbers listed there is a lot going on. We are primarily interested in the main number listed at the top. This number is the slope (or intercept in the case of the “Constant” row). The number in parenthesis is the standard error for each slope in the model. You could use this standard error and the slope estimate above it to calculate t-statistics and p-values exactly. However, the asterisks give you an easy visual short cut to determine the rough size of the p-value. These asterisks indicate if the p-value is below a certain level, as shown in the notes at the bottom. The cut-offs of 0.05, 0.01, and 0.001 used here are pretty standard for the discipline. So an asterisks generally means that the result is “statistically significant.” However, its important to keep in mind as noted above that these cut-offs are ultimately arbitrary and should never be confused with the substantive size of the effect itself. At the bottom, you typically get a number of summary measures of the model. The only two we care about are the number of observations and the \\(R^2\\) of the model. The advantage of organizing the table in this fashion is that we can easily see how the relationship between a given independent variable and the dependent variable changes as we add in other control variables by just looking at the numbers across a row. For example, we can see from Model 1 that the percent of the population with less than a high school diploma is initially pretty strongly positively related to violent crime rates. A one percentage point increase in this variable is associated with 78.83 more violent crimes per 100,000 population, on average. Controlling for the median age of the population in Model 2reduces this effect slightly but we still see a strong relationship. However, once we control for the poverty rate in a state the percent less than high school diploma effect completely vanishes (it becomes 0.05 which is effectively zero in this case). The poverty rate, on the other hand, has a big positive association with violent crime rates. What is going on here? It seems that the percent of the population with less than a high school diploma is only indirectly related to violent crime rates by its positive association with the poverty rate. But high poverty rates are much more directly responsible for high violent crime rates. In other words, a low high school completion rate predicts higher poverty rates and higher poverty rates predict more violent crimes, but a low high school completion rate does not directly predict more violent crimes. "],
["including-categorical-variables-as-predictors.html", "Including Categorical Variables as Predictors", " Including Categorical Variables as Predictors To this point, we only know how to include quantitative variables into OLS regression models. However, it turns out you can use a fairly easy trick to include categorical variables as independent variables in OLS regression models. By including categorical variables as independent variables, we expand considerably the range of things that we can do with OLS regression models. The most difficult part of this trick is correctly interpreting your results. Indicator variables As an example, I am going to look at the relationship between religious affiliation and sexual frequency. To keep our example simple I am going to dichotomize the religious affiliation variable, which means I am going to collapse it into two categories, rather than the six categories in the dataset. I will use a simply dichotomy of “Not Religious/Religious.” In R, I can create this variable like so: sex$norelig &lt;- sex$relig==&quot;None&quot; This is technically a boolean variable, which means it takes a TRUE or FALSE value. For our purposes, TRUE is a non-religious person. We already know how to look at the relationship between sexual frequency and this dichotomized religious affiliation variable. We can look at the mean differences in sexual frequency across our two categories: tapply(sex$sexf, sex$norelig, mean) ## FALSE TRUE ## 48.33671 59.84862 59.84862-48.33671 ## [1] 11.51191 The non-religious have sex 11.5 more times per year than the religious, on average. Hallelujah? We can represent this same mean difference in a regression model framework by using an indicator variable. An indicator variable is a variable that only takes a value of zero or one. It takes a value of one when the observation is in the indicated category and a zero otherwise. Mathematically, we would say: \\[nonrelig_i=\\begin{cases} 1 &amp; \\text{if non-religious}\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] The indicated category is the category which gets a one on the indicator variable. In this case the indicated category is non-religious. The reference category is the category that gets no indicator variable. In this case, that is just the religious group. Later on, we will see that this can become slightly more complicated. You can think of the indicator variable as an on/off switch where 1 indicates that it is “on” (i.e. the observation belongs to the indicated category) and 0 indicates that it is “off” (i.e. the observation does not belong to the indicated category). What would happen if we put this indicator variable into a regression model predicting sexual frequency like so: \\[\\hat{frequency}_i=b_0+b_1(nonrelig_i)\\] How would we interpret the slope and intercept for such a model? Figure 51 shows a scatterplot of this relationship. Figure 51: A scatterplot of the religious indicator variable by sexual frequency. Points are jittered to avoid overplotting. The mean for each group is plotted in red. Notice that all of the points align vertically either at the 0 or 1 on the x-axis. This is because the indicator variable can only take these two values. I have jittered points slightly to avoid overplotting. I have also plotted the means of the two groups in red dots and the OLS regression line for the scatterplot in blue. It turns out, that in order to be the best-fitting line, this OLS regression line must connect the two dots that represent the mean of each group. What will the slope of this line be? If we go up “one unit” on the non-religious indicator variable we have gone from a religious person to a non-religious person and the change in predicted sexual frequency is equal to the mean difference of 11.5 between the groups. The intercept is given by the value at zero which is just given by the mean sexual frequency among the religious of 48.3. So, the OLS regression line should look like: \\[\\hat{frequency}_i=48.3+11.5(nonrelig_i)\\] I can calculate these same numbers in R with the lm command: coef(lm(sexf~norelig, data=sex)) ## (Intercept) noreligTRUE ## 48.33671 11.51191 The numbers are the same. More important than the numbers, however, is the interpretation of the numbers. The intercept is the mean of the dependent variable for the reference category. The slope is the mean difference between the reference category and the indicated category. In this case, I would say: Religious individuals have sex 48.3 times per year, on average. Non-religious individuals have sex 11.5 times more per year than non-religious individuals, on average. Note that I can derive the sexual frequency of the non-religious from these two numbers by taking the value for the non-religious and adding the mean difference to find out that non-religious individuals have sex 59.8 times per year, on average. Reversing the indicator variable What if I switched my indicator variable so that the religious were indicated and the non-religious were the reference category? \\[relig_i=\\begin{cases} 1 &amp; \\text{if religious}\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] Lets try it out in R and see (the != below is computer lingo for “not equal to”): sex$religious &lt;- sex$relig!=&quot;None&quot; coef(lm(sexf~religious, data=sex)) ## (Intercept) religiousTRUE ## 59.84862 -11.51191 Lets compare the two models: \\[\\hat{frequency}_i=48.3+11.5(nonrelig_i)\\] \\[\\hat{frequency}_i=59.8-11.5(relig_i)\\] Both models give me the exact same information, but from the perspective of a different reference group. The first model tells me the mean sexual frequency of the religious (48.3) and how much more sex the non-religious have on average (11.5). The second model tells me the mean sexual frequency of the non-religious (59.8) and how much less sex the religious have (-11.5). I can easily derive one model from the other, without actually having to calculate it in R. Therefore, which category you set as the reference category is really a matter of taste, rather than one of consequence. The results are the same either way. Categorical variables with more than two categories What if I have a categorical variable that has more than two categories? Lets expand the religious variable that I dichotomized back to its original scale. There are six different categories: Fundamentalist Protestant, Mainline Protestant, Catholic, Jewish, Other, and None: summary(sex$relig) ## Fund Protestant Mainline Protestant Catholic ## 556 529 507 ## Jewish Other None ## 39 145 327 Lets look at the mean sexual frequency for each of these groups. round(tapply(sex$sexf, sex$relig, mean),1) ## Fund Protestant Mainline Protestant Catholic ## 49.6 44.2 49.2 ## Jewish Other None ## 39.8 57.9 59.8 Figure 52 plots these means on a number line to get a visual display of the differences: Figure 52: The mean sexual frequency of each group arrayed on a vertical number line. The red lines indicate the distance between each group and the reference category of fundamentalist Protestant. Nones and others clearly have much higher mean sexual frequency than the remaining religious groups and Jews have much lower mean sexual frequency. The three Christian groups cluster in the middle, although mainline protestants have a lower mean sexual frequency than the other two. This plot also shows the mean differences between the groups, with fundamentalist Protestants set as the reference category. The vertical distances from the dotted red line (the mean of fundamentalist Protestants) give the mean differences between each religious group and fundamentalist Protestants. So we can see that “Nones” have sex 10.2 more times per year than fundamentalist Protestants, on average, and mainline Protestants have sex 5.4 fewer times per year, on average, than fundamentalist Protestants. We can use the same logic of indicator variables we developed above to represent the mean differences between groups observed here in a regression model framework. However, because we now have six categories, we will need five indicator variables. You always need one less indicator variable than the number of categories. The category which doesn’t get an indicator variable is your reference category. As per the graph above, I will make Fundamentalist Protestants my reference category. Therefore, I need one indicator variable for each of the other five categories: \\[main_i=\\begin{cases} 1 &amp; \\text{if main}\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] \\[catholic_i=\\begin{cases} 1 &amp; \\text{if catholic}\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] \\[jewish_i=\\begin{cases} 1 &amp; \\text{if jewish}\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] \\[other_i=\\begin{cases} 1 &amp; \\text{if other religion}\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] \\[none_i=\\begin{cases} 1 &amp; \\text{if no religion}\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] Now lets put these variables into an OLS regression model: \\[\\hat{frequency}_i=b_0+b_1(main_i)+b_2(catholic_i)+b_3(jewish_i)+b_4(other_i)+b_5(none_i)\\] We can figure out how all this works by getting the predicted value for the member of a specific group. That respondent should get a 1 for the variable where they are a member and a zero on all other variables. For example, a fundamentalist protestant should get a zero on all of these variables: \\[\\hat{frequency}_i=b_0+b_1(0)+b_2(0)+b_3(0)+b_4(0)+b_5(0)=b_0\\] So, the intercept is the predicted value for fundamentalist Protestants. Similarly we could calculate the predicted value for mainline Protestants: \\[\\hat{frequency}_i=b_0+b_1(1)+b_2(0)+b_3(0)+b_4(0)+b_5(0)=b_0+b_1\\] The difference between the two is \\(b_1\\), so this “slope” gives the mean difference between mainline and fundamentalist Protestants. We could do the same thing for Catholics: \\[\\hat{frequency}_i=b_0+b_1(0)+b_2(1)+b_3(0)+b_4(0)+b_5(0)=b_0+b_2\\] The mean difference between Catholics and fundamentalist Protestants is given by \\(b_2\\). In general, each of the “slopes” is the mean difference between the indicated category and the reference category. In this case, the reference category is fundamentalist Protestants so each of the slopes gives the mean difference between that religious category and fundamentalist Protestant, just like the graph above. R is fairly intelligent about handling all of these indicator variables and you don’t actually have to create these five different variables. If you put a categorical variable into your regression formula, R will know to treat it as a set of indicator categories. The only catch is that R will already have a default category set as the reference. It just so happens that in our GSS data, fundamentalist Protestants are already set as the reference. So I can run this model by: model &lt;- lm(sexf~relig, data=sex) round(coef(model),2) ## (Intercept) religMainline Protestant religCatholic ## 49.60 -5.44 -0.36 ## religJewish religOther religNone ## -9.84 8.30 10.25 You can tell which category is the reference by which category is left out here. Note how the coefficients (given by the estimates column) match the mean differences I calculated above in the graph. We are simply reproducing these mean differences in a regression model framework. Categorical and quantitative variables combined in a single model If all we are doing is reproducing mean differences between categories, what good is this method? After all, we already know how to do that. The major advantage of putting these mean differences into a regression model framework is that we can control for other potentially confounding variables. These sexual frequency differences by religious affiliation are a prime example. Lets take a look at the age differences between religious affiliations: round(tapply(sex$age, sex$relig, mean),1) ## Fund Protestant Mainline Protestant Catholic ## 45.4 47.3 44.9 ## Jewish Other None ## 53.7 38.6 39.5 Notice how closely these age differences mirror the differences in sexual frequency. Others and nones are the youngest, while Jews are the oldest. Among Christians, mainline Protestants are older than fundamentalist Protestants and Catholics. We also know from prior work that age has a negative effect on sexual frequency. This should make us suspicious that some (or all) of the observed differences in sexual frequency between religious groups simply reflect age differences between those groups. We can easily address this issue by simply including age as a control variable in our model: model &lt;- lm(sexf~relig+age, data=sex) round(coef(model),2) ## (Intercept) religMainline Protestant religCatholic ## 107.90 -3.04 -0.96 ## religJewish religOther religNone ## 0.88 -0.43 2.69 ## age ## -1.28 We now interpret those slopes as the mean difference in sexual frequency between fundamentalist Protestants and the indicated category, among individuals of the same age. So for example, we would interpret the 2.69 on “None” as: The model predicts that, among individuals of the same age, those with no religious preference have sex 2.69 more times per year than fundamentalist protestants, on average. We would also interpret the age effect controlling for religious affiliation like so: The model predicts, that holding religious affiliation constant, a one year increase in age is associated with 1.28 fewer instances of sex per year, on average. Table 14 below helps to highlight the change in the effects once age is controlled. Table 14: OLS regression models predicting sexual frequency Model 1 Model 2 Intercept 49.60*** 107.90*** (2.26) (3.68) Mainline Protestant -5.44 -3.04 (3.24) (2.99) Catholic -0.36 -0.96 (3.28) (3.02) Jewish -9.84 0.88 (8.84) (8.17) Other 8.30 -0.43 (4.98) (4.61) None 10.25** 2.69 (3.72) (3.45) Age -1.28*** (0.07) R-squared 0.01 0.16 N 2103 2103 ***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05. Standard errors in parenthesis. Reference category is fundamentalist Protestant All of the coefficients (except for Catholic, which was tiny anyway) have declined substantially in size. The mean differences from fundamentalist Protestants for both Jews and other religions have basically disappeared and the “None” effect has been severely reduced. In other words, almost all or all of the observed differences in sexual frequency by religious affiliation were indirectly a product of underlying age differences between religious affiliations. If you were just about ready to convert to a different religion to get laid more often, you may want to hold off for the moment. "],
["interaction-terms.html", "Interaction Terms", " Interaction Terms By definition, a linear model is an additive model. As you increase or decrease the value of one independent variable you increase or decrease the predicted value of the dependent variable by a set amount, regardless of the other values of the independent variable. This is an assumption built into the linear model by its additive form, and it may misrepresent some relationships where independent variables interact with one another to produce more complicated effects. In particular, in this section, we want to know whether the effect (i.e. the slope) of one independent variable varies by the value of another independent variable. The nature of additive models As an example for this section, I am going to look at the relationship between movie genre, runtime, and tomato meter ratings. To simplify things, I am going to only look at these relationships for two genres: action and comedy. I can limit my movies dataset to these two genres with the following command: movies.short &lt;- subset(movies, Genre==&quot;Comedy&quot; | movies$Genre==&quot;Action&quot;) Now lets look at a simple model where genre and runtime both predict Tomato Meter ratings. round(coef(lm(TomatoMeter~Genre+Runtime, data=movies.short)),2) ## (Intercept) GenreComedy Runtime ## -1.75 4.43 0.41 Genre is a categorical variable and action movies are set as the reference category. In equation form, the model looks like: \\[\\hat{meter}_i=-1.75+4.43(comedy_i)+0.31(runtime_i)\\] I can interpret my slopes as follows: The model predicts that when comparing movies of the same runtime, comedies have Tomato Meter ratings 4.43 percentage points higher than action movies, on average. The model predicts that, holding constant movie genre, a one minute increase in movie runtime is associated with a 0.31 percentage point increase in the Tomato Meter rating, on average. This is an additive model. If we move from an action movie to a comedy of the same runtime, our predicted Tomato Meter rating goes up by 4.43, regardless of the actual value of runtime. If we increase movie runtime by one minute while keeping genre the same, our predicted Tomato Meter rating goes up by 0.41, regardless of whether that genre is action or comedy. It may help to graphically visualize the nature of this additive relationship. We can do this by plotting lines showing the relationship between runtime and Tomato Meter ratings separately for our two different genres of action and comedy. The line for action movies is given by: \\[\\hat{meter}_i=-1.75+4.43(0)+0.41(runtime_i)=-1.75+0.41(runtime_i)\\] The line for comedy movies is given by: \\[\\hat{meter}_i=-1.75+4.43(1)+0.41(runtime_i)=2.68+0.41(runtime_i)\\] Each line has an intercept and a slope. Notice that the intercepts are different but the slopes are the same. That means we have two parallel lines at different levels. Figure 53 overlays these two parallel lines on top of a scatterplot of movie runtime by tomato meter for these two genres. Figure 53: Predicted Tomato Meter by runtime for two genres based on an additive OLS regression model. The lines must be parallel. The parallel lines are an assumption of the OLS regression model structure we have used. There are two consequences of this assumption. First, At every single level of runtime, the predicted Tomato Meter difference between comedy and action movies is exactly 4.62. This can be seen on the graph by the consistent gap between the blue and red line. Second, the effect of runtime on the Tomato Meter rating is assumed to be the same for action and comedy movies. This can be seen on the graph by the fact that both lines have the exact same slope. Although these may seem like two different issues, they are really the same issue from different perspectives. If we were to allow the slopes of the blue and red line to be different, then the gap between them would not be static. The questions is how can we allow the slopes of the two lines to be different. This is where the concept of the interaction term comes in. The interaction term An interaction term is a variable that is constructed from two other variables by multiplying those two variables together. In our case, we can easily construct an interaction term as follows: movies.short$comedy &lt;- movies.short$Genre==&quot;Comedy&quot; movies.short$interaction &lt;- movies.short$Runtime*movies.short$comedy In this case, I had to create a real indicator variable for comedy before I could multiply them, but then I just multiply this indicator variable by movie runtime. Now lets add this interaction term to the model: model &lt;- lm(TomatoMeter~Runtime+comedy+interaction, data=movies.short) round(coef(model), 2) ## (Intercept) Runtime comedyTRUE interaction ## -14.45 0.52 24.36 -0.19 We now have an additional “slope” for the interaction term. Lets write this model out in equation form to try to figure out what is going on here. \\[\\hat{meter}_i=-14.45+24.36(comedy_i)+0.52(runtime_i)-0.19(runtime_i*comedy_i)\\] Remember that the interaction term is just a literal multiplication of the two other variables. To figure out how this all works, lets once again separate this into two lines predicting Tomato Meter by runtime, for comedies and action movies separately. For action movies, the equation is: \\[\\hat{meter}_i=-14.45+24.36(0)+0.52(runtime_i)-0.19(runtime_i*0)=-14.45+0.52(runtime_i)\\] For comedy movies, the equation is: \\[\\hat{meter}_i=-14.45+24.36(1)+0.52(runtime_i)-0.19(runtime_i*1)=(-14.45+24.36)+(0.52-0.19)(runtime_i)=9.91+0.33(run_i)\\] We now have two lines with different intercept and different slopes. The interaction term has allowed the effect of runtime on the Tomato Meter to vary by type of genre. In this case, the interaction term tells us how much smaller the slope is for comedy movies than for action movies. We can also just plot the lines to see how it looks, as I have done in Figure 54. Figure 54: An interaction term allows for non-parallel lines, and thus different effects of runtime on tomato meter ratings by genre The pattern here is fairly clear. Short comedies get better ratings than short action movies, while long comedies get worse ratings than long action movies. Put another way, comedies get less “return” in terms of their ratings when increasing their length than do action movies. This can be seen by the much steeper slope for action movies. Interpreting interaction terms Interpreting interaction terms can be tricky, because the inclusion of an interaction term also changes the meaning of other slopes in the model. The slopes for the two variables that make up the interaction term are called the main effects. In our example, those two variables are runtime and the comedy indicator variable and the main effects of these variables are 0.52 and 24.36, respectively. The most important rule to remember is that when an interaction term is in a model, the main effects are only the expected effects when the other variable involved in the interaction is zero. This is because the interaction implies that the effects of the two variables are not constant but rather change depending on the value of the other variable in the interaction term. Therefore, we can only interpret effects at a particular value of the other variable. So I would interpret these main effects as follows: The model predicts that among action movies, a one minute increase in movie runtime is associated with a 0.52 point increase in the Tomato Meter rating, on average. The model predicts that among movies with zero minutes of runtime (outside the scope of data of course), comedies are predicted to have Tomato Meter ratings 24.36 points higher than action movies, on average. Notice that I did not have to say I was controlling for the other variable. I am doing more than controlling when I include an interaction term. I am conditioning the effect of one variable on the value of another. That is why I instead use the phrase “among observations that are zero on the other variable.” Note that you could also include other non-interacted variables in this model as well, like maturity rating, in which case you would also need to indicate that you controlled for those variables. Interpreting interaction terms themselves can also be tricky because they are the difference in the effect of on variable depending on the value of another. One approach is to interpret this difference in effect directly. In this case, we would say: The model predicts that the predicted increase in Tomato Meter ratings for a one minute increase in movie runtime is 0.19 points smaller for comedy movies than for action movies, on average. You have to be careful with this type of interpretation. In this case, both slopes were still positive so I can talk about how the effect was smaller. However, in some cases, the slopes may end up in different directions entirely which would require a somewhat different interpretation. Another approach is to actually calculate the slope for the indicated category (comedies) and interpret it directly: The model predicts that among comedy movies, a one minute increase in movie runtime is associated with a 0.33 increase in the Tomato Meter rating, on average (which is lower than for action movies). In short, you have to be careful and thoughtful when thinking about how to interpret interaction terms. Interaction terms in R In the example above, I created the interaction term manually, but I didn’t actually need to do this. R has a shortcut method for calculating interaction terms: model &lt;- lm(TomatoMeter~Runtime*Genre, data=movies.short) round(coef(model),2) ## (Intercept) Runtime GenreComedy ## -14.45 0.52 24.36 ## Runtime:GenreComedy ## -0.19 The results are exactly the same as before. To include an interaction term between two variables I just have to connect them with a * rather than a + in the lm formula. By default, R will include each variable separately as well as their interaction. Interaction terms with multiple categories In the above example, I only compared comedy and action movies in order to keep the comparison simple, but it is possible to run the same analysis on the full movie dataset to see how runtime varies across all genres. model &lt;- lm(TomatoMeter~Runtime*Genre, data=movies) round(coef(model),2) ## (Intercept) Runtime ## -14.45 0.52 ## GenreAnimation GenreComedy ## 11.96 24.36 ## GenreDrama GenreFamily ## 59.50 -29.06 ## GenreHorror GenreMusical/Music ## 2.51 21.07 ## GenreMystery GenreRomance ## 2.30 66.59 ## GenreSciFi/Fantasy GenreThriller ## 5.92 12.26 ## Runtime:GenreAnimation Runtime:GenreComedy ## 0.14 -0.19 ## Runtime:GenreDrama Runtime:GenreFamily ## -0.39 0.32 ## Runtime:GenreHorror Runtime:GenreMusical/Music ## -0.03 -0.12 ## Runtime:GenreMystery Runtime:GenreRomance ## 0.06 -0.52 ## Runtime:GenreSciFi/Fantasy Runtime:GenreThriller ## -0.03 -0.04 Thats a lot of numbers! There is a slope for each genre except action (10 in all) and an interaction between runtime and each genre except action (another 10 in all). What we are estimating here are 11 different lines (on for each genre) for the relationship between runtime and Tomato Meter rating. Because action movies are the reference, the main effect of runtime is the slope for action movies (0.52). The interaction terms show us how much larger or smaller the effect of runtime is for each given genre. So the effect is 0.39 smaller for dramas for a total effect of 0.13 (0.52-0.39). It is 0.32 larger for family movies for a total effect of 0.84 (0.52+0.32), and so forth. Similarly, the intercept is the intercept only for action movies. To get the intercept for other genres, we take the intercept value itself and add the main effect of genre. So for dramas the intercept is -14.45+59.5=45.05 and for family movies it is -14.45-29.06=-43.51. If we put all these slopes and intercepts together, we will get 11 lines as shown in Figure 55. Figure 55: Interaction terms allow each genre to get a different return from increasing runtime There is a lot going on here, but we can detect some interesting patterns. Almost all of the lines are positive indicating that longer movies tend to generally get better ratings. This is not true of Romances however, where there is a slight negative relationship between movie runtime and Tomato Meter ratings. Dramas also have a fairly flat slope and a high intercept, so they tend to outperform most other short movies but don’t fare as well compared to other genres when they are longer. The steepest slope is for family movies, which apparently are horrible when short (think “Beethoven 6: Beethoven saves Christmas, again” or something), but do much better when longer. Interaction terms with two categorical variables The examples so far have involved interacting a quantitative variable with a categorical variable which gives you a different line for each category of your categorical variable. However, we can also create an interaction term between two categorical variables. As an example, lets look at differences in wages in the earnings dataset by race and education. To simplify things, I am going to dichotimize race into white/non-white and education into less than Bachelor’s degree/Bachelor’s degree or more, as follows: earnings$nwhite &lt;- earnings$race!=&quot;White&quot; earnings$college &lt;- as.numeric(earnings$educ)&gt;3 Lets look at mean wages across these combination of categories: tapply(earnings$wages, earnings[,c(&quot;nwhite&quot;,&quot;college&quot;)], mean) ## college ## nwhite FALSE TRUE ## FALSE 19.99246 33.91417 ## TRUE 16.74181 31.82208 White college graduates make $33.91 per hour, on average, while non-white college graduates make $31.82 per hour, on average. Whites without a college degree make $19.99, on average, while non-whites without a college degree make $16.74, on average. If we put this in a table, I can show that there are four different ways to make comparisons between these numbers. Table 15: Mean wages in dollars per hour by race and education No degree Bachelor’s degree Difference White 19.99 33.91 13.92 Non-white 16.74 31.82 15.08 Difference -3.25 -2.09 1.16 If we look at the two differences along the far-right column, we are seeing the “returns” in terms of wages for a college degree separately for whites and non-whites. The return for whites is $13.92 per hour and the return for non-whites is higher at $15.08. If we look at the differences along the bottom row, we are seeing the racial inequality in wages separately for those with no degree and those with a college degree. Among those with no college degree, non-whites make $3.25 less per hour than whites. Among those with a college degree, non-whites make $2.09 less per hour than whites. The racial gap in wages gets smaller among those who have completed a college degree. Now lets look at the difference in the differences. For the racial gap in wages this is given by -3.25-(-2.09)=1.16. For the returns to a college degree this is given by 15.08-13.92=1.16. The difference in the differences is the same! This is because we are looking at the same relationship in two different ways. If non-whites get a better return to college than whites, then the racial gap in wages must get smaller among the college-educated. Similarly, if the racial gap in wages gets smaller at the college level, it tells us that non-whites must get a better return on their college education. This 1.16 number is basically an interaction term. We can interpret the number as the difference in returns to wages from a college degree between whites and non-whites. Alternatively, we can interpret the number as the difference in the racial wage gap between those with no degree and those with a college degree. Either way, we have the same information, with the same finding: greater educational attainment reduces racial inequality because minorities get a greater return on their college degrees. Lets try modeling this relationship with an OLS regression model. First lets try a model without interaction terms: model &lt;- lm(wages~nwhite+college, data=earnings) coef(model) ## (Intercept) nwhiteTRUE collegeTRUE ## 19.850655 -2.866436 14.263698 Lets put this into an equation framework: \\[\\hat{wages}_i=19.85-2.87(nwhite_i)+14.26(college_i)\\] We can use this equation to fill in the predicted valued of the same table we calculated by hand above: Table 16: Predicted wages in dollars per hour by race and education from an additive model No degree Bachelor’s degree Difference White 19.85 19.85+14.26=34.11 14.26 Non-white 19.85-2.87=16.98 19.85-2.87+14.26=31.24 14.26 Difference -2.87 -2.87 0 The predicted values do not match the exact values above. More importantly, if you look at the differences, you can see that the returns to education are assumed to be identical for whites and non-whites ($14.26) and the racial gap is assumed to be the same for those with no degree and those with a college degree (-$2.87). This is the limitation of the additive model. We assume that the effects of race and college completion are not affected by each other. If we want to determine whether returns to college are different by race, we need to model the interaction term, as follows: model &lt;- lm(wages~nwhite*college, data=earnings) coef(model) ## (Intercept) nwhiteTRUE collegeTRUE ## 19.992457 -3.250648 13.921709 ## nwhiteTRUE:collegeTRUE ## 1.158565 In equation form: \\[\\hat{income}_i=19.99-3.25(nwhite_i)+13.92(college_i)+1.16(nwhite_i*college_i)\\] Lets use this model to get predicted values in our table: Table 17: Predicted wages in dollars per hour by race and education from an interactive model No degree Bachelor’s degree Difference White 19.99 19.99+13.92=33.91 13.92 Non-white 19.99-3.25=16.74 19.99-3.25+13.92+1.16=31.82 15.08 Difference -3.25 -2.09 1.16 Our model now fits the data exactly and the differences are allowed to vary by the other category, so that we can see the differences in returns to college by race and the differences in the racial gap by education level. The interaction term itself of 1.16 is the same to what we calculated by hand. If we were to interpret the intercept and slopes from the model above, we would say: Whites with no college degree had mean wages of $19.99 per hour. Among those with no college degree, non-whites earn $3.25 less per hour than whites, on average. Among whites, those with a college degree have wages $13.92 per hour higher on average than those without a college degree. The returns to wages from a college degree are $1.16 larger for non-whites than they are for whites, on average. "],
["useful-references.html", "Useful References ", " Useful References "],
["example-datasets.html", "Example Datasets", " Example Datasets We will utilize several different datasets throughout this course to develop concepts and to provide examples. You should become familiar with these datasets. Below, I provide a brief description of each of the datasets we will use. If you are taking my undergraduate course, you will have access to these datasets through RStudio Cloud. Otherwise, you can download a ZIP file of these datasets here. You should unzip this folder to your own computer and place it somewhere you can easily access it (e.g. the desktop). You should use this folder to organize all of your work for the course.The datasets are in a *.RData format. You can simply click on them in your filesystem viewer to load them into R. Crimes The crimes data contain information on crime rates and demographic variables for all fifty US states and the District of Columbia. The crime rates are for the year 2010 and come from the FBI’s Uniform Crime Reports (UCR). The UCR is a program where local law enforcement agencies all report crime statistics to the FBI and these are aggregated into final crime statistics. For our purposes, we are dividing crimes into two main categories of violent and property crime. The demographic characteristics come from the American Community Survey (ACS) between the years 2008 and 2012. The ACS is an annual sample of the US population. To get a large enough sample in each state to calculate correct statistics (with little sampling error), I combine five years of data that are “centered”\" on 2010. Here is a full description of all variables in the dataset that we will use. Violent: violent crimes per 100,000 population within each state. This includes the crimes of murder, rape, robbery, and aggravated assault. By dividing the number of crimes by the population size, we avoid the problem of larger population states having more crimes because of a larger population. This is often called the crime “rate.” Property: property crimes per 100,000 population. This includes the crimes of burglary, larceny, and motor vehicle theft.MedianAge: Median age of a state’s population. PctMale: Percent of a state population that is male. PctLessHS: The percent of the state population over the age of 25 without a high school diploma. MedianIncomeHH: Median household income in a state. This is measured in thousands of dollars (i.e. 35 means $35,000). We are taking the income of each household (meaning all members of that household combined) rather than individual level income. For most purposes, this is thought to be a better measure because consumption and savings are typically organized at the household level. Unemployment: Unemployment rate in the state. The unemployment “rate” is really just a percentage. Its the percentage of individuals who are not working but want to work among all those in the labor force (those who are working or looking for work). Poverty: Poverty rate in the state. The poverty “rate” is also really just a percentage. It is the percent of individuals living below the poverty line. The poverty line is a number developed by the federal government. It was originally developed in the 1960s and is adjusted for inflation every year. Many people critique the poverty line as being too low because it has not kept pace with increases in the consumer price index. Gini: A measure of income inequality in the state. The gini coefficient is a widely used measure of how unequally income is distributed. If gini is zero, then everyone has exactly the same income. If gini is 100, then one person makes all the money and everyone else zero. The higher the gini coefficient, the more income inequality exists. Movies The movie data contain information about 2,612 movies produced between 2001 and 2013. The data come from the Open Movie Database, which itself contains data from the Internet Movie Database and Rotten Tomatoes. To simplify our analyses, I have limited the analysis to movies that played in the US and received 10 or more reviews. I have also excluded all shorts, documentaries, foreign language films, and movies that received an NC-17 rating or were unrated. Here are the variables we have for each movie: Year: The calendar year of the film’s release. Rating: The movie’s maturity rating (G, PG, PG-13, R). Runtime: The length of the movie in minutes. Oscars: The number of Oscar awards that the movie received. This includes Oscars that go to individual actors (leading and supporting), as well as more general awards (best screenplay, editing, cinematography, etc.), and best picture overall. TomatoMeter: The tomato meter is the percent of reviews that are judged to be positive by Rotten Tomatoes staff. This metric goes from 0 to 100 percent. Note that movies are spread out pretty evenly across the range of this variable, something we call a “uniform” distribution. Note, that this method makes no distinction between how positive a positive review was or how negative a negative review was, so its perfectly possible for two movies with the same Tomato Meter to be viewed very differently by reviewers. TomatoRating: The tomato rating is a combination of all reviews where the review used some kind of numeric rating (e.g. 3 out of 4 stars, 7 out of 10). Rotten Tomatoes “normalizes” these scores so that they are all recorded on the same basis. The scale of this normalized score goes from 1 to 10. Unlike the Tomato Meter, this scale should be capable of distinguishing how strongly positive or negative the review was. BoxOffice: The box office returns for the movie in millions of US dollars. Genre: The genre of the film. This is a tricky variable to create. In actuality, movies could be listed as multiple genres in the original dataset, with twenty different genres to choose from. For example, “No Country for Old Men” is listed in the genres of crime, drama, and thriller while “Lord of the Rings: Return of the King” is listed as action, adventure, and fantasy. This is probably the best way to treat genres, but for our purposes it adds a lot of complexity. Therefore, I have recoded movies into a single “best” genre based on a decision rule where certain genres trump all others on an ordered basis. For example, comedy trumps romance, so romantic comedies will always show up in this dataset as comedies. The ordering of this system is Animation &gt; Family &gt; Musical &gt; Horror &gt; SciFi/Fantasy &gt; Comedy &gt; Romance &gt; Action &gt; Thriller &gt; Mystery &gt; Drama &gt; All Others. For the most part, this system works well, but you may notice some odd discrepancies for a few movies. Politics This data comes from the 2016 American National Election Study (ANES). The ANES is a survey of the American electorate that is conducted every two years. The study collects information on a variety of political attitudes and voting behaviors. For our purposes, we are going to primarily look at respondent’s vote for president and attitudes on three issues: (1) birthright citizenship, (2) gay marriage, and (3) global warming. The variables we will look at are: brcitizen: Respondents were asked whether they would support a proposal to change the US Constitution to remove birthright citizenship (citizenship automatically granted to individuals born in the US regardless of their parent’s citizenship status). Respondents could either favor, oppose, or neither favor or oppose. gaymarriage: Respondents were asked for their position on gay marriage and were given the choices of “no legal recognition”, “civil union (but no marriage)”, “support gay marriage.” globalwarm: A question on whether the respondent believes that anthropogenic global warming is happening. I constructed this variable from two separate questions. The first question asks whether respondents think that global warming has been happening with the options being that it “probably has” or “probably has not.” The second question asks whether respondents thought that global warming was caused by human activity (either entirely or partially). I combine these into a single dichotomous variable where individuals either think the earth is warming from human activity or that it is not warming from human activity, where the latter category includes people who think it isn’t warming at all and people who think it is warming but not because of human activity. party: The political party with which the respondent identifies. This does not necessarily mean that a respondent is officially registered with a given party. relig: The respondent’s religion. This category is based on the combination of people’s statement about the kind of services they typically attend along with several non-exclusive yes/no questions about their religion (e.g. evangelical, Pentecostal, agnostic, atheist). age: The age of the respondent. gender: The respondent’s self-reported gender, recorded as “Male”,“Female”, or “Other.” race: the racial identification of the respondent. Respondents could write in multiple races, but to keep it simple, we will combine the small number of individuals who reported multiple races with those who listed “Other” as their race. educ: The education of the respondent. This is recorded as an ordinal variable. The “Some college” response indicates individuals who have attended college (including 2-year programs) but have not earned a BA. income: The family income of the respondent in 1000s of dollars. Respondents did not give actual dollar amounts here but rather indicated which bracket of income (e.g. $20,000-30,000) they fell within. For the purposes of our class, I randomly select an actual value within this bracket for each respondent. workstatus: The work status of the respondent. Respondents could either be working, unemployed, or out of the labor force. The last category refers to people who are not employed and not currently looking for work, whereas unemployed indicates a person who is not employed an is currently looking for work. military: Whether the respondent has ever served or is currently serving in the US military. Popularity This data comes from the National Longitudinal Study of Adolescent to Adult Health (Add Health), conducted by the Carolina Population Center at UNC-Chapel Hill and supported by a grant from the National Institute of Child Health and Human Development. The first wave of the study which we are using surveyed adolescents between 7th and 12th grade in school in the 1994-95 school year. One of the particularly valuable features of the Add Health survey is that many respondents were in the “saturation sample” which sampled all students at 16 schools. In this saturation sample, students were asked about who were their friends and sexual partners, which allows researchers to construct network maps of adolescent social systems. We will use this saturation sample to look at a various basic measure of that network that estimates students’ popularity. This measure, which is called “in degree” in the network analysis literature, measures the number of times a student was nominated as a friend by other students in the school. We will treat it as a simple proxy measure of a student’s popularity. We can then look at what other student characteristics were positively or negatively associated with a student’s popularity. Here is a full description of all variables in the dataset that we will use. indegree: The number of friend nominations received by other students at the same school. This is the measure of popularity that we will use. race: A six-category nominal variable indicating the race that the student best thought described them when asked to choose a single race: white, black, Latino, Asian, American Indian, other. sex: Add Health reports this as a student’s “biological” sex. Students were only reported as male or female.grade: current grade of the student as a quantitative variable. psuedoGPA: Students were asked for the most recent letter grade in four course types: math, language arts, science, and math. This variable was constructed by calculating GPA from those responses. honorsociety: A true/false variable for whether a student was in honor society or not. alcoholuse: A true/false variable that is true if the student reported drinking at least once or twice a month in the last twelve months. smoker: A true/false variable that is true if student smoked more than 5 cigarettes in the past 30 days. bandchoir: a true/false variable that is true if the student was in band or choir. academicclub: a true/false variable that was true if the student was in an academically-oriented club such as math club, book club, etc. nsports: The number of different school sports a student reported participating in. Students who reported more than six sports were top-coded at the value of six. parentinc: Parent’s household income measured in $1000’s of dollars. Sex The sex data come from a special supplemental questionnaire that was added to the General Social Survey (GSS) in 2004. The GSS is a survey of attitudes that is conducted every two years by the National Opinion Research Council (NORC). In the 2004 supplement, respondents were asked questions about their sexual behavior. We will be looking specifically at respondents reported frequency of sexual activity and its relationship to demographic characteristics such as age, education, and marital status. Here are the variables we will look at: sexf: A quantitative variable indicating the frequency of sexual activity as the number of sexual encounters per year. The sequal frequency response was coded as an ordinal scale variable in which respondents were given a set of options from less to more sexual activity in the previous year. For our purposes, I have recoded the ordinal sexfreq variable into a quantitative variable by giving everyone the midpoint number of sexual acts per year based upon their answer. For example, individuals who said (2 or 3 times a month) were given a value of \\(2.5*12=30\\). This will allow us to use sexual frequency as a dependent variable in regression models. age: The age of the respondent. The GSS only surveys adults aged 18 years and older. gender: The gender of the respondent. educ: Years of education for the respondent. marital: Marital status of the respondent: Never married, married, divorced, widowed, separated. relig: Religious affiliation of the respondent. Protestants have been divided into “mainline” and “fundamentalist” based on a coding of specific denominations used by the GSS. Titanic The titanic data contain information on all 1,309 passengers aboard the Titanic. The data do not include information about the crew. The data primarily come from the online database, Encyclopedia Titanica. Here are the variables we will look at: survival: Did the passenger survive? sex: The reported sex of the passenger. age: The age of the passenger. This variable is reported in whole numbers for those over one year old and as a decimal (based on months of age) for infants under a year of age. agegroup: A categorical variable indicating whether the person was an adult or a child. I have constructed this variable from the age variable. The cutoff for adults is sixteen years of age. pclass: There were three passenger classes: First, second, and third (also known as steerage). To give some pop culture references, Rose was first class, and Jack was third class. Most of the passengers were in third class. fare: The fare paid for the ticket, measured in British pounds. family: The number of family members traveling with the passenger. These family members can either be parents, spouses, siblings, or children. Wages This data has information on the hourly wages of US workers in 2018. The data here are extracted from Current Population Survey data via IPUMS. I used the earning data from the outgoing rotation groups (ORG) for each month of the CPS. Each household in the CPS is is part of a rolling panel in which they are in for four months, out for eight months, and back in for four months. In the fourth and eight month of inclusion they are given additional questions as part of the outgoing rotation group. The hourly wage of salaried workers is assessed by a question on hours worked in a typical week and earnings in the prior week. I limited the data only to those individuals between the ages of 18 and 65 in order to capture the age range of the typical worker. The dataset contains the following variables: wages: The hourly wage for the respondent. For workers who report being paid hourly, this value is based on a direct question that asked for respondents’ hourly wages. For individuals in salaried positions, this value was derived by dividing the earnings from the previous week by the hours worked in the previous week. Anyone who reported a wage of less than one dollar is removed. Any wage higher than $99.99 is top-coded as $99.99. age: age of the respondent in years. gender: Male or Female. race: The respondent’s racial identification recoded from two separate questions on race and hispanicity into the following categories: White, Black, Latino, Asian, Indigenous, and Other/Multiple races. The indigenous category includes American Indians, Pacific Islanders, and Alaska Natives. marstat: The respondent’s current marital status: never married, married, divorced or separated, and widowed. education: The respondent’s highest educational attainment: no high school diploma, high school diploma, associate’s degree, bachelor’s degree, graduate degree. The last category includes master’s degrees, professional degrees, and doctoral degrees. occup: The broad occupational category of the respondent. In the actual CPS data, there are hundreds of different occupations listed. For our purposes, I have simplified this into a broader (and smaller) set of occupational categories that we will use for the analysis. Here are the categories of the occupational variable, along with some examples of specific occupations: Managers: Human resources Managers, Operations Managers Business/Finance Specialist: Claims Adjusters, Compliance Officers, Accountants, Tax Preparers STEM: Computer Programmers, Civil Engineers, Biological Scientists Doctors: Dentists, Surgeons, Optometrists Legal: Lawyers, Judges, Paralegals Education: Preschool and Kindergarten Teachers, Librarians Arts, Design, and Media: Artists, Dancers and Choreographers, Writers and Authors Other Healthcare: Registered Nurses, Physical Therapists, Dental Hygienists Social Services: Clergy, Social Workers Service: Waiters and Waitresses, Barbers, Bartenders Sales: Cashier, Telemarketer Administrative Support: Bank Tellers, Data Entry Keyers, Receptionist Manual: Carpenters, Logging Workers, Mining Machine Operators, Small Engine Mechanic nchild: Number of own children living in the household with the respondent. foreign_born: A variable indicating whether the respondent is foreign born or not. Recorded as “Yes” or “No”. earn_type: This variable indicates whether the respondent reported being paid hourly wages or by salary. earningwt: A technical weighting variable for use with any CPS analysis of earnings. "],
["common-r-commands.html", "Common R Commands", " Common R Commands Below is a list of common commands that we use for the undergraduate class, along with some examples. You can view a help file in RStudio for each command by searching for the command name in the help tab in the lower right panel. You can also just type the name of the command preceded by a “?” into the console. For example, if you wanted to understand how barplot works, type: ?barplot The list here does not contain information about making plots in R. That information is in the Plotting Cookbook appendix. Univariate Statistics mean Calculate the mean of a quantitative variable. Remember that this command will not work for categorical variables. mean(earnings$wages) ## [1] 24.27601 median Calculate the median of a quantitative variable. Remember that this command will not work for categorical variables. median(earnings$wages) ## [1] 19.21667 sd Calculate the standard deviation of a quantitative variable. Remember that this command will not work for categorical variables. sd(earnings$wages) ## [1] 16.23676 IQR Calculate the interquartile range of a quantitative variable. Remember that this command will not work for categorical variables. IQR(earnings$wages) ## [1] 17 quantile Calculate percentiles of a distribution. Remember that this command will not work for categorical variables. By default, the quantile command will return the quartiles (0,25,50,75,100 percentiles). If you want different percentiles, you will have to specify the probs argument. quantile(earnings$wages) ## 0% 25% 50% 75% 100% ## 1.00000 13.00000 19.21667 30.00000 99.99000 #get the 10th and 90th percentile instead quantile(earnings$wages, probs = c(0.1,0.9)) ## 10% 90% ## 10.000 47.596 table Calculate the absolute frequencies of the categories a categorical variable. table(movies$Genre) ## ## Action Animation Comedy Drama Family ## 207 139 781 332 155 ## Horror Musical/Music Mystery Romance SciFi/Fantasy ## 220 103 39 138 258 ## Thriller ## 181 prop.table Calculate the proportions (i.e. relative frequencies) of the categories of a categorical variable. This command must be run on the output from a table command. You can do that in one command by nesting the table command inside the prop.table command. prop.table(table(movies$Genre)) ## ## Action Animation Comedy Drama Family ## 0.08108108 0.05444575 0.30591461 0.13004309 0.06071289 ## Horror Musical/Music Mystery Romance SciFi/Fantasy ## 0.08617313 0.04034469 0.01527615 0.05405405 0.10105758 ## Thriller ## 0.07089698 summary Provide a summary of a variable, either categorical or quantitative. summary(earnings$wages) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 13.00 19.22 24.28 30.00 99.99 summary(movies$Genre) ## Action Animation Comedy Drama Family ## 207 139 781 332 155 ## Horror Musical/Music Mystery Romance SciFi/Fantasy ## 220 103 39 138 258 ## Thriller ## 181 Bivariate Statistics table Can be used to create a two-way table, although further work needs to be done to extract useful information from the two-way table. table(movies$Genre, movies$Rating) ## ## G PG PG-13 R ## Action 0 3 98 106 ## Animation 37 92 6 4 ## Comedy 1 71 352 357 ## Drama 2 36 116 178 ## Family 16 131 8 0 ## Horror 0 1 60 159 ## Musical/Music 0 7 57 39 ## Mystery 0 0 7 32 ## Romance 0 12 64 62 ## SciFi/Fantasy 0 10 186 62 ## Thriller 0 0 37 144 prop.table Calculate the conditional distributions from a two-way table. The first argument here must be a two-way table output from the table command. It is very important that you also add a second argument that indicated the way you want the conditional distributions. 1 will give you distributions conditional on the row variable and 2 will give you distributions conditional on the column variable. ## ## G PG PG-13 R ## Action 0.000000000 0.014492754 0.473429952 0.512077295 ## Animation 0.266187050 0.661870504 0.043165468 0.028776978 ## Comedy 0.001280410 0.090909091 0.450704225 0.457106274 ## Drama 0.006024096 0.108433735 0.349397590 0.536144578 ## Family 0.103225806 0.845161290 0.051612903 0.000000000 ## Horror 0.000000000 0.004545455 0.272727273 0.722727273 ## Musical/Music 0.000000000 0.067961165 0.553398058 0.378640777 ## Mystery 0.000000000 0.000000000 0.179487179 0.820512821 ## Romance 0.000000000 0.086956522 0.463768116 0.449275362 ## SciFi/Fantasy 0.000000000 0.038759690 0.720930233 0.240310078 ## Thriller 0.000000000 0.000000000 0.204419890 0.795580110 tapply Calculate a statistic (e.g. mean, median, sd, IQR) for a quantitative variable across the categories of a categorical variable. The first argument should be the quantitative variable. The second argument should be the categorical variable. The third argument should be the name of the command that will calculate the desired statistic. tapply(movies$Runtime, movies$Rating, mean) ## G PG PG-13 R ## 90.80357 99.71901 108.02321 105.25547 tapply(movies$Runtime, movies$Rating, median) ## G PG PG-13 R ## 90 96 105 102 tapply(movies$Runtime, movies$Rating, sd) ## G PG PG-13 R ## 14.63796 13.95487 17.58490 16.07108 cor Calculate the correlation coefficient between two quantitative variables. cor(crimes$Violent, crimes$Gini) ## [1] 0.5454737 Statistical Inference nrow Return the number of observations in a dataset. nrow(crimes) ## [1] 51 qt Calculate the t-value needed for a confidence interval. For a 95% confidence interval, the first argument should always be 0.975. The second argument should be the appropriate degrees of freedom for the statistic and dataset. qt(0.975, nrow(politics)-1) ## [1] 1.960524 pt Calculate the p-value for a hypothesis test. The first argument should always be the negative version of the t-statistic and the second argument should be the appropriate degrees of freedom for the statistic and dataset. 2*pt(-2.1, nrow(politics)-1) ## [1] 0.03578782 OLS Regression Models lm Run an OLS regression model. The first argument should always be a formula of the form dependent~independent1+independent2+.... To simplify the writing of variable names, it is often useful to specify a second argument data that identifies that dataset being used. Then you don’t have to include dataset_name$ in the formula. **Remember to always put the dependent (y) variable on the left hand side of the equation. #simple model with one independent variable model_simple &lt;- lm(wages~age, data=earnings) #same simple model but recenter age on 45 years of age model_recenter &lt;- lm(wages~I(age-45), data=earnings) #a model with multiple independent variables, both quantitative and qualitative model_multiple &lt;- lm(wages~I(age-45)+education+race+gender+nchild, data=earnings) #a model like the previous but also with interaction between gender and nchild model_interaction &lt;- lm(wages~I(age-45)+education+race+gender*nchild, data=earnings) Once a model object is created, information can be extracted with either the coef command which just reports the slopes and intercept, or a full summary command which gives more information. coef(model_interaction) ## (Intercept) I(age - 45) ## 17.3568021 0.2242916 ## educationHS Diploma educationAA Degree ## 4.5382688 7.4288321 ## educationBachelors Degree educationGraduate Degree ## 16.2657784 23.0187910 ## raceBlack raceLatino ## -3.4176245 -2.1133582 ## raceAsian raceIndigenous ## 0.5641751 -1.5198248 ## raceOther/Multiple genderFemale ## -0.4331997 -4.3777137 ## nchild genderFemale:nchild ## 1.2629571 -0.7490706 summary(model_interaction) ## ## Call: ## lm(formula = wages ~ I(age - 45) + education + race + gender * ## nchild, data = earnings) ## ## Residuals: ## Min 1Q Median 3Q Max ## -43.638 -7.779 -2.198 4.568 90.578 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 17.356802 0.159721 108.669 &lt; 2e-16 *** ## I(age - 45) 0.224292 0.002858 78.471 &lt; 2e-16 *** ## educationHS Diploma 4.538269 0.154451 29.383 &lt; 2e-16 *** ## educationAA Degree 7.428832 0.181143 41.011 &lt; 2e-16 *** ## educationBachelors Degree 16.265778 0.164396 98.943 &lt; 2e-16 *** ## educationGraduate Degree 23.018791 0.178161 129.202 &lt; 2e-16 *** ## raceBlack -3.417625 0.123798 -27.607 &lt; 2e-16 *** ## raceLatino -2.113358 0.109491 -19.302 &lt; 2e-16 *** ## raceAsian 0.564175 0.157602 3.580 0.000344 *** ## raceIndigenous -1.519825 0.321284 -4.730 2.24e-06 *** ## raceOther/Multiple -0.433200 0.303134 -1.429 0.152987 ## genderFemale -4.377714 0.090203 -48.532 &lt; 2e-16 *** ## nchild 1.262957 0.043476 29.049 &lt; 2e-16 *** ## genderFemale:nchild -0.749071 0.063268 -11.840 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.74 on 145633 degrees of freedom ## Multiple R-squared: 0.284, Adjusted R-squared: 0.2839 ## F-statistic: 4443 on 13 and 145633 DF, p-value: &lt; 2.2e-16 Utility functions round Used for rounding the results of numbers to a given number of decimal places. By default, it will round to whole numbers, but you can specify the number of decimal places in the second argument. 100*round(prop.table(table(movies$Genre)), 2) ## ## Action Animation Comedy Drama Family ## 8 5 31 13 6 ## Horror Musical/Music Mystery Romance SciFi/Fantasy ## 9 4 2 5 10 ## Thriller ## 7 sort Sort a vector of numbers from smallest to largest (default), or largest to smallest (with additional argument decreasing=TRUE). sort(100*round(prop.table(table(movies$Genre)), 2), decreasing = TRUE) ## ## Comedy Drama SciFi/Fantasy Horror Action ## 31 13 10 9 8 ## Thriller Family Animation Romance Musical/Music ## 7 6 5 5 4 ## Mystery ## 2 sort(100*round(prop.table(table(movies$Genre)), 2)) ## ## Mystery Musical/Music Animation Romance Family ## 2 4 5 5 6 ## Thriller Action Horror SciFi/Fantasy Drama ## 7 8 9 10 13 ## Comedy ## 31 "],
["plotting-cookbook.html", "Plotting Cookbook", " Plotting Cookbook This appendix will provide ggplot example R code and output for of all the graphs that we might use this term. For further information, I highly recommend Kieran Healy’s Data Visualization book and Hadley Wikham’s ggplot2 book. All the examples provided will use the standard example datasets that we have been working with throughout the term. Barplots Oddly, barplots are one of the trickiest graphs in ggplotif you want proportions rather than absolute frequencies for each category. To make proportions, I need to specify y=..prop.. in the aesthetics, and then use group=1 to ensure that all categories are part of a single group and thus have proportions that sum up to one. In the code below, I am also using the scale_y_continuous command and the scales::percent command to label the tickmarks on the y-axis as percents rather than proportions. ggplot(titanic, aes(x=pclass, y=..prop.., group=1))+ geom_bar()+ scale_y_continuous(labels=scales::percent)+ labs(x=&quot;passenger class&quot;, y=NULL, title=&quot;Distribution of passenger class on Titanic&quot;)+ theme_bw() Figure 56: A barplot The result of this code is shown in Figure 56. Because the tickmark labels are self-explanatory I use a value of NULL for the y-axis label. Histograms Histograms are a snap in ggplot. The binwidth argument will allow you to easily resize bin widths without having to provide every break as base plot does. ggplot(movies, aes(x=Runtime))+ geom_histogram(binwidth = 5, col=&quot;black&quot;)+ labs(x=&quot;movie runtime (minutes)&quot;, title=&quot;Histogram of movie runtime&quot;)+ theme_bw() Figure 57: A basic histogram Figure 57 shows the basic histogram from the code above. By default, ggplot will not place a border around bars, but I like to make the border a slightly different color (e.g. col=\"black\") to provide better visual definition. You can also calculate density instead of count, if you prefer, by adding the y=..density.. option to the aesthetics. Density is the proportion of cases in a bin, divided by bin width. If you plot a histogram with density, you can also overlay this figure with a smoothed line approximating the distribution called a kernel density. These results are shown in Figure 58. ggplot(movies, aes(x=Runtime, y=..density..))+ geom_histogram(binwidth = 5, col=&quot;black&quot;)+ geom_density(col=&quot;red&quot;, fill=&quot;grey&quot;, alpha=0.5)+ labs(x=&quot;movie runtime (minutes)&quot;, title=&quot;Histogram of movie runtime&quot;)+ theme_bw() Figure 58: A histogram with kernel density smoother Boxplots The only tricky thing to a boxplot in ggplot is to remember that the aesthetic is y not x. Its not required, but setting x=\"\" in the aesthetics and then setting the x label to NULL will also create a cleaner display with no x-axis. I also apply a geom_boxplot only argument here that colors outliers a different color. ggplot(movies, aes(x=&quot;&quot;, y=Runtime))+ geom_boxplot(fill=&quot;grey&quot;, outlier.color = &quot;red&quot;)+ labs(y=&quot;movie runtime (minutes)&quot;, x=NULL, title=&quot;Boxplot of movie runtime&quot;)+ theme_bw() Figure 59: A basic boxplot You can also try a geom_violin instead which basically gives you a mirrored kernel density plot. For a single variable, use x=1 because geom_violin requires an x aesthetic. Below, I show how you can extend this into a comparative violin plot. ggplot(movies, aes(x=1, y=Runtime))+ geom_violin(fill=&quot;grey&quot;)+ scale_x_continuous(labels=NULL)+ labs(x=NULL, y=&quot;movie runtime (minutes)&quot;, title=&quot;Violin plot of movie runtime&quot;)+ theme_bw() Figure 60: A violin plot Comparative Barplots Comparative barplots are definitely one of the most difficult cases to graph properly in ggplot because grouping causes all kinds of issues with calculating proportions correctly. The best approach that I have found is to use faceting to separate out the barplots of one variable by the categories of the other variable. The basic code is then identical to that for a single barplot above with the addition of a facet_wrap command to identify the second categorical variable to use for faceting. ggplot(titanic, aes(x=survival, y=..prop.., group=1))+ geom_bar()+ facet_wrap(~pclass)+ scale_y_continuous(labels = scales::percent)+ labs(x=NULL, y=NULL, title=&quot;Survival on the Titanic by class&quot;)+ theme_bw() Figure 61: A comparative barplot made by faceting Figure 61 shows the result. In this case, since survival and death are mirror images, we could convey the same information, but with much less ink by a simple dotplot of the percent surviving by passenger class. However, this would require some preparatory work to prepare the data in the way we want to display it. For the undergraduate class, you are not required to understand how to prepare this data. tab &lt;- prop.table(table(titanic$survival, titanic$pclass),2) tab &lt;- subset(as.data.frame.table(tab), Var1==&quot;Survived&quot;, select=c(&quot;Var2&quot;,&quot;Freq&quot;)) colnames(tab) &lt;- c(&quot;pclass&quot;,&quot;prop_surv&quot;) ggplot(tab, aes(x=pclass, y=prop_surv))+ geom_point()+ scale_y_continuous(labels=scales::percent, limits=c(0,0.8))+ labs(x=NULL, y=NULL, title=&quot;Percent surviving Titanic by class&quot;)+ coord_flip()+ theme_bw() Figure 62: A simple dotplot Figure 62 shows the resulting dotplot. If that is a little too spartan for you, you could try a lollipop graph, shown in Figure 63. The geom_lollipop is in the ggalt library. ggplot(tab, aes(x=pclass, y=prop_surv))+ geom_lollipop()+ scale_y_continuous(labels=scales::percent, limits=c(0,0.7))+ labs(x=NULL, y=NULL, title=&quot;Percent surviving Titanic by class&quot;)+ coord_flip()+ theme_bw() Figure 63: A lollipop plot Comparative Boxplots To construct comparative boxplots, we just need to add an x to the aesthetic of a boxplot. ggplot(movies, aes(x=Genre, y=Runtime))+ geom_boxplot(fill=&quot;grey&quot;, outlier.color = &quot;red&quot;)+ labs(y=&quot;movie runtime (minutes)&quot;, title=&quot;Boxplot of movie runtime by genre&quot;)+ theme_bw() Figure 64: A basic comparative boxplot The mediocre results are shown in Figure 64. The labels on the x-axis often run into one another, so coord_flip is a good option to avoid that. In addition, I can also use the varwidth argument in geom_boxplot to scale the width of boxplots relative to the number of observations. ggplot(movies, aes(x=Genre, y=Runtime))+ geom_boxplot(fill=&quot;grey&quot;, outlier.color = &quot;red&quot;, varwidth=TRUE)+ labs(y=&quot;movie runtime (minutes)&quot;, title=&quot;Boxplot of movie runtime by genre&quot;)+ coord_flip()+ theme_bw() Figure 65: Flipping coordinates for better display My improved comparative boxplot is shown in Figure 65. However, Its also often useful to re-order the categories by the mean or median of the quantitative variable. We can do this with the reorder command: ggplot(movies, aes(x=reorder(Genre, Runtime, median), y=Runtime))+ geom_boxplot(fill=&quot;grey&quot;, outlier.color = &quot;red&quot;, varwidth=TRUE)+ labs(y=&quot;movie runtime (minutes)&quot;, x=NULL, title=&quot;Boxplot of movie runtime by genre&quot;)+ coord_flip()+ theme_bw() Figure 66: Re-order categories by median The results are shown in Figure 66. In this case, we used the reorder command to reorder the categories of Genre by the median value of Runtime. We could also try a comparative violin plot as shown in Figure 67. ggplot(movies, aes(x=reorder(Genre, Runtime, median), y=Runtime))+ geom_violin(fill=&quot;grey&quot;)+ labs(y=&quot;movie runtime (minutes)&quot;, x=NULL, title=&quot;Boxplot of movie runtime by genre&quot;)+ coord_flip()+ theme_bw() Figure 67: A comparative violin plot Scatterplots The geom_point command gives us the the geom we want for scatterplots. We can also use geom_smooth to graph a variety of lines to our data. Setting an alpha value can help with overplotting as well by providing semi-transparency. ggplot(politics, aes(x=age, y=income))+ geom_point(alpha=0.5)+ scale_y_continuous(labels=scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;family income (in thousands USD)&quot;, title=&quot;Scatterplot between age and family income&quot;, caption=&quot;ANES 2016&quot;)+ theme_bw() Figure 68: Basic Scatterplot The results are shown in Figure 68. As more points are plotted in the same position, the overall plot darkens. However, there is still a lot of overplotting because age is only recorded in whole numbers. In cases like this, we can use geom_jitter rather than geom_point to randomly perturb each value pair a little bit, as shown in Figure 14. ggplot(politics, aes(x=age, y=income))+ geom_jitter(alpha=0.5)+ scale_y_continuous(labels=scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;family income (in thousands USD)&quot;, title=&quot;Scatterplot between age and family income&quot;, caption=&quot;ANES 2016&quot;)+ theme_bw() Figure 69: Jittering points to deal with overplotting It is often useful to apply some model to the data to draw a best-fitting line through the points. The geom_smooth function will do this for us and we can use the method argument to specify what kind of model we want to fit. You an apply non-parametric smoothers like LOESS as well as an OLS regression line. Lets first try a LOESS smoother. ggplot(politics, aes(x=age, y=income))+ geom_jitter(alpha=0.5)+ geom_smooth(method=&quot;loess&quot;)+ scale_y_continuous(labels=scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;family income (in thousands USD)&quot;, title=&quot;Scatterplot between age and family income&quot;, caption=&quot;ANES 2016&quot;)+ theme_bw() Figure 70: Plotting loess smoother to data Figure 70 shows us the smoothed line. It looks like an inverted U-shaped distribution. Family income is also highly right skewed. Lets try scale_y_log10 to put income on a log-scale (advanced). ggplot(politics, aes(x=age, y=income))+ geom_jitter(alpha=0.5)+ geom_smooth(method=&quot;loess&quot;)+ scale_y_log10(labels=scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;family income (in thousands USD)&quot;, title=&quot;Scatterplot between age and family income&quot;, caption=&quot;ANES 2016&quot;)+ theme_bw() Figure 71: Changing scale of y-axis to logarithmic Figure 71 shows the scatterplot with an exponential scale for the y-axis. Note how the values go from $1K to $10K to $100K at even intervals. Now lets try switch the smoothing to a linear model, by simply changing the method in geom_smooth: ggplot(politics, aes(x=age, y=income))+ geom_jitter(alpha=0.5)+ geom_smooth(method=&quot;lm&quot;)+ scale_y_log10(labels=scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;family income (in thousands USD)&quot;, title=&quot;Scatterplot between age and family income&quot;, caption=&quot;ANES 2016&quot;)+ theme_bw() Figure 72: Fitting an OLS regression line Figure 72 shows the OLS line predicting income. Its pretty flat which to some extent hides the underlying curvilinear relationship. In practice, I would probably want to apply a quadratic term on age to any regression model. Finally, we can add add in an aesthetic for a third categorical variable by applying color to the plots. In this case, I will color the plots by highest degree received. ggplot(politics, aes(x=age, y=income))+ geom_jitter(alpha=0.5, aes(color=educ))+ geom_smooth(method=&quot;lm&quot;)+ scale_color_brewer(palette = &quot;YlOrRd&quot;)+ scale_y_log10(labels=scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;family income (in thousands USD)&quot;, title=&quot;Scatterplot between age and family income&quot;, caption=&quot;ANES 2016&quot;, color=&quot;Highest degree&quot;)+ theme_bw() Figure 73: Adding color aesthetic Figure 73 shows the result. Because I defined the color aesthetic in geom_jitter rather than the base ggplot command, I still only get one smoothed OLS regression line for the whole dataset. If I wanted separate lines by educational level, I could add the color aesthetic to ggplot instead. ggplot(politics, aes(x=age, y=income, color=educ))+ geom_jitter(alpha=0.5)+ geom_smooth(method=&quot;lm&quot;)+ scale_color_brewer(palette = &quot;YlOrRd&quot;)+ scale_y_log10(labels=scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;family income (in thousands USD)&quot;, title=&quot;Scatterplot between age and family income&quot;, caption=&quot;ANES 2016&quot;, color=&quot;Highest degree&quot;)+ theme_bw() Figure 74: Separate OLS lines by color aesthetic Figure 74 shows the same plot but now with separate lines for each educational level. We are basically plotting interaction terms here between age and educational level. Instead of coloration, I could have used faceting to try to separate out differences by education. Figure 75 shows the results using this approach. For this graph, I use two separate geom_smooth commands to get the LOESS and linear fit for each panel. ggplot(politics, aes(x=age, y=income))+ geom_jitter(alpha=0.3)+ geom_smooth(method=&quot;lm&quot;)+ geom_smooth(method=&quot;loess&quot;, color=&quot;red&quot;)+ facet_wrap(~educ)+ scale_y_log10(labels=scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;family income (in thousands USD)&quot;, title=&quot;Scatterplot between age and family income&quot;, caption=&quot;ANES 2016&quot;)+ theme_bw() Figure 75: facet by education instead of color If you have a small number of cases, it might make sense to label your values. You can label values with geom_text but you can often run into problems with labels being truncated or overlapping. ggplot(crimes, aes(x=Unemployment, y=Property))+ geom_point(alpha=0.7)+ geom_smooth(method=&quot;lm&quot;, se=FALSE)+ geom_text(aes(label=State), size=2)+ labs(x=&quot;Unemployment rate&quot;, y=&quot;Property crimes per 100,000 population&quot;, title=&quot;Scatterplot of unemployment and property crime&quot;)+ theme_bw() Figure 76: labeling points can look really bad Figure 76 shows labeling with the default geom_text approach. You can fiddle with a variety of arguments here that let you offset the labels but it rarely produces clean labels. The library ggrepel has a function geom_text_repel that will work harder to place labels in a readable position: ggplot(crimes, aes(x=Unemployment, y=Property))+ geom_point(alpha=0.7)+ geom_smooth(method=&quot;lm&quot;, se=FALSE)+ geom_text_repel(aes(label=State), size=2)+ labs(x=&quot;Unemployment rate&quot;, y=&quot;Property crimes per 100,000 population&quot;, title=&quot;Scatterplot of unemployment rate and property crime rate&quot;)+ theme_bw() Figure 77: repel works better for labeling points, but still can get too busy "],
["r-stat-lab.html", "R Stat Lab", " R Stat Lab "]
]
